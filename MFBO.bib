% Encoding: UTF-8

@Article{Haftka1991,
  author    = {Haftka, Raphael T.},
  title     = {Combining global and local approximations},
  journal   = {AIAA Journal},
  year      = {1991},
  volume    = {29},
  number    = {9},
  pages     = {1523--1525},
  month     = sep,
  issn      = {0001-1452},
  comment   = {doi: 10.2514/3.10768},
  doi       = {10.2514/3.10768},
  groups    = {MF modeling, Multiplicative},
  publisher = {American Institute of Aeronautics and Astronautics},
  url       = {https://doi.org/10.2514/3.10768},
}

@Article{Chang1993,
  author    = {Chang, Kwan J. and Haftka, Raphael T. and Giles, Gary L. and Kao, Pi-Jen},
  title     = {Sensitivity-based scaling for approximating structural response},
  journal   = {Journal of Aircraft},
  year      = {1993},
  volume    = {30},
  number    = {2},
  pages     = {283--288},
  month     = mar,
  issn      = {0021-8669},
  comment   = {doi: 10.2514/3.48278},
  doi       = {10.2514/3.48278},
  groups    = {Multiplicative},
  publisher = {American Institute of Aeronautics and Astronautics},
  url       = {https://doi.org/10.2514/3.48278},
}

@Article{Goldfeld2005,
  author   = {Goldfeld, Y. and Vervenne, K. and Arbocz, J. and van Keulen, F.},
  title    = {Multi-fidelity optimization of laminated conical shells for buckling},
  journal  = {Structural and Multidisciplinary Optimization},
  year     = {2005},
  volume   = {30},
  number   = {2},
  pages    = {128--141},
  issn     = {1615-1488},
  abstract = {Optimum laminate configuration for minimum weight of filament-wound laminated conical shells is investigated subject to a buckling load constraint. In the case of a composite laminated conical shell, due to the manufacturing process, the thickness and the ply orientation are functions of the shell coordinates, which ultimately results in coordinate dependence of the stiffness matrices (A,B,D). These effects influence both the buckling load and the weight of the structure and complicate the optimization problem considerably. High computational cost is involved in calculating the buckling load by means of a high-fidelity analysis, e.g. using the computer code STAGS-A. In order to simplify the optimization procedure, a low-fidelity model based on the assumption of constant material properties throughout the shell is adopted, and buckling loads are calculated by means of a low-fidelity analysis, e.g. using the computer code BOCS. This work proposes combining the high-fidelity analysis model (based on exact material properties) with the low-fidelity model (based on nominal material properties) by using correction response surfaces, which approximate the discrepancy between buckling loads determined from different fidelity analyses. The results indicate that the proposed multi-fidelity approaches using correction response surfaces can be used to improve the computational efficiency of structural optimization problems.},
  doi      = {10.1007/s00158-004-0506-9},
  groups   = {Multiplicative},
  refid    = {Goldfeld2005},
  url      = {https://doi.org/10.1007/s00158-004-0506-9},
}

@Article{Sun2010,
  author   = {Sun, Guangyong and Li, Guangyao and Stone, Michael and Li, Qing},
  title    = {A two-stage multi-fidelity optimization procedure for honeycomb-type cellular materials},
  journal  = {Computational Materials Science},
  year     = {2010},
  volume   = {49},
  number   = {3},
  pages    = {500--511},
  issn     = {0927-0256},
  abstract = {This paper presents a two-stage multi-fidelity (or named variable fidelity) method for surrogate models in honeycomb crashworthiness design. The high/low-fidelity analyses adopt fine/coarse finite element (FE) meshes, respectively. In Stage I, a correction response surface (RS) was constructed from the ratio or difference between high-fidelity and low-fidelity analyses at few sample points. In Stage II, the high-fidelity response is approximated via a radial basis function (RBF), either alone or incorporating with the correction RS model from Stage I. It is shown that the presented two-stage multi-fidelity model is more appropriate than any single-fidelity counterpart in the honeycomb optimization under the same computational cost.},
  doi      = {10.1016/j.commatsci.2010.05.041},
  groups   = {Multiplicative},
  keywords = {Honeycomb, Cellular materials, multi-fidelity optimization, Variable fidelity, Response surface method (RSM), Radial basis function (RBF), Crashworthiness},
  url      = {https://www.sciencedirect.com/science/article/pii/S0927025610003137},
}

@Article{ZhangYiming2018,
  author    = {Zhang, Yiming and Kim, Nam H. and Park, Chanyoung and Haftka, Raphael T.},
  journal   = {AIAA Journal},
  title     = {Multifidelity Surrogate Based on Single Linear Regression},
  year      = {2018},
  issn      = {0001-1452},
  month     = oct,
  number    = {12},
  pages     = {4944--4952},
  volume    = {56},
  abstract  = {Multifidelity surrogates (MFS) combine low-fidelity models with few high-fidelity samples to infer the response of the high-fidelity model for design optimization or uncertainty quantification. Most publications in MFS focus on Bayesian frameworks based on Gaussian process. Other types of surrogates might be preferred for some applications. In this paper, a simple and yet powerful MFS based on single linear regression is proposed, termed as linear regression multifidelity surrogate (LR-MFS), especially for fitting high-fidelity data with noise. The LR-MFS considers the low-fidelity model as a basis function and identifies unknown coefficients of both the low-fidelity model and the discrepancy function using a single linear regression. Because the proposed LR-MFS is obtained from standard linear regression, it can take advantage of established regression techniques such as prediction variance, D-optimal design, and inference. The LR-MFS is first compared with three Bayesian frameworks using a benchmark dataset from the simulations of a fluidized-bed process. The LR-MFS showed a comparable accuracy with the best Bayesian frameworks. The effect of combining multiple low-fidelity models was also discussed. Then the LR-MFS is evaluated using an algebraic function with different sampling plans. The LR-MFS bested co-kriging for 55?63% cases with an increasing number of high-fidelity (HF) samples. The sources of uncertainty with an increasing number of samples were also discussed. For both examples, the LR-MFS proved to be better than fitting only HF samples and robust with noisy data.},
  comment   = {doi: 10.2514/1.J057299},
  doi       = {10.2514/1.J057299},
  groups    = {Additive},
  publisher = {American Institute of Aeronautics and Astronautics},
  url       = {https://doi.org/10.2514/1.J057299},
}

@Article{Palar2016,
  author   = {Palar, Pramudita Satria and Tsuchiya, Takeshi and Parks, Geoffrey Thomas},
  title    = {Multi-fidelity non-intrusive polynomial chaos based on regression},
  journal  = {Computer Methods in Applied Mechanics and Engineering},
  year     = {2016},
  volume   = {305},
  pages    = {579--606},
  issn     = {0045-7825},
  abstract = {In this paper we present a multi-fidelity (MF) extension of non-intrusive polynomial chaos based on regression (point collocation) for uncertainty quantification purposes. The proposed method uses the principle of a global correction function from a previous similar method that uses spectral projection to estimate the coefficients. Due to its usage of regression to estimate the coefficients, the present method offers high flexibility in the sampling and generation of the polynomial basis. The method takes advantage of a nested sampling plan to create the samples for the low-fidelity (LF) and correction expansions where the high-fidelity (HF) samples are a subset of the LF ones. To build the polynomial basis, a total order or hyperbolic truncation strategy is used with a highly flexible combination of the LF and correction polynomial expansions. The method is demonstrated on some artificial test problems and aerodynamic problems of the Euler flow around an airfoil and common three-dimensional research models. In order to derive the strategies for successful MF approximation, the effect of the correlation and the errors between the LF and HF functions is also studied. The results show that high correlation and moderately low errors are important to improve the MF approximation’s accuracy. On a common research model problem, the MF approach with partially-converged simulations as the LF samples can successfully reduce the computational cost to about 40% for similar accuracy compared to an approach using a single HF expansion.},
  doi      = {10.1016/j.cma.2016.03.022},
  groups   = {Additive},
  keywords = {Uncertainty quantification, Multi-fidelity, Polynomial chaos, Point collocation, Flexible sampling, Hyperbolic truncation},
  url      = {https://www.sciencedirect.com/science/article/pii/S0045782516301049},
}

@InCollection{Lewis2000,
  author    = {Lewis, Robert and Nash, Stephen},
  title     = {A multigrid approach to the optimization of systems governed by differential equations},
  booktitle = {The 8th Symposium on Multidisciplinary Analysis and Optimization},
  publisher = {American Institute of Aeronautics and Astronautics},
  year      = {2000},
  number    = {4890},
  series    = {Multidisciplinary Analysis Optimization Conferences},
  month     = sep,
  comment   = {doi:10.2514/6.2000-4890},
  doi       = {10.2514/6.2000-4890},
  groups    = {MF modeling, Additive},
  journal   = {8th Symposium on Multidisciplinary Analysis and Optimization},
  url       = {https://doi.org/10.2514/6.2000-4890},
}

@Article{FernandezGodino2019,
  author   = {Fernández-Godino, M. Giselle and Dubreuil, Sylvain and Bartoli, Nathalie and Gogu, Christian and Balachandar, S. and Haftka, Raphael T.},
  journal  = {Structural and Multidisciplinary Optimization},
  title    = {Linear regression-based multifidelity surrogate for disturbance amplification in multiphase explosion},
  year     = {2019},
  issn     = {1615-1488},
  number   = {6},
  pages    = {2205--2220},
  volume   = {60},
  abstract = {When simulations are very expensive and many are required, as for optimization or uncertainty quantification, a way to reduce cost is using surrogates. With multiple simulations to predict the quantity of interest, some being very expensive and accurate (high-fidelity simulations) and others cheaper but less accurate (low-fidelity simulations), it may be worthwhile to use multifidelity surrogates (MFSs). Moreover, if we can afford just a few high-fidelity simulations or experiments, MFS becomes necessary. Co-Kriging, which is probably the most popular MFS, replaces both low-fidelity and high-fidelity simulations by a single MFS. A recently proposed linear regression-based MFS (LR-MFS) offers the option to correct the LF simulations instead of correcting the LF surrogate in the MFS. When the low-fidelity simulation is cheap enough for use in an application, such as optimization, this may be an attractive option. In this paper, we explore the performance of LR-MFS using exact and surrogate-replaced low-fidelity simulations. The problem studied is a cylindrical dispersal of 100-μ m-diameter solid particles after detonation and the quantity of interest is a measure of the amplification of the departure from axisymmetry. We find very substantial accuracy improvements for this problem using the LR-MFS with exact low-fidelity simulations. Inspired by these results, we also compare the performance of co-Kriging to the use of Kriging to correct exact low-fidelity simulations and find a similar accuracy improvement when simulations are directly used. For this problem, further improvements in accuracy are achievable by taking advantage of inherent parametric symmetries. These results may alert users of MFSs to the possible advantages of using exact low-fidelity simulations when this is affordable.},
  doi      = {10.1007/s00158-019-02387-4},
  groups   = {Additive},
  refid    = {Fernández-Godino2019},
  url      = {https://doi.org/10.1007/s00158-019-02387-4},
}

@Article{Song2019,
  author   = {Song, Xueguan and Lv, Liye and Sun, Wei and Zhang, Jie},
  title    = {A radial basis function-based multi-fidelity surrogate model: exploring correlation between high-fidelity and low-fidelity models},
  journal  = {Structural and Multidisciplinary Optimization},
  year     = {2019},
  volume   = {60},
  number   = {3},
  pages    = {965--981},
  issn     = {1615-1488},
  abstract = {In computational simulation, a high-fidelity (HF) model is generally more accurate than a low-fidelity (LF) model, while the latter is generally more computationally efficient than the former. To take advantages of both HF and LF models, a multi-fidelity surrogate model based on radial basis function (MFS-RBF) is developed in this paper by combining HF and LF models. To determine the scaling factor between HF and LF models, a correlation matrix is augmented by further integrating LF responses. The scaling factor and relevant basis function weights are then calculated by employing corresponding HF responses. MFS-RBF is compared with Co-Kriging model, multi-fidelity surrogate based on linear regression (LR-MFS) model, CoRBF model, and three single-fidelity surrogates. The impact of key factors, such as the cost ratio of LF to HF models and different combinations of HF and LF samples, is also investigated. The results show that (i) MFS-RBF presents a better accuracy and robustness than the three benchmark MFS models and single-fidelity surrogates in about 90% cases of this paper; (ii) MFS-RBF is less sensitive to the correlation between HF and LF models than the three MFS models; (iii) by fixing the total computational cost, the cost ratio of LF to HF models is suggested to be less than 0.2, and 10-80% of the total cost should be used for LF samples; (iv) the MFS-RBF model is able to save an average of 50 to 70% computational cost if HF and LF models are highly correlated.},
  doi      = {10.1007/s00158-019-02248-0},
  groups   = {Additive},
  refid    = {Song2019},
  url      = {https://doi.org/10.1007/s00158-019-02248-0},
}

@Article{Kou2019,
  author   = {Kou, Jiaqing and Zhang, Weiwei},
  title    = {Multi-fidelity modeling framework for nonlinear unsteady aerodynamics of airfoils},
  journal  = {Applied Mathematical Modelling},
  year     = {2019},
  volume   = {76},
  pages    = {832--855},
  issn     = {0307-904X},
  abstract = {Aerodynamic data can be obtained from different sources, which vary in fidelity, availability and cost. As the fidelity of data increases, the cost of data acquisition usually becomes higher. Therefore, to obtain accurate unsteady aerodynamic model with very low cost and the desired level of accuracy, this paper proposes an unsteady multi-fidelity aerodynamic modeling framework. The approach integrates ideas from data fusion, multi-fidelity modeling, nonlinear system identification and machine learning. Data fusion reduces the total cost of data generation for model construction, while multi-fidelity modeling with a nonlinear autoregressive with exogenous input (NARX) description provides a general framework for unsteady aerodynamics. The correction term from the low-fidelity model to the high-fidelity result is then identified by a machine learning approach, i.e., a multi-kernel neural network. To validate the proposed method, unsteady aerodynamics of a NACA0012 airfoil pitching at Mach number 0.8 is modeled. The high-fidelity data is obtained from a Navier-Stokes-equation-based solver, while the low-fidelity solution is taken from an Euler-equation-based flow solver. The main difference between two types of data is that the high-fidelity solution takes into account the viscous effect, while the low-fidelity solution is based the invisicid flow assumption. Besides, to mimic the practical situation where high-fidelity data are limited in amount and diversity due to high cost (e.g., the experimental condition), only three high-fidelity unsteady aerodynamic solutions from harmonic motion are available. After performing a multi-fidelity analysis on a typical harmonic motion, the model is applied to the prediction of aerodynamic loads from either new harmonic motions or random motions. The multi-fidelity model shows a good agreement with the high-fidelity solution, indicating that by using only a few high-fidelity data and a low-fidelity model, high-fidelity results can be accurately reproduced. Furthermore, the model convergence with respect to increasing training data, and the comparison with a single high-fidelity reduced-order model (ROM) are also studied. The proposed approach becomes more accurate as the number of high-fidelity samples increases, and outperforms a single aerodynamic ROM in most of test cases. Compared with ROM method, additional computational cost for the proposed approach is small, therefore the total time cost of model training is still low.},
  doi      = {10.1016/j.apm.2019.06.034},
  groups   = {Additive},
  keywords = {Multi-fidelity, Data fusion, Data-driven modeling, Reduced-order model, Unsteady aerodynamics, Transonic flow},
  url      = {https://www.sciencedirect.com/science/article/pii/S0307904X19303968},
}

@Article{Meng2020,
  author   = {Meng, Xuhui and Karniadakis, George Em},
  title    = {A composite neural network that learns from multi-fidelity data: {A}pplication to function approximation and inverse {PDE} problems},
  journal  = {Journal of Computational Physics},
  year     = {2020},
  volume   = {401},
  pages    = {109020},
  issn     = {0021-9991},
  abstract = {Currently the training of neural networks relies on data of comparable accuracy but in real applications only a very small set of high-fidelity data is available while inexpensive lower fidelity data may be plentiful. We propose a new composite neural network (NN) that can be trained based on multi-fidelity data. It is comprised of three NNs, with the first NN trained using the low-fidelity data and coupled to two high-fidelity NNs, one with activation functions and another one without, in order to discover and exploit nonlinear and linear correlations, respectively, between the low-fidelity and the high-fidelity data. We first demonstrate the accuracy of the new multi-fidelity NN for approximating some standard benchmark functions but also a 20-dimensional function that is not easy to approximate with other methods, e.g. Gaussian process regression. Subsequently, we extend the recently developed physics-informed neural networks (PINNs) to be trained with multi-fidelity data sets (MPINNs). MPINNs contain four fully-connected neural networks, where the first one approximates the low-fidelity data, while the second and third construct the correlation between the low- and high-fidelity data and produce the multi-fidelity approximation, which is then used in the last NN that encodes the partial differential equations (PDEs). Specifically, by decomposing the correlation into a linear and nonlinear part, the present model is capable of learning both the linear and complex nonlinear correlations between the low- and high-fidelity data adaptively. By training the MPINNs, we can: (1) obtain the correlation between the low- and high-fidelity data, (2) infer the quantities of interest based on a few scattered data, and (3) identify the unknown parameters in the PDEs. In particular, we employ the MPINNs to learn the hydraulic conductivity field for unsaturated flows as well as the reactive models for reactive transport. The results demonstrate that MPINNs can achieve relatively high accuracy based on a very small set of high-fidelity data. Despite the relatively low dimension and limited number of fidelities (two-fidelity levels) for the benchmark problems in the present study, the proposed model can be readily extended to very high-dimensional regression and classification problems involving multi-fidelity data.},
  doi      = {10.1016/j.jcp.2019.109020},
  groups   = {Additive},
  keywords = {Multi-fidelity, Physics-informed neural networks, Adversarial data, Porous media, Reactive transport},
  url      = {https://www.sciencedirect.com/science/article/pii/S0021999119307260},
}

@Article{Gratiet2013,
  author    = {Gratiet, Loic Le},
  journal   = {SIAM/ASA Journal on Uncertainty Quantification},
  title     = {Bayesian Analysis of Hierarchical Multifidelity Codes},
  year      = {2013},
  month     = jan,
  number    = {1},
  pages     = {244--269},
  volume    = {1},
  abstract  = {This paper deals with the Gaussian process--based approximation of a code which can be run at different levels of accuracy. This method, which is a particular case of cokriging, allows us to improve a surrogate model of a complex computer code using fast approximations of it. In particular, we focus on the case of a large number of code levels on the one hand and on a Bayesian approach when we have two levels on the other hand. The main results of this paper are a new approach to estimating the model parameters which provides a closed form expression for an important parameter of the model (the scale factor), a reduction of the numerical complexity by simplifying the covariance matrix inversion, and a new Bayesian modeling that gives an explicit representation of the joint distribution of the parameters and that is not computationally expensive. A thermodynamic example is used to illustrate the comparison between 2-level and 3-level cokriging.},
  doi       = {10.1137/120884122},
  groups    = {Hybrid},
  publisher = {Society for Industrial and Applied Mathematics},
}

@Article{Xiong2008,
  author   = {Xiong, Ying and Chen, Wei and Tsui, Kwok-Leung},
  title    = {A New Variable-Fidelity Optimization Framework Based on Model Fusion and Objective-Oriented Sequential Sampling},
  journal  = {Journal of Mechanical Design},
  year     = {2008},
  volume   = {130},
  number   = {11},
  month    = oct,
  issn     = {1050-0472},
  abstract = {Computational models with variable fidelity have been widely used in engineering design. To alleviate the computational burden, surrogate models are used for optimization without directly invoking expensive high-fidelity simulations. In this work, a model fusion technique based on the Bayesian-Gaussian process modeling is employed to construct cheap surrogate models to integrate information from both low-fidelity and high-fidelity models, while the interpolation uncertainty of the surrogate model due to the lack of sufficient high-fidelity simulations is quantified. In contrast to space filling, the sequential sampling of a high-fidelity simulation model in our proposed framework is objective-oriented, aiming for improving a design objective. Strategy based on periodical switching criteria is studied, which is shown to be effective in guiding the sequential sampling of a high-fidelity model toward improving a design objective as well as reducing the interpolation uncertainty. A design confidence metric is proposed as the stopping criterion to facilitate design decision making against the interpolation uncertainty. Examples are provided to illustrate the key ideas and features of model fusion, sequential sampling, and design confidence--the three key elements in the proposed variable-fidelity optimization framework.},
  doi      = {10.1115/1.2976449},
  groups   = {Additive},
  url      = {https://doi.org/10.1115/1.2976449},
}

@Article{Zheng2013,
  author    = {Zheng, Jun and Shao, Xinyu and Gao, Liang and Jiang, Ping and Li, Zilong},
  title     = {A hybrid variable-fidelity global approximation modelling method combining tuned radial basis function base and kriging correction},
  journal   = {Journal of Engineering Design},
  year      = {2013},
  volume    = {24},
  number    = {8},
  pages     = {604--622},
  month     = aug,
  issn      = {0954-4828},
  comment   = {doi: 10.1080/09544828.2013.788135},
  doi       = {10.1080/09544828.2013.788135},
  groups    = {Additive},
  publisher = {Taylor \& Francis},
  url       = {https://doi.org/10.1080/09544828.2013.788135},
}

@Article{Park2017,
  author   = {Park, Chanyoung and Haftka, Raphael T. and Kim, Nam H.},
  title    = {Remarks on multi-fidelity surrogates},
  journal  = {Structural and Multidisciplinary Optimization},
  year     = {2017},
  volume   = {55},
  number   = {3},
  pages    = {1029--1050},
  issn     = {1615-1488},
  abstract = {Different multi-fidelity surrogate (MFS) frameworks have been used for optimization or uncertainty quantification. This paper investigates differences between various MFS frameworks with the aid of examples including algebraic functions and a borehole example. These MFS include three Bayesian frameworks using 1) a model discrepancy function, 2) low fidelity model calibration and 3) a comprehensive approach combining both. Three counterparts in simple frameworks are also included, which have the same functional form but can be built with ready-made surrogates. The sensitivity of frameworks to the choice of design of experiments (DOE) is investigated by repeating calculations with 100 different DOEs. Computational cost savings and accuracy improvement over a single fidelity surrogate model are investigated as a function of the ratio of the sampling costs between low and high fidelity simulations. For the examples considered, MFS frameworks were found to be more useful for saving computational time rather than improving accuracy. For the Hartmann 6 function example, the maximum cost saving for the same accuracy was 86 %, while the maximum accuracy improvement for the same cost was 51 %. It was also found that DOE can substantially change the relative standing of different frameworks. The cross-validation error appears to be a reasonable candidate for estimating poor MFS frameworks for a specific problem but it does not perform well compared to choosing single fidelity surrogates.},
  doi      = {10.1007/s00158-016-1550-y},
  groups   = {Additive},
  refid    = {Park2017},
  url      = {https://doi.org/10.1007/s00158-016-1550-y},
}

@Article{ZhangY2018,
  author   = {Zhang, Yu and Han, Zhong-Hua and Zhang, Ke-Shi},
  journal  = {Structural and Multidisciplinary Optimization},
  title    = {Variable-fidelity expected improvement method for efficient global optimization of expensive functions},
  year     = {2018},
  issn     = {1615-1488},
  number   = {4},
  pages    = {1431--1451},
  volume   = {58},
  abstract = {The efficient global optimization method (EGO) based on kriging surrogate model and expected improvement (EI) has received much attention for optimization of high-fidelity, expensive functions. However, when the standard EI method is directly applied to a variable-fidelity optimization (VFO) introducing assistance from cheap, low-fidelity functions via hierarchical kriging (HK) or cokriging, only high-fidelity samples can be chosen to update the variable-fidelity surrogate model. The theory of infilling low-fidelity samples towards the improvement of high-fidelity function is still a blank area. This article proposes a variable-fidelity EI (VF-EI) method that can adaptively select new samples of both low and high fidelity. Based on the theory of HK model, the EI of the high-fidelity function associated with adding low- and high-fidelity sample points are analytically derived, and the resulting VF-EI is a function of both the design variables x and the fidelity level l. Through maximizing the VF-EI, both the sample location and fidelity level of next numerical evaluation are determined, which in turn drives the optimization converging to the global optimum of high-fidelity function. The proposed VF-EI is verified by six analytical test cases and demonstrated by two engineering problems, including aerodynamic shape optimizations of RAE 2822 airfoil and ONERA M6 wing. The results show that it can remarkably improve the optimization efficiency and compares favorably to the existing methods.},
  doi      = {10.1007/s00158-018-1971-x},
  groups   = {Additive, Sequential model-based},
  refid    = {Zhang2018},
  url      = {https://doi.org/10.1007/s00158-018-1971-x},
}

@Article{deBaar2015,
  author   = {de Baar, Jouke and Roberts, Stephen and Dwight, Richard and Mallol, Benoit},
  journal  = {Computers {\&} Fluids},
  title    = {Uncertainty quantification for a sailing yacht hull, using multi-fidelity kriging},
  year     = {2015},
  issn     = {0045-7930},
  pages    = {185--201},
  volume   = {123},
  abstract = {Uncertainty quantification (UQ) for CFD-based ship design can require a large number of simulations, resulting in significant overall computational cost. Presently, we use an existing method, multi-fidelity Kriging, to reduce the number of simulations required for the UQ analysis of the performance of a sailing yacht hull, considering uncertainties in the tank blockage, mass and centre of gravity. We compare the UQ results with experimental values.},
  doi      = {10.1016/j.compfluid.2015.10.004},
  groups   = {Additive},
  keywords = {Uncertainty quantification, Multi-fidelity, Kriging, RANS, Free-surface},
  url      = {https://www.sciencedirect.com/science/article/pii/S0045793015003382},
}

@Article{Rokita2018,
  author    = {Rokita, Tomer and Friedmann, Peretz P.},
  title     = {Multifidelity coKriging for High-Dimensional Output Functions with Application to Hypersonic Airloads Computation},
  journal   = {AIAA Journal},
  year      = {2018},
  volume    = {56},
  number    = {8},
  pages     = {3060--3070},
  month     = jul,
  issn      = {0001-1452},
  abstract  = {An accurate surrogate model is capable of reproducing high-fidelity results at a fraction of the computational cost. coKriging is an efficient method for constructing surrogate models when two levels of fidelity (low and high) tools are available. This paper extends the functionality of coKriging. First, a practical implementation to high-dimensional output functions is developed by introducing POD-coKriging. Second, coKriging is reformulated to allow different sizes of input/output variables between low- and high-fidelity tools. The extended POD-coKriging method is validated against an analytical problem, and its performance for high-dimensional output functions is demonstrated with the prediction of surface pressure distribution on a two-dimensional deflected panel in hypersonic flow. Finally, properties of the extended POD-coKriging were used to estimate the total cost reduction of sample generation, and it is shown that an order-of-magnitude cost reduction is achievable for practical cases.},
  comment   = {doi: 10.2514/1.J056620},
  doi       = {10.2514/1.J056620},
  groups    = {Additive},
  publisher = {American Institute of Aeronautics and Astronautics},
  url       = {https://doi.org/10.2514/1.J056620},
}

@Article{Kaps2022,
  author   = {Kaps, Arne and Czech, Catharina and Duddeck, Fabian},
  title    = {A hierarchical kriging approach for multi-fidelity optimization of automotive crashworthiness problems},
  journal  = {Structural and Multidisciplinary Optimization},
  year     = {2022},
  volume   = {65},
  number   = {4},
  pages    = {114},
  issn     = {1615-1488},
  abstract = {Multi-fidelity optimization schemes enriching expensive high-fidelity functions with cheap-to-evaluate low-fidelity functions have gained popularity in recent years. In the present work, an optimization scheme based on a hierarchical kriging is proposed for large-scale and highly non-linear crashworthiness problems. After comparison to other multi-fidelity techniques an infill criterion called variable-fidelity expected improvement is applied and evaluated. This is complemented by two innovative techniques, a new approach regarding initial sampling and a novel way to generate the low-fidelity model for crash problems are suggested. For the former, a modified Latin hypercube sampling, pushing samples more towards design space boundaries, increases the quality of sampling selection. For the latter, a projection-based non-intrusive model order reduction technique accelerates and simplifies the low-fidelity model evaluation. The proposed techniques are investigated with two application problems from the field of automotive crashworthiness--a size optimization problem for lateral impact and a shape optimization problem for frontal impact. The use of a multi-fidelity scheme compared to baseline single-fidelity optimization saves computational effort while keeping an acceptable level of accuracy. Both suggested modifications, independently and especially combined, increase computational performance and result quality in the presented examples.},
  doi      = {10.1007/s00158-022-03211-2},
  groups   = {Additive},
  refid    = {Kaps2022},
  url      = {https://doi.org/10.1007/s00158-022-03211-2},
}

@Article{Jiang2019,
  author    = {Jiang, Ping and Cheng, Ji and Zhou, Qi and Shu, Leshi and Hu, Jiexiang},
  title     = {Variable-Fidelity Lower Confidence Bounding Approach for Engineering Optimization Problems with Expensive Simulations},
  journal   = {AIAA Journal},
  year      = {2019},
  volume    = {57},
  number    = {12},
  pages     = {5416--5430},
  month     = aug,
  issn      = {0001-1452},
  abstract  = {Engineering design optimization with expensive simulations is usually a computationally prohibitive process. As one of the most famous efficient global optimization approaches, the lower confidence bounding (LCB) approach has been widely applied to relieve this computational burden. However, the LCB approach can be used only for design optimization problems with the single-fidelity level. In this paper, a variable-fidelity lower confidence bounding (VF-LCB) approach is developed to extend the LCB approach to engineering problems with multifidelity levels. First, a VF-LCB function is analytically derived to adaptively select LF or HF samples according to the predicted values and uncertainty from the VF metamodel. Then the coefficient of variations (CoV) of the predicted values and uncertainty of the VF model, which can reflect the degree of dispersion of the prediction values and uncertainty, respectively, are introduced to balance the global exploration and local exploitation objectively. To illustrate the effectiveness and merits of the proposed VF-LCB approach, eight numerical examples, and the design of micro-aerial vehicle (MAV) fuselage are tested. Comparative results between the proposed approach and the other five existing methods show that the average cost savings are about 25% for eight numerical examples and about 45% for the MAV problem compared with the other five existing methods.},
  comment   = {doi: 10.2514/1.J058283},
  doi       = {10.2514/1.J058283},
  groups    = {Additive},
  publisher = {American Institute of Aeronautics and Astronautics},
  url       = {https://doi.org/10.2514/1.J058283},
}

@Article{Kuya2011,
  author    = {Kuya, Yuichi and Takeda, Kenji and Zhang, Xin and Forrester, Alexander I. J.},
  title     = {Multifidelity Surrogate Modeling of Experimental and Computational Aerodynamic Data Sets},
  journal   = {AIAA Journal},
  year      = {2011},
  volume    = {49},
  number    = {2},
  pages     = {289--298},
  month     = feb,
  issn      = {0001-1452},
  comment   = {doi: 10.2514/1.J050384},
  doi       = {10.2514/1.J050384},
  groups    = {Additive},
  publisher = {American Institute of Aeronautics and Astronautics},
  url       = {https://doi.org/10.2514/1.J050384},
}

@Article{Toal2011,
  author    = {Toal, David J. J. and Keane, Andy J.},
  title     = {Efficient Multipoint Aerodynamic Design Optimization Via Cokriging},
  journal   = {Journal of Aircraft},
  year      = {2011},
  volume    = {48},
  number    = {5},
  pages     = {1685--1695},
  month     = sep,
  issn      = {0021-8669},
  comment   = {doi: 10.2514/1.C031342},
  doi       = {10.2514/1.C031342},
  groups    = {Additive},
  publisher = {American Institute of Aeronautics and Astronautics},
  url       = {https://doi.org/10.2514/1.C031342},
}

@Article{Keane2012,
  author    = {Keane, Andy J.},
  title     = {Cokriging for Robust Design Optimization},
  journal   = {AIAA Journal},
  year      = {2012},
  volume    = {50},
  number    = {11},
  pages     = {2351--2364},
  month     = nov,
  issn      = {0001-1452},
  comment   = {doi: 10.2514/1.J051391},
  doi       = {10.2514/1.J051391},
  groups    = {Additive},
  publisher = {American Institute of Aeronautics and Astronautics},
  url       = {https://doi.org/10.2514/1.J051391},
}

@Article{Leary2003,
  author   = {Leary, Stephen J. and Bhaskar, Atul and Keane, Andy J.},
  title    = {A Knowledge-Based Approach To Response Surface Modelling in Multifidelity Optimization},
  journal  = {Journal of Global Optimization},
  year     = {2003},
  volume   = {26},
  number   = {3},
  pages    = {297--319},
  issn     = {1573-2916},
  abstract = {This paper is concerned with approximations for expensive function evaluation - the expensive functions arising in an engineering design context. The problem of reducing the computational cost of generating sufficient learning samples is addressed. Several approaches of using a priori knowledge to achieve computational economy are presented. In all these, the results of a cheap model are treated as knowledge to be incorporated in the training process. Several approaches are described here: in particular, we focus on neural based systems. This approach is then developed as a new knowledge-based kriging model which is shown to be as accurate as neural based alternatives while being much easier to train. Examples from the domain of structural optimization are given to demonstrate the approach.},
  doi      = {10.1023/A:1023283917997},
  groups   = {Additive},
  refid    = {Leary2003},
  url      = {https://doi.org/10.1023/A:1023283917997},
}

@Article{Goh2013,
  author    = {Goh, Joslin and Bingham, Derek and Holloway, James Paul and Grosskopf, Michael J. and Kuranz, Carolyn C. and Rutter, Erica},
  title     = {Prediction and Computer Model Calibration Using Outputs From Multifidelity Simulators},
  journal   = {Technometrics},
  year      = {2013},
  volume    = {55},
  number    = {4},
  pages     = {501--512},
  month     = nov,
  issn      = {0040-1706},
  comment   = {doi: 10.1080/00401706.2013.838910},
  doi       = {10.1080/00401706.2013.838910},
  groups    = {Additive},
  publisher = {Taylor \& Francis},
  url       = {https://doi.org/10.1080/00401706.2013.838910},
}

@Article{Toal2023,
  author   = {Toal, David J. J.},
  title    = {{Applications of multi-fidelity multi-output Kriging to engineering design optimization}},
  journal  = {Structural and Multidisciplinary Optimization},
  year     = {2023},
  volume   = {66},
  number   = {6},
  pages    = {125},
  issn     = {1615-1488},
  abstract = {Surrogate modelling is a popular approach for reducing the number of high fidelity simulations required within an engineering design optimization. Multi-fidelity surrogate modelling can further reduce this effort by exploiting low fidelity simulation data. Multi-output surrogate modelling techniques offer a way for categorical variables e.g. the choice of material, to be included within such models. While multi-fidelity multi-output surrogate modelling strategies have been proposed, to date only their predictive performance rather than optimization performance has been assessed. This paper considers three different multi-fidelity multi-output Kriging based surrogate modelling approaches and compares them to ordinary Kriging and multi-fidelity Kriging. The first approach modifies multi-fidelity Kriging to include multiple outputs whereas the second and third approaches model the different levels of simulation fidelity as different outputs within a multi-output Kriging model. Each of these techniques is assessed using three engineering design problems including the optimization of a gas turbine combustor in the presence of a topological variation, the optimization of a vibrating truss where the material can vary and finally, the parallel optimization of a family of airfoils.},
  doi      = {10.1007/s00158-023-03567-z},
  groups   = {Applications of MF modeling, Additive},
  refid    = {Toal2023},
  url      = {https://doi.org/10.1007/s00158-023-03567-z},
}

@Article{Zhou2020,
  author   = {Zhou, Qi and Wu, Yuda and Guo, Zhendong and Hu, Jiexiang and Jin, Peng},
  title    = {A generalized hierarchical co-Kriging model for multi-fidelity data fusion},
  journal  = {Structural and Multidisciplinary Optimization},
  year     = {2020},
  volume   = {62},
  number   = {4},
  pages    = {1885--1904},
  issn     = {1615-1488},
  abstract = {Multi-fidelity (MF) surrogate models have shown great potential in simulation-based design since they can make a trade-off between high prediction accuracy and low computational cost by augmenting the small number of expensive high-fidelity (HF) samples with a large number of cheap low-fidelity (LF) data. In this work, a generalized hierarchical co-Kriging (GCK) surrogate model is proposed for MF data fusion with both nested and non-nested sampling data. Specifically, a comprehensive Gaussian process (GP) Bayesian framework is developed by aggregating calibrated LF Kriging model and discrepancy stochastic Kriging model. The stochastic Kriging model enables the GCK model to consider the predictive uncertainty from the LF Kriging model at HF sampling points, making it possible to estimate the model parameter separately under both nested and non-nested sampling data. The performance of the GCK model is compared with three well-known Kriging-based MF surrogates, i.e., hybrid Kriging-scaling (HKS) model, KOH autoregressive (KOH) model, and hierarchical Kriging (HK) model, by testing them on two numerical examples and two real-life cases. The influence of correlations between LF and HF samples and the cost ratio between them are also analyzed. Comparison results on the illustrated cases demonstrate that the proposed GCK model shows great potential in MF modeling under non-nested sampling data, especially when the correlations between LF and HF samples are weak.},
  doi      = {10.1007/s00158-020-02583-7},
  groups   = {Additive},
  refid    = {Zhou2020},
  url      = {https://doi.org/10.1007/s00158-020-02583-7},
}

@Article{Hao2020,
  author   = {Hao, Peng and Feng, Shaojun and Li, Yuwei and Wang, Bo and Chen, Huihan},
  title    = {Adaptive infill sampling criterion for multi-fidelity gradient-enhanced kriging model},
  journal  = {Structural and Multidisciplinary Optimization},
  year     = {2020},
  volume   = {62},
  number   = {1},
  pages    = {353--373},
  issn     = {1615-1488},
  abstract = {Multi-fidelity surrogate (MFS) method is very promising for the optimization of complex problems. The optimization capability of MFS can be improved by infilling samples in the optimization process. Furthermore, once the gradient information is provided, the gradient-enhanced kriging (GEK) can be utilized to construct a more accurate MFS model. However, for the existing infill sampling criterions, it is difficult to improve the optimization speed without sacrificing the optimization gains. In this paper, a novel infill sampling criterion named Adaptive Multi-fidelity Expected Improvement (AMEI) is proposed, in which the prediction accuracy and the optimization potential of the surrogate model are both considered. With a set of extra samples calculated, the AMEI determines which fidelity model for the new sample is to be added. Through two numerical examples and two engineering examples, it can be found that the AMEI always provides the best optimization result with the fewest analysis calls, and the robustness is also good. The optimization capability and efficiency of the AMEI have been demonstrated compared with traditional criterions.},
  doi      = {10.1007/s00158-020-02493-8},
  groups   = {Hybrid, Sequential model-based},
  refid    = {Hao2020},
  url      = {https://doi.org/10.1007/s00158-020-02493-8},
}

@Article{Cheng2021,
  author   = {Cheng, Meng and Jiang, Ping and Hu, Jiexiang and Shu, Leshi and Zhou, Qi},
  title    = {A multi-fidelity surrogate modeling method based on variance-weighted sum for the fusion of multiple non-hierarchical low-fidelity data},
  journal  = {Structural and Multidisciplinary Optimization},
  year     = {2021},
  volume   = {64},
  number   = {6},
  pages    = {3797--3818},
  issn     = {1615-1488},
  abstract = {Multi-fidelity (MF) surrogate models have been widely adopted in simulation-based engineering design problems to reduce the computational cost by fusing data with diverse fidelity levels. Most of the MF modeling methods only apply to the problems with hierarchical low-fidelity (LF) models. However, the LF models obtained from different simplification approaches often vary in fidelity levels throughout the design space, namely, the multiple LF models are non-hierarchical. To address this challenge, a MF surrogate modeling method based on variance-weighted sum (VWS-MFS) is developed to flexibly handle multiple non-hierarchical LF data in this work. Firstly, each set of the non-hierarchical LF data is allocated diverse weights according to uncertainties quantified by variances of constructed Kriging models, which enables all the LF data to be fused and contribute to the trend function reflecting the response trend of the true model. Secondly, for more precise scaling factor between HF and LF models and mean square error (MSE) estimation, an improved hierarchical kriging (IHK) model is introduced to construct the MF surrogate model enabling the LF model scaled by a varied scaling factor to capture the characteristics of the HF model. The performance of the proposed VWS-MFS method is compared to three MF surrogate models through several numerical examples and one engineering case. Results show that the proposed method provides more accurate MF surrogate models under the same computational cost. Additionally, the proposed method saved the computational cost by more than 59.61% with the same model accuracy compared to the Kriging model built with HF data for the engineering case.},
  doi      = {10.1007/s00158-021-03055-2},
  groups   = {Hybrid},
  refid    = {Cheng2021},
  url      = {https://doi.org/10.1007/s00158-021-03055-2},
}

@Article{Ohsaki2009,
  author    = {Ohsaki, Makoto and Miyamura, Tomoshi and Kohiyama, Masayuki and Hori, Muneo and Noguchi, Hirohisa and Akiba, Hiroshi and Kajiwara, Koichi and Ine, Tatsuhiko},
  title     = {High-precision finite element analysis of elastoplastic dynamic responses of super-high-rise steel frames},
  journal   = {Earthquake Engineering and Structural Dynamics},
  year      = {2009},
  volume    = {38},
  number    = {5},
  pages     = {635--654},
  month     = apr,
  issn      = {0098-8847},
  abstract  = {Abstract A new framework is presented for analysis of dynamic collapse behavior of steel frames using large-scale parallel finite element method based on the domain decomposition method. The analysis software is based on ADVC as a part of the E-Simulator that takes advantage of recent development of computer science and high-performance parallel computing in computational mechanics. By making an analysis model with fine meshing, a complicated sequence of local buckling of columns and beams can be simulated. Numerical results are shown for dynamic collapse analysis of single-story and 5-story frame models to show that the global and local behaviors are simultaneously simulated by a high-precision finite element analysis. Eigenvalue analysis is also carried out for a 31-story frame to demonstrate that dynamic analysis can be carried out for a structure discretized to solid elements with more than 70 million DOFs. Copyright ? 2009 John Wiley \& Sons, Ltd.},
  doi       = {10.1002/eqe.900},
  groups    = {Examples of computational cost},
  keywords  = {large-scale structure, dynamic collapse analysis, finite element analysis, E-simulator, steel frame},
  publisher = {John Wiley \& Sons, Ltd},
  url       = {https://doi.org/10.1002/eqe.900},
}

@Book{Bathe2006,
  author    = {Bathe, Klaus-Jürgen},
  publisher = {Prentice Hall},
  title     = {Finite element procedures},
  year      = {2006},
  address   = {New Jersey, USA},
  isbn      = {097900490X},
  groups    = {FEM},
}

@Article{Gano2006a,
  author    = {Gano, Shawn E. and Renaud, John E. and Agarwal, Harish and Tovar, Andrés},
  title     = {Reliability-based design using variable-fidelity optimization},
  journal   = {Structure and Infrastructure Engineering},
  year      = {2006},
  volume    = {2},
  number    = {3-4},
  pages     = {247--260},
  month     = sep,
  issn      = {1573-2479},
  comment   = {doi: 10.1080/15732470600590408},
  doi       = {10.1080/15732470600590408},
  groups    = {Hybrid},
  publisher = {Taylor \& Francis},
  url       = {https://doi.org/10.1080/15732470600590408},
}

@Article{Nguyen2015,
  author   = {Nguyen, Nhu Van and Tyan, Maxim and Lee, Jae-Woo},
  title    = {A modified variable complexity modeling for efficient multidisciplinary aircraft conceptual design},
  journal  = {Optimization and Engineering},
  year     = {2015},
  volume   = {16},
  number   = {2},
  pages    = {483--505},
  issn     = {1573-2924},
  abstract = {This paper describes a modified variable complexity modeling (MVCM) framework that uses a neural network to replace the Taylor series after several warm-up iterations. The MVCM framework with an additive scaling function is most efficient in terms of high-fidelity function evaluation savings compared with traditional variable complexity modeling (VCM) among multiplicative and hybrid scaling functions. The MVCM framework achieves 59.1 and 68.6 % savings in high-fidelity function evaluations for one-dimensionally and two-dimensionally constrained problems, respectively, compared with the VCM method. The MVCM framework provides a larger trust region than the VCM due to the global behavior of neural networks. The MVCM solution also converges closely to the high-fidelity function. The MVCM framework is integrated with an in-house low-fidelity aircraft design synthesis program  and a high-fidelity analysis (AADL3D) for the conceptual design of multidisciplinary regional jet aircraft (RJA) to enhance the optimal RJA configuration compared with low-fidelity analysis results. The optimal RJA wing configuration using the MVCM framework provides more realistic and reasonable configurations compared to the results of low-fidelity analysis with a short turnaround time.},
  doi      = {10.1007/s11081-014-9273-7},
  groups   = {Hybrid},
  refid    = {Nguyen2015},
  url      = {https://doi.org/10.1007/s11081-014-9273-7},
}

@Article{Tyan2015,
  author    = {Tyan, Maxim and Nguyen, Nhu Van and Lee, Jae-Woo},
  title     = {Improving variable-fidelity modelling by exploring global design space and radial basis function networks for aerofoil design},
  journal   = {Engineering Optimization},
  year      = {2015},
  volume    = {47},
  number    = {7},
  pages     = {885--908},
  month     = jul,
  issn      = {0305-215X},
  comment   = {doi: 10.1080/0305215X.2014.941290},
  doi       = {10.1080/0305215X.2014.941290},
  groups    = {Hybrid},
  publisher = {Taylor \& Francis},
  url       = {https://doi.org/10.1080/0305215X.2014.941290},
}

@Article{Hu2016,
  author    = {Hu, Dianyin and Mao, Jianxing and Wang, Rongqiao and Jia, Zhigang and Song, Jun},
  title     = {Optimization Strategy for a Shrouded Turbine Blade Using Variable-Complexity Modeling Methodology},
  journal   = {AIAA Journal},
  year      = {2016},
  volume    = {54},
  number    = {9},
  pages     = {2808--2818},
  month     = may,
  issn      = {0001-1452},
  abstract  = {This paper presents the development of a collaborative optimization framework in combination with a variable-complexity modeling technique for the multidisciplinary coupling analysis and design of a shrouded turbine blade. The multidisciplinary optimization design of the shrouded turbine blade involves a high-fidelity detailed computational model and medium-fidelity models, which can become prohibitively expensive. In this investigation, a variable-complexity modeling methodology is introduced, where low-fidelity models and a scaling function are used to approximate the medium- and high-fidelity models through the optimizers in an inner-loop optimization to reduce computational expense. The optimization framework developed includes the collaborative optimization process, parametric modeling of the shrouded turbine blade, fluid?structure interaction solver using arbitrary Lagrangian?Eulerian formulation, an adaptive hexahedral structure mesh generator by establishing virtual blocks and parametric fixed points, and a variable-complexity modeling method combining the multiplicative and additive corrections to manage three levels of fidelity models. On the shrouded turbine-blade design problem, it achieves a feasible optimizer only calling nine high-fidelity analyses. Response surface model variation and cross-validation tests are performed to verify the predictive power of the response surface model in the multidisciplinary design optimization process of the shrouded turbine blade.},
  comment   = {doi: 10.2514/1.J054742},
  doi       = {10.2514/1.J054742},
  groups    = {Hybrid},
  publisher = {American Institute of Aeronautics and Astronautics},
  url       = {https://doi.org/10.2514/1.J054742},
}

@Article{Absi2016,
  author   = {Absi, Ghina N. and Mahadevan, Sankaran},
  title    = {Multi-fidelity approach to dynamics model calibration},
  journal  = {Mechanical Systems and Signal Processing},
  year     = {2016},
  volume   = {68-69},
  pages    = {189--206},
  issn     = {0888-3270},
  abstract = {This paper investigates the use of structural dynamics computational models with multiple levels of fidelity in the calibration of system parameters. Different types of models may be available for the estimation of unmeasured system properties, with different levels of physics fidelity, mesh resolution and boundary condition assumptions. In order to infer these system properties, Bayesian calibration uses information from multiple sources (including experimental data and prior knowledge), and comprehensively quantifies the uncertainty in the calibration parameters. Estimating the posteriors is done using Markov Chain Monte Carlo sampling, which requires a large number of computations, thus making the use of a high-fidelity model for calibration prohibitively expensive. On the other hand, use of a low-fidelity model could lead to significant error in calibration and prediction. Therefore, this paper develops an approach for model parameter calibration with a low-fidelity model corrected using higher fidelity simulations, and investigates the trade-off between accuracy and computational effort. The methodology is illustrated for a curved panel located in the vicinity of a hypersonic aircraft engine, subjected to acoustic loading. Two models (a frequency response analysis and a full time history analysis) are combined to calibrate the damping characteristics of the panel.},
  doi      = {10.1016/j.ymssp.2015.07.019},
  groups   = {Hybrid},
  keywords = {Multi-fidelity, Bayesian calibration, Hypersonic vehicle, Model uncertainty, Information fusion, Damping coefficient},
  url      = {https://www.sciencedirect.com/science/article/pii/S0888327015003386},
}

@Article{Rumpfkeil2017,
  author    = {Rumpfkeil, Markus P. and Beran, Philip S.},
  journal   = {AIAA Journal},
  title     = {Construction of Dynamic Multifidelity Locally Optimized Surrogate Models},
  year      = {2017},
  issn      = {0001-1452},
  month     = jul,
  number    = {9},
  pages     = {3169--3179},
  volume    = {55},
  abstract  = {The conceptual design phase for the development of new and revolutionary aircraft should include both low- and high-fidelity multidisciplinary engineering analyses to enable reasonable computational cost and accuracy tradeoffs. Multifidelity locally optimized surrogate models can be employed to fuse the multifidelity information to give an accurate representation of the underlying design space. These surrogates can be further enhanced with derivative information and augmented with dynamic training point selection to reduce overall computational cost. In this paper, a framework for multifidelity locally optimized surrogate models is developed, and the superior accuracy for the same computational cost or cheaper cost for the same accuracy compared to standard single-fidelity global surrogate models is demonstrated via analytic test functions. The multifidelity locally optimized surrogate models are also applied to the creation of an aerodynamic database involving the steady turbulent flow around a NACA 00xx airfoil, where the influence of Mach number, angle of attack, and thickness-to-chord ratio variations on the lift and drag coefficient are studied.},
  comment   = {doi: 10.2514/1.J055834},
  doi       = {10.2514/1.J055834},
  groups    = {Hybrid},
  publisher = {American Institute of Aeronautics and Astronautics},
  url       = {https://doi.org/10.2514/1.J055834},
}

@Article{Rumpfkeil2019,
  author    = {Rumpfkeil, Markus P. and Beran, Philip S.},
  title     = {Multifidelity Sparse Polynomial Chaos Surrogate Models Applied to Flutter Databases},
  journal   = {AIAA Journal},
  year      = {2019},
  volume    = {58},
  number    = {3},
  pages     = {1292--1303},
  month     = nov,
  issn      = {0001-1452},
  abstract  = {Multifidelity sparse polynomial chaos expansion (MFSPCE) models of critical flutter dynamic pressures as a function of Mach number, angle of attack, and thickness-to-chord ratio are constructed in lieu of solely using computationally expensive high-fidelity engineering analyses. Compressed sensing is used to determine a sparse representation, and an all-at-once approach is employed to create multifidelity polynomial chaos expansions with hybrid additive/multiplicative bridge functions. To demonstrate that accurate MFSPCE models can be obtained at lower computational cost than high-fidelity full-order polynomial chaos expansions, two analytic test functions and a more complex application example, which is the well-known Advisory Group for Aerospace Research and Development 445.6 aeroelastic model, are employed. The high- and low-fidelity levels considered are Euler and panel solutions, respectively, which are all combined with a modal structural solver.},
  comment   = {doi: 10.2514/1.J058452},
  doi       = {10.2514/1.J058452},
  groups    = {Hybrid},
  publisher = {American Institute of Aeronautics and Astronautics},
  url       = {https://doi.org/10.2514/1.J058452},
}

@Article{Wang2021,
  author   = {Wang, Shuo and Liu, Yin and Zhou, Qi and Yuan, Yongliang and Lv, Liye and Song, Xueguan},
  title    = {A multi-fidelity surrogate model based on moving least squares: fusing different fidelity data for engineering design},
  journal  = {Structural and Multidisciplinary Optimization},
  year     = {2021},
  volume   = {64},
  number   = {6},
  pages    = {3637--3652},
  issn     = {1615-1488},
  abstract = {In numerical simulations, a high-fidelity (HF) simulation is generally more accurate than a low-fidelity (LF) simulation, while the latter is generally more computationally efficient than the former. To take advantages of both HF and LF simulations, a multi-fidelity surrogate (MFS) model based on moving least squares (MLS), termed as adaptive MFS-MLS, is proposed. The MFS-MLS calculates the LF scaling factors and the unknown coefficients of the discrepancy function simultaneously using an extended MLS model. In the proposed method, HF samples are not regarded as equally important in the process of constructing MFS-MLS models, and adaptive weightings are given to different HF samples. Moreover, both the size of the influence domain and the scaling factors can be determined adaptively according to the training samples. The MFS-MLS model is compared with three state-of-the-art MFS models and three single-fidelity surrogate models in terms of the prediction accuracy through multiple benchmark numerical cases and an engineering problem. In addition, the effects of key factors on the performance of the MFS-MLS model, such as the correlation between HF and LF models, the cost ratio of HF to LF samples, and the combination of HF and LF samples, are also investigated. The results show that MFS-MLS is able to provide competitive performance with high computational efficiency.},
  doi      = {10.1007/s00158-021-03044-5},
  groups   = {Hybrid},
  refid    = {Wang2021},
  url      = {https://doi.org/10.1007/s00158-021-03044-5},
}

@Article{Viana2009,
  author   = {Viana, Felipe A. C. and Steffen, Valder and Butkewitsch, Sergio and de Freitas Leal, Marcus},
  title    = {Optimization of aircraft structural components by using nature-inspired algorithms and multi-fidelity approximations},
  journal  = {Journal of Global Optimization},
  year     = {2009},
  volume   = {45},
  number   = {3},
  pages    = {427--449},
  issn     = {1573-2916},
  abstract = {In this work, a flat pressure bulkhead reinforced by an array of beams is designed using a suite of heuristic optimization methods (Ant Colony Optimization, Genetic Algorithms, Particle Swarm Optimization and LifeCycle Optimization), and the Nelder-Mead simplex direct search method. The compromise between numerical performance and computational cost is addressed, calling for inexpensive, yet accurate analysis procedures. At this point, variable fidelity is proposed as a tradeoff solution. The difference between the low-fidelity and high-fidelity models at several points is used to fit a surrogate that corrects the low-fidelity model at other points. This allows faster linear analyses during the optimization; whilst a reduced set of expensive non-linear analyses are run “off-line,” enhancing the linear results according to the physics of the structure. Numerical results report the success of the proposed methodology when applied to aircraft structural components. The main conclusions of the work are (i) the variable fidelity approach enabled the use of intensive computing heuristic optimization techniques; and (ii) this framework succeeded in exploring the design space, providing good initial designs for classical optimization techniques. The final design is obtained when validating the candidate solutions issued from both heuristic and classical optimization. Then, the best design can be chosen by direct comparison of the high-fidelity responses.},
  doi      = {10.1007/s10898-008-9383-x},
  groups   = {Additive},
  refid    = {Viana2009},
  url      = {https://doi.org/10.1007/s10898-008-9383-x},
}

@Article{Bandler2004,
  author      = {Bandler, J. W. and Cheng, Q. S. and Nikolova, N. K. and Ismail, M. A.},
  title       = {Implicit space mapping optimization exploiting preassigned parameters},
  journal     = {IEEE Transactions on Microwave Theory and Techniques},
  year        = {2004},
  volume      = {52},
  number      = {1},
  pages       = {378--385},
  issn        = {1557-9670},
  call-number = {52},
  doi         = {10.1109/TMTT.2003.820892},
  groups      = {Input-input space mapping},
}

@Article{Koziel2006,
  author      = {Koziel, S. and Bandler, J. W. and Madsen, K.},
  journal     = {IEEE Transactions on Microwave Theory and Techniques},
  title       = {{A space-mapping framework for engineering optimization--Theory and implementation}},
  year        = {2006},
  issn        = {1557-9670},
  number      = {10},
  pages       = {3721--3730},
  volume      = {54},
  call-number = {54},
  doi         = {10.1109/TMTT.2006.882894},
  groups      = {Input-input space mapping},
}

@Article{Robinson2008,
  author    = {Robinson, T. D. and Eldred, M. S. and Willcox, K. and Haimes, R.},
  title     = {Surrogate-Based Optimization Using Multifidelity Models with Variable Parameterization and Corrected Space Mapping},
  journal   = {AIAA Journal},
  year      = {2008},
  volume    = {46},
  number    = {11},
  pages     = {2814--2822},
  month     = nov,
  issn      = {0001-1452},
  comment   = {doi: 10.2514/1.36043},
  doi       = {10.2514/1.36043},
  groups    = {Input-input space mapping},
  publisher = {American Institute of Aeronautics and Astronautics},
  url       = {https://doi.org/10.2514/1.36043},
}

@Article{TaoSiyu2019,
  author    = {Tao, Siyu and Apley, Daniel W. and Chen, Wei and Garbo, Andrea and Pate, David J. and German, Brian J.},
  journal   = {AIAA Journal},
  title     = {Input Mapping for Model Calibration with Application to Wing Aerodynamics},
  year      = {2019},
  issn      = {0001-1452},
  month     = apr,
  number    = {7},
  pages     = {2734--2745},
  volume    = {57},
  abstract  = {A wide range of model calibration methods and formulas have been introduced in the literature for calibrating low-fidelity (LF) computer models against high-fidelity (HF) computer models or data. Although most existing model calibration techniques assume that LF and HF models have identical inputs, in some engineering applications, for example, wing aerodynamics computations, inputs to LF and HF models are often defined differently due to different levels of abstraction in modeling or simulation. For these problems, this paper proposes a new model calibration method that calibrates a mapping from HF model inputs to LF inputs by matching HF and LF model outputs. The method incorporates regularization to prevent overfitting and to allow calibration parameter selection. In the application to calibrating aerodynamic simulation models, three advantages of the proposed method are demonstrated. First, it achieves higher calibration accuracy than the traditional bias correction method when HF data are scarce. Second, it provides convenient and effective calibration parameter selection in the calibration process. Finally, it enables physical insights to be drawn from the calibrated input mapping. Specifically, for the test cases examined, the calibration adjusts wing twist angles to compensate for the neglect of thickness in the vortex lattice representation of the wing geometry.},
  comment   = {doi: 10.2514/1.J057711},
  doi       = {10.2514/1.J057711},
  groups    = {Input-input space mapping},
  publisher = {American Institute of Aeronautics and Astronautics},
  url       = {https://doi.org/10.2514/1.J057711},
}

@Article{Acar2021,
  author   = {Acar, Erdem and Bayrak, Gamze and Jung, Yongsu and Lee, Ikjin and Ramu, Palaniappan and Ravichandran, Suja Shree},
  title    = {{Modeling, analysis, and optimization under uncertainties: a review}},
  journal  = {Structural and Multidisciplinary Optimization},
  year     = {2021},
  volume   = {64},
  number   = {5},
  pages    = {2909--2945},
  issn     = {1615-1488},
  abstract = {Design optimization of structural and multidisciplinary systems under uncertainty has been an active area of research due to its evident advantages over deterministic design optimization. In deterministic design optimization, the uncertainties of a structural or multidisciplinary system are taken into account by using safety factors specified in the regulations or design codes. This uncertainty treatment is a subjective and indirect way of dealing with uncertainty. On the other hand, design under uncertainty approaches provide an objective and direct way of dealing with uncertainty. This paper provides a review of the uncertainty treatment practices in design optimization of structural and multidisciplinary systems under uncertainties. To this end, the activities in uncertainty modeling are first reviewed, where theories and methods on uncertainty categorization (or classification), uncertainty handling (or management), and uncertainty characterization are discussed. Second, the tools and techniques developed and used for uncertainty modeling and propagation are discussed under the broad two classes of probabilistic and non-probabilistic approaches. Third, various design optimization methods under uncertainty which incorporate all the techniques covered in uncertainty modeling and analysis are reviewed. In addition to these in-depth reviews on uncertainty modeling, uncertainty analysis, and design optimization under uncertainty, some real-life engineering applications and benchmark test examples are provided in this paper so that readers can develop an appreciation on where and how the discussed techniques can be applied and how to compare them. Finally, concluding remarks are provided, and areas for future research are suggested.},
  doi      = {10.1007/s00158-021-03026-7},
  groups   = {Surveys},
  refid    = {Acar2021},
  url      = {https://doi.org/10.1007/s00158-021-03026-7},
}

@Article{Hino2006,
  author   = {Hino, Ryutaro and Yoshida, Fusahito and Toropov, Vassili V.},
  title    = {Optimum blank design for sheet metal forming based on the interaction of high- and low-fidelity {FE} models},
  journal  = {Archive of Applied Mechanics},
  year     = {2006},
  volume   = {75},
  number   = {10},
  pages    = {679--691},
  issn     = {1432-0681},
  abstract = {This paper deals with an optimization methodology for the design of the sheet-metal-forming process. In this study, a new design optimization system is developed which employs an iterative optimization technique and numerical simulation of a sheet-metal-forming process. The main feature of this new optimization method is that it is based on the interaction of high- and low-fidelity simulation models in order to reduce overall computing time. In the iterative optimization procedure, only the corrected low-fidelity model is used. The high-fidelity model, which requires much longer computing time, is used only for the correction of the low-fidelity analysis and validation of the final solution. To demonstrate the developed optimization method on a practical application, it is applied to the optimum blank design for deep-drawing process of a rectangular box. In deep-drawing, the flange of the drawn product is usually trimmed off to obtain the desired product geometry, and the trimmed material is wasted. Therefore, the formulation of the optimization problem is to determine the optimum initial blank geometry which minimizes the amount of the trimmed material, that is, the waste of material. It is confirmed that the blank design was optimized successfully in remarkably short computing time by the developed optimization method.},
  doi      = {10.1007/s00419-006-0047-3},
  groups   = {Multiplicative},
  refid    = {Hino2006},
  url      = {https://doi.org/10.1007/s00419-006-0047-3},
}

@Article{Bandler1994,
  author      = {Bandler, J. W. and Biernacki, R. M. and Chen, Shao Hua and Grobelny, P. A. and Hemmers, R. H.},
  title       = {Space mapping technique for electromagnetic optimization},
  journal     = {IEEE Transactions on Microwave Theory and Techniques},
  year        = {1994},
  volume      = {42},
  number      = {12},
  pages       = {2536--2544},
  issn        = {1557-9670},
  call-number = {42},
  doi         = {10.1109/22.339794},
  groups      = {Input-input space mapping},
}

@Article{Hebbal2021smo,
  author   = {Hebbal, Ali and Brevault, Loïc and Balesdent, Mathieu and Talbi, El-Ghazali and Melab, Nouredine},
  journal  = {Structural and Multidisciplinary Optimization},
  title    = {Multi-fidelity modeling with different input domain definitions using deep {G}aussian processes},
  year     = {2021},
  issn     = {1615-1488},
  number   = {5},
  pages    = {2267--2288},
  volume   = {63},
  abstract = {Multi-fidelity approaches combine different models built on a scarce but accurate dataset (high-fidelity dataset), and a large but approximate one (low-fidelity dataset) in order to improve the prediction accuracy. Gaussian processes (GPs) are one of the popular approaches to exhibit the correlations between these different fidelity levels. Deep Gaussian processes (DGPs) that are functional compositions of GPs have also been adapted to multi-fidelity using the multi-fidelity deep Gaussian process (MF-DGP) model. This model increases the expressive power compared to GPs by considering non-linear correlations between fidelities within a Bayesian framework. However, these multi-fidelity methods consider only the case where the inputs of the different fidelity models are defined over the same domain of definition (e.g., same variables, same dimensions). However, due to simplification in the modeling of the low fidelity, some variables may be omitted or a different parametrization may be used compared to the high-fidelity model. In this paper, deep Gaussian processes for multi-fidelity (MF-DGP) are extended to the case where a different parametrization is used for each fidelity. The performance of the proposed multi-fidelity modeling technique is assessed on analytical test cases and on structural and aerodynamic real physical problems.},
  doi      = {10.1007/s00158-020-02802-1},
  groups   = {Input-input space mapping, Output-output space mapping},
  refid    = {Hebbal2021},
  url      = {https://doi.org/10.1007/s00158-020-02802-1},
}

@Article{Zhou2017aei,
  author   = {Zhou, Qi and Jiang, Ping and Shao, Xinyu and Hu, Jiexiang and Cao, Longchao and Wan, Li},
  journal  = {Advanced Engineering Informatics},
  title    = {A variable fidelity information fusion method based on radial basis function},
  year     = {2017},
  issn     = {1474-0346},
  pages    = {26--39},
  volume   = {32},
  abstract = {Radial basis function (RBF) model has been widely used in complex engineering design process to replace the computational-intensive simulation models. This paper proposes a variable-fidelity metamodeling (VFM) approach based on RBF, in which different levels fidelity information can be integrated and fully exploited. In the proposed VFM approach, a RBF metamodel is constructed for the low-fidelity (LF) model as a start. Then by taking the constructed LF metamodel as a prior-knowledge and mapping the output space of the LF metamodel to that of the studied high-fidelity (HF) model, a variable fidelity (VF) metamodel is created to approximate the relationships between the design variables and corresponding output responses. A numerical illustrative example is adopted to make a detailed comparison between the VFM approach developed in this research and three existing scaling function based VFM approaches, considering different sample sizes and sample noises. Results illustrate that the proposed VFM approach outperforms the scaling function based VFM approaches both in global and local accuracy. Then the proposed VFM approach is applied to two engineering problems, modeling aerodynamic data for a three-dimensional aircraft and the prediction of weld bead profile in laser welding, to illustrate its ability in support of complex engineering design.},
  doi      = {10.1016/j.aei.2016.12.005},
  groups   = {Output-output space mapping},
  keywords = {Variable fidelity, Information fusion, Radial basis function, Simulation-based design, Metamodel},
  url      = {https://www.sciencedirect.com/science/article/pii/S1474034616302750},
}

@Article{Zheng2014,
  author   = {Zheng, Jun and Shao, Xinyu and Gao, Liang and Jiang, Ping and Qiu, Haobo},
  title    = {A prior-knowledge input {LSSVR} metamodeling method with tuning based on cellular particle swarm optimization for engineering design},
  journal  = {Expert Systems with Applications},
  year     = {2014},
  volume   = {41},
  number   = {5},
  pages    = {2111--2125},
  issn     = {0957-4174},
  abstract = {Engineering design is usually a daunting optimization task which often involving time-consuming, even computation-prohibitive process. To reduce the computational expense, metamodels are commonly used to replace the actual expensive simulations or experiments. In this paper, a new and efficient metamodeling method named prior-knowledge input least square support vector regression (PKI-LSSVR) is developed, in which samples from different levels of fidelity are incorporated to gain an accurate approximation with limited times of the high-fidelity (HF) expensive simulations. The low-fidelity (LF) output serves as a prior-knowledge of the real response function, and then is used as the input variables of least square support vector regression (LSSVR). When the corresponding HF response is gained, a function that maps the LF outputs to HF outputs is constructed via LSSVR. The predictive accuracy of LSSVR models is highly dependent on their learning parameters. Therefore, a novel optimization method, cellular particle swarm optimization (CPSO), is exploited to seek the optimal hyper-parameters for PKI-LSSVR in order to improve its generalization capability. To get a better optimization performance, a new neighborhood function is developed for CPSO where the global and local search is efficiently balanced by adaptively varied neighbor radius. Several numerical experiments and one engineering case verify the efficiency of the proposed PKI-LSSVR method. Sample quality merits including sample sizes and noise, and metamodel performance evaluation measures incorporating accuracy, robustness, and efficiency are considered.},
  doi      = {10.1016/j.eswa.2013.09.010},
  groups   = {Output-output space mapping},
  keywords = {Variable fidelity metamodel, Support vector regression, Cellular particle swarm optimization, Design optimization},
  url      = {https://www.sciencedirect.com/science/article/pii/S0957417413007410},
}

@Article{Perdikaris2017,
  author    = {Perdikaris, P. and Raissi, M. and Damianou, A. and Lawrence, N. D. and Karniadakis, G. E.},
  title     = {Nonlinear information fusion algorithms for data-efficient multi-fidelity modelling},
  journal   = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  year      = {2017},
  volume    = {473},
  number    = {2198},
  pages     = {20160751},
  month     = feb,
  comment   = {doi: 10.1098/rspa.2016.0751},
  doi       = {10.1098/rspa.2016.0751},
  groups    = {Output-output space mapping},
  publisher = {Royal Society},
  url       = {https://doi.org/10.1098/rspa.2016.0751},
}

@Article{Jiang2018,
  author   = {Jiang, Ping and Xie, Tingli and Zhou, Qi and Shao, Xinyu and Hu, Jiexiang and Cao, Longchao},
  title    = {A space mapping method based on {G}aussian process model for variable fidelity metamodeling},
  journal  = {Simulation Modelling Practice and Theory},
  year     = {2018},
  volume   = {81},
  pages    = {64--84},
  issn     = {1569-190X},
  abstract = {Computational simulation models with different fidelities are usually available in the design of engineering products for obtaining the quantity of interest (QOI). To integrate and fully exploit variable fidelity information, a space mapping based variable-fidelity metamodeling (VFM) approach is developed in this work. Firstly, a Gaussian process (GP) model is constructed for the low-fidelity (LF) model. Secondly, a variable-fidelity metamodel is constructed by taking the predicted information from this GP model as a prior-knowledge of the QOI and directly mapped into the outputs space of the high-fidelity (HF) model. This space mapping process is performed by constructing another GP model. A mathematic example is first adopted for illustrating how the proposed approach works under different sample sizes and sample noises. Then, the proposed approach is applied to two real-life cases, modeling of the maximum stress for the structure of a Small Waterplane Area Twin Hull (SWATH) catamaran and predicting weld geometry in fiber laser keyhole welding, to illustrate its ability in support of complex engineering design.},
  doi      = {10.1016/j.simpat.2017.11.010},
  groups   = {Output-output space mapping},
  keywords = {Gaussian process model, Simulation-based design, Space mapping, Variable fidelity, Metamodel},
  url      = {https://www.sciencedirect.com/science/article/pii/S1569190X17301715},
}

@Article{Xu2023,
  author   = {Xu, Kaiqin and Shu, Leshi and Zhong, Linjun and Jiang, Ping and Zhou, Qi},
  title    = {A bi-fidelity {B}ayesian optimization method for multi-objective optimization with a novel acquisition function},
  journal  = {Structural and Multidisciplinary Optimization},
  year     = {2023},
  volume   = {66},
  number   = {3},
  pages    = {53},
  issn     = {1615-1488},
  abstract = {In engineering design optimization, there are often multiple conflicting optimization objectives. Bayesian optimization (BO) is successfully applied in solving multi-objective optimization problems to reduce computational expense. However, the expensive expense associated with high-fidelity simulations has not been fully addressed. Combining the BO methods with the bi-fidelity surrogate model can further reduce expense by using the information of samples with different fidelities. In this paper, a bi-fidelity BO method for multi-objective optimization based on lower confidence bound function and the hierarchical Kriging model is proposed. In the proposed method, a novel bi-fidelity acquisition function is developed to guide the optimization process, in which a cost coefficient is adopted to balance the sampling cost and the information provided by the new sample. The proposed method quantifies the effect of samples with different fidelities for improving the quality of the Pareto set and fills the blank of the research domain in extending BO based on the lower confidence bound (LCB) function with bi-fidelity surrogate model for multi-objective optimization. Compared with the four state-of-the-art BO methods, the results show that the proposed method is able to obviously reduce the expense while obtaining high-quality Pareto solutions.},
  doi      = {10.1007/s00158-023-03509-9},
  groups   = {Additive},
  refid    = {Xu2023},
  url      = {https://doi.org/10.1007/s00158-023-03509-9},
}

@Article{Shu2021,
  author   = {Shu, Leshi and Jiang, Ping and Wang, Yan},
  title    = {A multi-fidelity {B}ayesian optimization approach based on the expected further improvement},
  journal  = {Structural and Multidisciplinary Optimization},
  year     = {2021},
  volume   = {63},
  number   = {4},
  pages    = {1709--1719},
  issn     = {1615-1488},
  abstract = {Sampling efficiency is important for simulation-based design optimization. While Bayesian optimization (BO) has been successfully applied in engineering problems, the cost associated with large-scale simulations has not been fully addressed. Extending the standard BO approaches to multi-fidelity optimization can utilize the information of low-fidelity models to further reduce the optimization cost. In this work, a multi-fidelity Bayesian optimization approach is proposed, in which hierarchical Kriging is used for constructing the multi-fidelity metamodel. The proposed approach quantifies the effect of HF and LF samples in multi-fidelity optimization based on a new concept of expected further improvement. A novel acquisition function is proposed to determine both the location and fidelity level of the next sample simultaneously, with the consideration of balance between the value of information provided by the new sample and the associated sampling cost. The proposed approach is compared with some state-of-the-art methods for multi-fidelity global optimization with numerical examples and an engineering case. The results show that the proposed approach can obtain global optimal solutions with reduced computational costs.},
  doi      = {10.1007/s00158-020-02772-4},
  groups   = {Additive, Sequential model-based},
  refid    = {Shu2021},
  url      = {https://doi.org/10.1007/s00158-020-02772-4},
}

@Article{Ribeiro2023,
  author   = {Ribeiro, Leonardo Gonçalves and Parente, Evandro and de Melo, Antônio Macário Cartaxo},
  title    = {Alternative variable-fidelity acquisition functions for efficient global optimization of black-box functions},
  journal  = {Structural and Multidisciplinary Optimization},
  year     = {2023},
  volume   = {66},
  number   = {7},
  pages    = {147},
  issn     = {1615-1488},
  abstract = {Surrogate-Based Optimization has been gaining significant interest in recent years due to its capability of performing optimization of expensive problems using few individual evaluations, thus reducing the computational cost. In that context, multi-fidelity models are able to show higher accuracy by using lower and higher fidelity sources to build the approximate response surface. This allows for a better exploration of the design space while not requiring an excessive number of expensive evaluations. However, for adaptive sampling Surrogate-Based Optimization, there are few robust techniques able to determine the optimal location and fidelity of new data points simultaneously. Also, handling of constrained problems has not yet been extensively explored for Surrogate-Based Optimization using multi-fidelity models. In this work, two alternative variable-fidelity acquisition functions are proposed, namely the Variable-Fidelity Lower Confidence Bound (VF-LCB) and the Variable-Fidelity Weighted Expected Improvement (VF-WEI). Constraint-handling methods which have been proposed for single-fidelity models are also extended to multi-fidelity model-based optimization. State-of-the-art modeling techniques will be used and compared, namely Co-Kriging and Hierarchical Kriging, as well as the popular single-fidelity Kriging model. We also show that Co-Kriging is not able to provide a smooth approximation for the HF source when sources are related by a constant additive term. Whenever possible, results found in this paper are to those from the literature in terms of accuracy and efficiency.},
  doi      = {10.1007/s00158-023-03607-8},
  groups   = {Opti Algorithms, Additive, Sequential model-based},
  refid    = {Ribeiro2023},
  url      = {https://doi.org/10.1007/s00158-023-03607-8},
}

@Article{Gano2006b,
  author   = {Gano, Shawn E. and Renaud, John E. and Martin, Jay D. and Simpson, Timothy W.},
  title    = {Update strategies for kriging models used in variable fidelity optimization},
  journal  = {Structural and Multidisciplinary Optimization},
  year     = {2006},
  volume   = {32},
  number   = {4},
  pages    = {287--298},
  issn     = {1615-1488},
  abstract = {Many optimization methods for simulation-based design rely on the sequential use of metamodels to reduce the associated computational burden. In particular, kriging models are frequently used in variable fidelity optimization. Nevertheless, such methods may become computationally inefficient when solving problems with large numbers of design variables and/or sampled data points due to the expensive process of optimizing the kriging model parameters in each iteration. One solution to this problem would be to replace the kriging models with traditional Taylor series response surface models. Kriging models, however, were shown to provide good approximations of computer simulations that incorporate larger amounts of data, resulting in better global accuracy. In this paper, a metamodel update management scheme (MUMS) is proposed to reduce the cost of using kriging models sequentially by updating the kriging model parameters only when they produce a poor approximation. The scheme uses the trust region ratio (TR-MUMS), which is a ratio that compares the approximation to the true model. Two demonstration problems are used to evaluate the proposed method: an internal combustion engine sizing problem and a control-augmented structural design problem. The results indicate that the TR-MUMS approach is very effective; on the demonstration problems, it reduced the number of likelihood evaluations by three orders of magnitude compared to using a global optimizer to find the kriging parameters in every iteration. It was also found that in trust region-based method, the kriging model parameters need not be updated using a global optimizer--local methods perform just as well in terms of providing a good approximation without affecting the overall convergence rate, which, in turn, results in a faster execution time.},
  doi      = {10.1007/s00158-006-0025-y},
  groups   = {Additive},
  refid    = {Gano2006},
  url      = {https://doi.org/10.1007/s00158-006-0025-y},
}

@Article{Durantin2017,
  author   = {Durantin, Cédric and Rouxel, Justin and Désidéri, Jean-Antoine and Glière, Alain},
  title    = {Multifidelity surrogate modeling based on radial basis functions},
  journal  = {Structural and Multidisciplinary Optimization},
  year     = {2017},
  volume   = {56},
  number   = {5},
  pages    = {1061--1075},
  issn     = {1615-1488},
  abstract = {Multiple models of a physical phenomenon are sometimes available with different levels of approximation. The high fidelity model is more computationally demanding than the coarse approximation. In this context, including information from the lower fidelity model to build a surrogate model is desirable. Here, the study focuses on the design of a miniaturized photoacoustic gas sensor which involves two numerical models. First, a multifidelity metamodeling method based on Radial Basis Function, the co-RBF, is proposed. This surrogate model is compared with the classical co-kriging method on two analytical benchmarks and on the photoacoustic gas sensor. Then an extension to the multifidelity framework of an already existing RBF-based optimization algorithm is applied to optimize the sensor efficiency. The co-RBF method does not bring better results than co-kriging but can be considered as an alternative for multifidelity metamodeling.},
  doi      = {10.1007/s00158-017-1703-7},
  groups   = {MF modeling, Additive},
  refid    = {Durantin2017},
  url      = {https://doi.org/10.1007/s00158-017-1703-7},
}

@Book{Santner2018,
  author    = {Santner, T. J. and Williams, B. J. and Notz, W. I.},
  publisher = {Springer},
  title     = {{The design and analysis of computer experiments}},
  year      = {2018},
  address   = {New York, USA},
  edition   = {2nd},
  isbn      = {9781493988471},
  doi       = {10.1007/978-1-4939-8847-1},
  groups    = {Books},
  url       = {https://doi.org/10.1007/978-1-4939-8847-1},
}

@Book{Rasmussen2006,
  title     = {Gaussian processes for machine learning},
  publisher = {The MIT Press},
  year      = {2006},
  author    = {Rasmussen, Carl Edward and Williams, Christopher K I},
  address   = {Massachusetts, USA},
  isbn      = {9780521872508},
  abstract  = {Modern statistical methods use complex, sophisticated models that can lead to intractable computations. Saddlepoint approximations can be the answer. Written from the user's point of view, this book explains in clear language how such approximate probability computations are made, taking readers from the very beginnings to current applications. The core material is presented in chapters 1-6 at an elementary mathematical level. Chapters 7-9 then give a highly readable account of higher-order asymptotic inference. Later chapters address areas where saddlepoint methods have had substantial impact: multivariate testing, stochastic systems and applied probability, bootstrap implementation in the transform domain, and Bayesian computation and inference. No previous background in the area is required. Data examples from real applications demonstrate the practical value of the methods. Ideal for graduate students and researchers in statistics, biostatistics, electrical engineering, econometrics, and applied mathematics, this is both an entry-level text and a valuable reference.},
  doi       = {10.7551/mitpress/3206.001.0001},
  groups    = {Books},
  url       = {https://doi.org/10.7551/mitpress/3206.001.0001},
}

@Article{Teichert2019,
  author   = {Teichert, Gregory H. and Garikipati, Krishna},
  title    = {Machine learning materials physics: Surrogate optimization and multi-fidelity algorithms predict precipitate morphology in an alternative to phase field dynamics},
  journal  = {Computer Methods in Applied Mechanics and Engineering},
  year     = {2019},
  volume   = {344},
  pages    = {666--693},
  issn     = {0045-7825},
  abstract = {Machine learning has been effective at detecting patterns and predicting the response of systems that behave free of natural laws. Examples include learning crowd dynamics, recommender systems and autonomous mobility. There also have been applications to the search for new materials that draw upon big-data classification problems. However, when it comes to physical systems governed by conservation laws, the role of machine learning has been more limited. Here, we present our recent work in exploring the role of machine learning methods in discovering, or aiding, the search for physics. Specifically, we focus on using machine learning algorithms to represent high-dimensional free energy surfaces with the goal of identifying precipitate morphologies in alloy systems. Traditionally, this problem has been approached by combining phase field models, which impose first-order dynamics, with elasticity, to traverse a free energy landscape in search of minima. Equilibrium precipitate morphologies occur at these minima. Here, we exploit the machine learning methods to represent high-dimensional data, combined with surrogate optimization, sensitivity analysis and multifidelity modeling as an alternate framework to explore phenomena controlled by energy extremization. This combination of data-driven methods offers an alternative to the imposition of first-order dynamics via phase field methods, and represents one aspect of applying machine learning to materials physics.},
  doi      = {10.1016/j.cma.2018.10.025},
  groups   = {Additive},
  keywords = {Deep Neural Networks, Mechanochemistry, Phase field, Heterogeneous computing},
  url      = {https://www.sciencedirect.com/science/article/pii/S0045782518305292},
}

@Article{Li2023,
  author   = {Li, Kunpeng and Li, Qingye and Lv, Liye and Song, Xueguan and Ma, Yunsheng and Lee, Ikjin},
  title    = {A nonlinearity integrated bi-fidelity surrogate model based on nonlinear mapping},
  journal  = {Structural and Multidisciplinary Optimization},
  year     = {2023},
  volume   = {66},
  number   = {9},
  pages    = {196},
  issn     = {1615-1488},
  abstract = {The variable-fidelity surrogate (VFS) modeling technique is a data fusion method used to enhance the prediction accuracy of less intensively sampled primary quantities of interest (i.e., high-fidelity samples) by incorporating a large number of auxiliary samples (i.e., low-fidelity samples). However, the VFS model constructed based on the work of Kennedy and O’Hagan overemphasizes the linear correlations between high-fidelity and low-fidelity models, thereby limiting the generalizability and application scenarios of VFS models. To address this issue, this study proposes a nonlinear integrated bi-fidelity (NI-BFS) model, which maps predictions of the low-fidelity model to the high-fidelity level in a nonlinear manner. This approach strengthens the model’s ability to learn the nonlinear correlation relationship between high-fidelity and low-fidelity models and alleviates the difficulty of fitting the discrepancy function. The performance of the NI-BFS model has been validated through a series of comparative experiments, where four advanced VFS models were used as benchmark models. Additionally, the NI-BFS model’s robustness and practical applicability have been investigated. The results demonstrate that the NI-BFS model outperforms the other benchmark models in all cases.},
  doi      = {10.1007/s00158-023-03633-6},
  groups   = {Output-output space mapping},
  refid    = {Li2023},
  url      = {https://doi.org/10.1007/s00158-023-03633-6},
}

@Article{Serani2019,
  author    = {Serani, A. and Pellegrini, R. and Wackers, J. and Jeanson, C.-E. and Queutey, P. and Visonneau, M. and Diez, M.},
  title     = {Adaptive multi-fidelity sampling for {CFD}-based optimisation via radial basis function metamodels},
  journal   = {International Journal of Computational Fluid Dynamics},
  year      = {2019},
  volume    = {33},
  number    = {6-7},
  pages     = {237--255},
  month     = aug,
  issn      = {1061-8562},
  comment   = {doi: 10.1080/10618562.2019.1683164},
  doi       = {10.1080/10618562.2019.1683164},
  groups    = {Additive},
  publisher = {Taylor \& Francis},
  url       = {https://doi.org/10.1080/10618562.2019.1683164},
}

@Article{Bryson2017,
  author   = {Bryson, Dean E. and Rumpfkeil, Markus P.},
  title    = {All-at-once approach to multifidelity polynomial chaos expansion surrogate modeling},
  journal  = {Aerospace Science and Technology},
  year     = {2017},
  volume   = {70},
  pages    = {121--136},
  issn     = {1270-9638},
  abstract = {A new approach to multifidelity, gradient-enhanced surrogate modeling using polynomial chaos expansions is presented. This approach seeks complementary additive and multiplicative corrections to low-fidelity data whereas current hybrid methods in the literature attempt to balance individually calculated calibrations. An advantage of the new approach is that least squares-optimal coefficients for both corrections and the model of interest are determined simultaneously using the high-fidelity data directly in the final surrogate. The proposed technique is compared to the weighted approach for three analytic functions and the numerical simulation of a vehicle's lift coefficient using Cartesian Euler CFD and panel aerodynamics. Investigation of the individual correction terms indicates the advantage of the proposed approach is that complementary calibrations separately adjust the low-fidelity data in local regions based on agreement or disagreement between the two fidelities. In cases where polynomials are suitable approximations to the true function, the new all-at-once approach is found to reduce error in the surrogate faster than the method of weighted combinations. When the low-fidelity is a good approximation of the true function, the proposed technique out-performs monofidelity approximations as well. Sparse grid constructions alleviate the growth of the training set as root-mean-square-error is calculated for increasingly higher polynomial orders. Utilizing gradient information provides an advantage at lower training grid levels for low-dimensional spaces, but worsens numerical conditioning of the system in higher dimensions.},
  doi      = {10.1016/j.ast.2017.07.043},
  groups   = {Hybrid},
  keywords = {Multifidelity, Multi-fidelity, Variable fidelity, Polynomial chaos, Surrogate modeling},
  url      = {https://www.sciencedirect.com/science/article/pii/S1270963816303844},
}

@Book{Arora2016,
  author    = {Arora, Jasbir Singh},
  publisher = {Academic Press},
  title     = {Introduction to optimum design},
  year      = {2016},
  address   = {Massachusetts, USA},
  edition   = {4th},
  isbn      = {0080470254},
  groups    = {Books},
}

@InCollection{Alexandrov2000,
  author    = {Alexandrov, N. and Nielsen, E. and Lewis, R. and Anderson, W.},
  title     = {First-order model management with variable-fidelity physics applied to multi-element airfoil optimization},
  booktitle = {The 8th Symposium on Multidisciplinary Analysis and Optimization},
  publisher = {American Institute of Aeronautics and Astronautics},
  year      = {2000},
  number    = {4886},
  series    = {Multidisciplinary Analysis Optimization Conferences},
  month     = sep,
  comment   = {doi:10.2514/6.2000-4886},
  doi       = {10.2514/6.2000-4886},
  groups    = {Gradient-based},
  journal   = {8th Symposium on Multidisciplinary Analysis and Optimization},
  url       = {https://doi.org/10.2514/6.2000-4886},
}

@Article{Bryson2018,
  author    = {Bryson, Dean E. and Rumpfkeil, Markus P.},
  title     = {Multifidelity Quasi-{N}ewton Method for Design Optimization},
  journal   = {AIAA Journal},
  year      = {2018},
  volume    = {56},
  number    = {10},
  pages     = {4074--4086},
  month     = jul,
  issn      = {0001-1452},
  abstract  = {Multifidelity approaches are frequently used in design when high-fidelity models are too expensive to use directly and lower-fidelity models of reasonable accuracy exist. In optimization, corrected low-fidelity data are typically used in a sequence of independent, approximate optimizations bounded by trust regions. A new, unified, multifidelity quasi-Newton approach is presented that preserves an approximate inverse Hessian between iterations, determines search directions from high-fidelity data, and uses low-fidelity models for line searches. The proposed algorithm produces better search directions, maintains larger step sizes, and requires significantly fewer low-fidelity function evaluations than Trust Region Model Management. The multifidelity quasi-Newton method also provides an expected optimal point that is forward looking and is useful in building superior low-fidelity corrections. The new approach is compared with Trust Region Model Management and the BFGS quasi-Newton method on several analytic test problems using polynomial and kriging corrections. For comparison, a technique is demonstrated to initialize high-fidelity optimization when transition away from approximate models is deemed fruitful. In summary, the unified multifidelity quasi-Newton approach required fewer or equal high-fidelity function evaluations than Trust Region Model Management in about two-thirds of the test cases, and similarly reduced cost in more than half of cases compared with BFGS.},
  comment   = {doi: 10.2514/1.J056840},
  doi       = {10.2514/1.J056840},
  groups    = {Gradient-based},
  publisher = {American Institute of Aeronautics and Astronautics},
  url       = {https://doi.org/10.2514/1.J056840},
}

@Article{Elham2015,
  author   = {Elham, Ali},
  title    = {Adjoint quasi-three-dimensional aerodynamic solver for multi-fidelity wing aerodynamic shape optimization},
  journal  = {Aerospace Science and Technology},
  year     = {2015},
  volume   = {41},
  pages    = {241--249},
  issn     = {1270-9638},
  abstract = {A quasi-three-dimensional method for wing aerodynamic analysis for drag prediction is presented. This method can predict the wing drag with a level of accuracy similar to higher fidelity three-dimensional CFD analysis, with a much lower computational cost. A tool has been developed based on the proposed method and the outputs of the tool have been validated using a higher fidelity CFD tool. Another advantage of the mentioned method (and the tool developed based on that) is to compute the derivatives of any function of interest, such as the wing drag, lift, or pitching moment, with respect to the design variables, mainly the wing geometry, using analytical methods. The tool uses a combination of the Adjoint method, the chain rule for differentiation, and the automatic differentiation to compute the sensitivities. The quasi-three-dimensional aerodynamic solver is used for a multi-fidelity wing aerodynamic shape optimization. A trust region algorithm is used to connect the low fidelity aerodynamic solver to a high fidelity CFD tool for wing drag prediction. The derivatives of the objective function are computed using the low fidelity solver, and the high fidelity solver is used to calibrate the results of the low fidelity one.},
  doi      = {10.1016/j.ast.2014.12.024},
  groups   = {Gradient-based},
  keywords = {Wing aerodynamic optimization, Adjoint derivative computation, Quasi-three-dimensional aerodynamic solver, Trust region algorithm},
  url      = {https://www.sciencedirect.com/science/article/pii/S1270963814002776},
}

@Article{WuN2022,
  author   = {Wu, Neil and Mader, Charles A. and Martins, Joaquim R. R. A.},
  journal  = {Structural and Multidisciplinary Optimization},
  title    = {A Gradient-based Sequential Multifidelity Approach to Multidisciplinary Design Optimization},
  year     = {2022},
  issn     = {1615-1488},
  number   = {4},
  pages    = {131},
  volume   = {65},
  abstract = {Multifidelity design optimization is a strategy that can reduce the high computational cost in cases where the high-fidelity model is too expensive to use directly in optimization. However, current multifidelity approaches cannot handle the high-dimensional problems commonly encountered in industrial settings. Furthermore, they cannot accommodate arbitrary analysis fidelities, directly handle multidisciplinary problems, or provably converge to the high-fidelity optimum. In this paper, we present a practical multifidelity approach that leverages the advantages of conventional gradient-based approaches. Rather than constructing a multifidelity surrogate, we perform a sequence of single-fidelity gradient-based optimizations. The framework determines the appropriate fidelity and updates it during the optimization process. Finally, we demonstrate the proposed approach on a multipoint aerostructural wing optimization problem with over a hundred design variables. The multifidelity approach reduces the computational cost by 59% compared to the high-fidelity approach while obtaining the same numerical optimum.},
  doi      = {10.1007/s00158-022-03204-1},
  groups   = {Gradient-based},
  refid    = {Wu2022},
  url      = {https://doi.org/10.1007/s00158-022-03204-1},
}

@Article{ZhangT2023,
  author   = {Zhang, Tao and Barakos, George N. and Furqan and Foster, Malcolm},
  journal  = {Aerospace Science and Technology},
  title    = {Multi-fidelity aerodynamic design and analysis of propellers for a heavy-lift e{VTOL}},
  year     = {2023},
  issn     = {1270-9638},
  pages    = {108185},
  volume   = {135},
  abstract = {This paper presents a multi-fidelity propeller design and analysis process, and demonstrates it for the preliminary design of a heavy-lift eVTOL vehicle proposed by GKN aerospace, known as Skybus. The multi-fidelity framework integrates tools and results of variable fidelity levels. A global scan of the design space was first carried out at the low-fidelity stage. The low-fidelity output was converted to 3D shapes and grids through an automatic meshing tool, and high-fidelity CFD simulations and gradient-based optimisation were launched to deliver further improved designs. The method was first verified using a benchmark multi-modal test function. The framework was then demonstrated for the propeller design of the Skybus vehicle. Rectangular blade designs with linear twist were first derived through the Blade Element Momentum Theory and used as the input to the low-fidelity stage for simplicity. Upon the baseline design, the low-fidelity stage managed to find an improved shape with 3% reduced power, while the high-fidelity optimisation further reduced the power by 2.2% under the equality thrust constraint. In addition, this work also reports the propeller pitch-RPM performance map tool that has effectively supported the Skybus development through fast performance predictions and operating condition determination. For propellers with both pitch and RPM regulation, the performance map explicitly correlates various performance scopes and to aid optimisation. The multi-fidelity construction of the performance map and demonstrations of its usage are then presented.},
  doi      = {10.1016/j.ast.2023.108185},
  groups   = {Gradient-based, Sequential model-based},
  url      = {https://www.sciencedirect.com/science/article/pii/S1270963823000822},
}

@Article{Kontogiannis2020,
  author   = {Kontogiannis, Spyridon G. and Demange, Jean and Savill, A. Mark and Kipouros, Timoleon},
  title    = {A comparison study of two multifidelity methods for aerodynamic optimization},
  journal  = {Aerospace Science and Technology},
  year     = {2020},
  volume   = {97},
  pages    = {105592},
  issn     = {1270-9638},
  abstract = {Industrial aerodynamic design applications require multiobjective optimization tools able to provide design feedback to the engineers. This is true especially when optimization studies are carried out during the conceptual design stage. The need for fast optimization methods has led to the development of multifidelity methods in a surrogate based optimization environment. Multifidelity tools have the potential to accelerate the design process, primarily due to the lower cost associated with the low fidelity tool. In addition to this, the design stage is shortened as mature and reliable high fidelity design information is provided earlier in the design cycle. Despite this high potential of these methods, there is no explicit comparison available in the literature between multifidelity surrogate based optimization tools for industrial aerodynamic problems. This paper aims at providing a direct comparison between two multiobjective multifidelity surrogate based optimization methods developed by our group. The first approach uses a trust region formulation for efficient multiobjective that does not require gradients. The second is using the concept of expected improvement to perform fast design space exploration based on a novel Kriging modification for multifidelity data. The tools are applied in two aerodynamic design problems: optimization of a high lift configuration in respect to maximum lift maximization and an airfoil design for transonic cruising conditions. These problems feature characteristics of industrial interest. They involve difficult physical analyses in the case of the high lift configuration and a more complex optimization formulation due to the increased dimensionality in the case of the transonic airfoil. Our presented methods are compared against a CFD-based optimization, a surrogate based optimization using only high fidelity data and a multifidelity surrogate based optimization based on Co-Kriging. Early results suggest that the trust region method can quickly provide improved designs leading to an efficient Pareto front. The expected improvement based method shows fast exploration attributes and a wide Pareto front.},
  doi      = {10.1016/j.ast.2019.105592},
  groups   = {Gradient-based},
  url      = {https://www.sciencedirect.com/science/article/pii/S1270963819319674},
}

@Article{Leusink2015,
  author   = {Leusink, Debbie and Alfano, David and Cinnella, Paola},
  title    = {Multi-fidelity optimization strategy for the industrial aerodynamic design of helicopter rotor blades},
  journal  = {Aerospace Science and Technology},
  year     = {2015},
  volume   = {42},
  pages    = {136--147},
  issn     = {1270-9638},
  abstract = {The industrial aerodynamic design of helicopter rotor blades needs to consider simultaneously the two typical flight conditions of hover and forward flight. In the present work this multi-objective design problem is tackled using a genetic algorithm, coupled to rotor performance simulation tools. The turn-around time of an optimization loop is acceptable in an industrial design loop when using low-cost, low-fidelity tools such as the comprehensive rotorcraft code HOST, but becomes excessively high when employing high-fidelity models like CFD methods. To incorporate high-fidelity models into the optimization loop while maintaining a moderate computational cost, a Multi-Fidelity Optimization (MFO) strategy is proposed: as a preliminary step, a HOST-based genetic algorithm optimization is used to reduce the parameter space and select a set of blade geometries used to initialize the high-fidelity stage. Secondly, the selected blades are re-evaluated by CFD and used to construct a high-fidelity surrogate model. Finally, a Surrogate Based Optimization (SBO) is carried out and the Pareto optimal individuals according to the SBO are recomputed by CFD for final performance evaluation. The proposed strategy is validated step by step. It is shown that an industrially acceptable number of CFD-simulations is sufficient to obtain blade designs with significantly higher performances than the baseline and SBO results issued from a standard Latin-Hypercube-Sampling initialization. The proposed MFO strategy represents an efficient method for the simultaneous optimization of rotor blade geometries in hover and forward flight.},
  doi      = {10.1016/j.ast.2015.01.005},
  groups   = {Model-then-optimize},
  keywords = {Aerodynamics, Optimization, Helicopter rotor blades},
  url      = {https://www.sciencedirect.com/science/article/pii/S1270963815000206},
}

@Article{Singh2017,
  author   = {Singh, Prashant and Couckuyt, Ivo and Elsayed, Khairy and Deschrijver, Dirk and Dhaene, Tom},
  title    = {Multi-objective Geometry Optimization of a Gas Cyclone Using Triple-Fidelity Co-{K}riging Surrogate Models},
  journal  = {Journal of Optimization Theory and Applications},
  year     = {2017},
  volume   = {175},
  number   = {1},
  pages    = {172--193},
  issn     = {1573-2878},
  abstract = {Cyclone separators are widely used in a variety of industrial applications. A low-mass loading gas cyclone is characterized by two performance parameters, namely the Euler and Stokes numbers. These parameters are highly sensitive to the geometrical design parameters defining the cyclone. Optimizing the cyclone geometry therefore is a complex problem. Testing a large number of cyclone geometries is impractical due to time constraints. Experimental data and even computational fluid dynamics simulations are time-consuming to perform, with a single simulation or experiment taking several weeks. Simpler analytical models are therefore often used to expedite the design process. However, this comes at the cost of model accuracy. Existing techniques used for cyclone shape optimization in literature do not take multiple fidelities into account. This work combines cheap-to-evaluate well-known mathematical models of cyclones, available data from computational fluid dynamics simulations and experimental data to build a triple-fidelity recursive co-Kriging model. This model can be used as a surrogate with a multi-objective optimization algorithm to identify a Pareto set of a finite number of solutions. The proposed scheme is applied to optimize the cyclone geometry, parametrized by seven design variables.},
  doi      = {10.1007/s10957-017-1114-3},
  groups   = {Model-then-optimize},
  refid    = {Singh2017},
  url      = {https://doi.org/10.1007/s10957-017-1114-3},
}

@Article{Yang2018,
  author   = {Yang, Yang and Gao, Zhongmei and Cao, Longchao},
  title    = {Identifying optimal process parameters in deep penetration laser welding by adopting Hierarchical-Kriging model},
  journal  = {Infrared Physics {\&} Technology},
  year     = {2018},
  volume   = {92},
  pages    = {443--453},
  issn     = {1350-4495},
  abstract = {The quality of welding joints is largely dependent on the laser welding process parameters. In this work, an integrated optimization methodology by combining the Hierarchical-Kriging model and non-dominated sorting genetic algorithm II (NSGA-II) is developed for identifying the optimal process parameters in deep penetration laser welding. Firstly, a three-dimensional thermo-mechanical finite element model is developed as a low-fidelity (LF) model, while the laser welding experiment is taken as a high-fidelity (HF) model. Then, the data sets from these two different levels fidelity models are integrated by Hierarchical-Kriging model to build the relationships between welding process parameters and bead profile and angular distortion. Secondly, the NSGA-II algorithm is employed to obtain the multi-objective Pareto optimal solutions based on the constructed Hierarchical-Kriging model. Finally, the effectiveness and reliability of the obtained optimum are verified by laser welding experiments. Results illustrate that the developed integrated optimal method provides a promising way to identify favorable process parameters for generating a desirable bead profile and reducing the angular distortion in deep penetration laser welding.},
  doi      = {10.1016/j.infrared.2018.07.006},
  groups   = {Model-then-optimize},
  keywords = {Laser welding, Finite element model, Hierarchical-Kriging model, Bead profile, Angular distortion},
  url      = {https://www.sciencedirect.com/science/article/pii/S1350449517307697},
}

@Article{Kishi2022,
  author    = {Kishi, Yuki and Kanazaki, Masahiro and Makino, Yoshikazu},
  title     = {Supersonic Forward-Swept Wing Design Using Multifidelity Efficient Global Optimization},
  journal   = {Journal of Aircraft},
  year      = {2022},
  volume    = {59},
  number    = {4},
  pages     = {1027--1040},
  month     = feb,
  issn      = {0021-8669},
  abstract  = {In this paper, the ability of the forward-swept supersonic wing to simultaneously reduce aerodynamic drag and sonic boom under cruise conditions is investigated via the optimization of airfoil distribution. The forward-swept wing is superior to the conventional backward-swept delta wing for reducing sonic booms. To realize optimum aerodynamic characteristics, airfoil distributions for the forward-swept and backward-swept wings were acquired and compared for low-drag, low-boom wing abilities. For sonic boom evaluations, the augmented Burgers equation and multipole analysis were applied to near-field pressure distributions calculated by Euler simulations of both configuration samples. This process was time-consuming, so a multifidelity approach was introduced with a multi-additional sampling. Low-drag and low-boom solutions were obtained for both planforms; the forward-swept wing was found to reduce sonic boom and aerodynamic drag more efficiently than the backward-swept wing. Based on functional analysis of variance, different design variables were noted to contribute toward reductions of the various objective functions. In case of sonic boom reduction via comparison of the low-boom solutions, for the given airfoil geometries at the wing tips for the forward-swept and backward-swept wings, the former had a larger twisted-down angle and the latter had a stronger camber shape.},
  comment   = {doi: 10.2514/1.C036422},
  doi       = {10.2514/1.C036422},
  groups    = {Sequential model-based},
  publisher = {American Institute of Aeronautics and Astronautics},
  url       = {https://doi.org/10.2514/1.C036422},
}

@Article{ZhangX2021,
  author   = {Zhang, Xinshuai and Xie, Fangfang and Ji, Tingwei and Zhu, Zaoxu and Zheng, Yao},
  journal  = {Computer Methods in Applied Mechanics and Engineering},
  title    = {Multi-fidelity deep neural network surrogate model for aerodynamic shape optimization},
  year     = {2021},
  issn     = {0045-7825},
  pages    = {113485},
  volume   = {373},
  abstract = {In the present study, an effective optimization framework of aerodynamic shape design is established based on the multi-fidelity deep neural network (MFDNN) model. The objective of the current work is to construct a high-accuracy multi-fidelity surrogate model correlating the configuration parameters of an aircraft and its aerodynamic performance by blending different fidelity information and adaptively learning their linear or nonlinear correlation without any prior assumption. In the optimization framework, the high-fidelity model using a CFD evaluation with fine grid and the low-fidelity model using the same CFD model with coarse grid are applied. Moreover, in each optimization iteration, the high-fidelity infilling strategy by adding the current optimal solution of surrogate model into the high-fidelity database is applied to improve the surrogate accuracy. The low-fidelity infilling strategy which can generate the solutions distributed uniformly in the whole design space is used to update the low-fidelity database for avoiding local optimum. Then, the proposed multi-fidelity optimization framework is validated by two standard synthetic benchmarks. Finally, it is applied to the high-dimensional aerodynamic shape optimization of a RAE2822 airfoil parameterized by 10 design variables and a DLR-F4 wing-body configuration parameterized by 30 design variables. The optimization results demonstrate that the proposed multi-fidelity optimization framework can remarkably improve optimization efficiency and outperform the single-fidelity method.},
  doi      = {10.1016/j.cma.2020.113485},
  groups   = {Sequential model-based},
  keywords = {Multi-fidelity, Surrogate model, Deep neural network, Aerodynamic shape optimization},
  url      = {https://www.sciencedirect.com/science/article/pii/S0045782520306708},
}

@Article{Tran2020jcp,
  author   = {Tran, Anh and Tranchida, Julien and Wildey, Tim and Thompson, Aidan P.},
  journal  = {The Journal of Chemical Physics},
  title    = {Multi-fidelity machine-learning with uncertainty quantification and {B}ayesian optimization for materials design: Application to ternary random alloys},
  year     = {2020},
  issn     = {0021-9606},
  month    = aug,
  number   = {7},
  pages    = {074705},
  volume   = {153},
  abstract = {We present a scale-bridging approach based on a multi-fidelity (MF) machine-learning (ML) framework leveraging Gaussian processes (GP) to fuse atomistic computational model predictions across multiple levels of fidelity. Through the posterior variance of the MFGP, our framework naturally enables uncertainty quantification, providing estimates of confidence in the predictions. We used density functional theory as high-fidelity prediction, while a ML interatomic potential is used as low-fidelity prediction. Practical materials’ design efficiency is demonstrated by reproducing the ternary composition dependence of a quantity of interest (bulk modulus) across the full aluminum-niobium-titanium ternary random alloy composition space. The MFGP is then coupled to a Bayesian optimization procedure, and the computational efficiency of this approach is demonstrated by performing an on-the-fly search for the global optimum of bulk modulus in the ternary composition space. The framework presented in this manuscript is the first application of MFGP to atomistic materials simulations fusing predictions between density functional theory and classical interatomic potential calculations.},
  doi      = {10.1063/5.0015672},
  groups   = {Sequential model-based},
  url      = {https://doi.org/10.1063/5.0015672},
}

@Article{Foumani2023,
  author   = {Foumani, Zanjani Zahra and Shishehbor, Mehdi and Yousefpour, Amin and Bostanabad, Ramin},
  title    = {Multi-fidelity cost-aware {B}ayesian optimization},
  journal  = {Computer Methods in Applied Mechanics and Engineering},
  year     = {2023},
  volume   = {407},
  pages    = {115937},
  issn     = {0045-7825},
  abstract = {Bayesian optimization (BO) is increasingly employed in critical applications such as materials design and drug discovery. An increasingly popular strategy in BO is to forgo the sole reliance on high-fidelity data and instead use an ensemble of information sources which provide inexpensive low-fidelity data. The overall premise of this strategy is to reduce the total sampling costs by querying inexpensive low-fidelity sources whose data are correlated with high-fidelity samples. Here, we propose a multi-fidelity cost-aware BO framework that dramatically outperforms the state-of-the-art technologies in terms of efficiency, consistency, and robustness. We demonstrate the advantages of our framework on analytic and engineering problems and argue that these benefits stem from our two main contributions: (1) we develop a novel acquisition function for multi-fidelity cost-aware BO that safeguards the convergence against the biases of low-fidelity data, and (2) we tailor a newly developed emulator for multi-fidelity BO which enables us to not only simultaneously learn from an ensemble of multi-fidelity datasets, but also identify the severely biased low-fidelity sources that should be excluded from BO.},
  doi      = {10.1016/j.cma.2023.115937},
  groups   = {Sequential model-based},
  keywords = {Bayesian optimization, Multi-fidelity modeling, Emulation, Resource allocation, Manifold learning, Gaussian process},
  url      = {https://www.sciencedirect.com/science/article/pii/S0045782523000609},
}

@Article{Tran2020cise,
  author   = {Tran, Anh and Wildey, Tim and McCann, Scott},
  journal  = {Journal of Computing and Information Science in Engineering},
  title    = {{sMF-BO-2CoGP: A sequential multi-fidelity constrained Bayesian optimization framework for design applications}},
  year     = {2020},
  issn     = {1530-9827},
  month    = apr,
  number   = {3},
  volume   = {20},
  abstract = {Bayesian optimization (BO) is an efiective surrogate-based method that has been widely used to optimize simulation-based applications. While the traditional Bayesian optimization approach only applies to single-fidelity models, many realistic applications provide multiple levels of fidelity with various computational complexity and predictive capability. In this work, we propose a multi-fidelity Bayesian optimization method for design applications with both known and unknown constraints. The proposed framework, called sMF-BO-2CoGP, is built on a multi-level CoKriging method to predict the objective function. An external binary classifier, which we approximate using a separate CoKriging model, is used to distinguish between feasible and infeasible regions. The sMF-BO-2CoGP method is demonstrated using a series of analytical examples, and a fiip-chip application for design optimization to minimize the deformation due to warping under thermal loading conditions.},
  doi      = {10.1115/1.4046697},
  groups   = {Sequential model-based},
  url      = {https://doi.org/10.1115/1.4046697},
}

@Article{Shi2020,
  author   = {Shi, Renhe and Liu, Li and Long, Teng and Wu, Yufei and Gary Wang, G.},
  title    = {Multi-Fidelity Modeling and Adaptive Co-{K}riging-Based Optimization for All-Electric Geostationary Orbit Satellite Systems},
  journal  = {Journal of Mechanical Design},
  year     = {2020},
  volume   = {142},
  number   = {2},
  month    = oct,
  issn     = {1050-0472},
  abstract = {All-electric geostationary orbit (GEO) satellite systems design is a challenging multidisciplinary design optimization (MDO) problem, which is computation-intensive due to the employment of expensive simulations. In this paper, the all-electric GEO satellite MDO problem with multi-fidelity models is investigated. The MDO problem involving six inter-coupled disciplines is formulated to minimize the total mass of the satellite system subject to a number of engineering constraints. To reduce the computational cost of the multidisciplinary analysis (MDA) process, multi-fidelity transfer dynamics models and finite element analysis (FEA) models are developed for the geosynchronous transfer orbit (GTO) and structure disciplines, respectively. To effectively solve the all-electric GEO satellite MDO problem using multi-fidelity models, an adaptive Co-Kriging-based optimization framework is proposed. In this framework, the samples from a high-fidelity MDA process are integrated with those from a low-fidelity MDA process to create a Co-Kriging metamodel with a moderate computational cost for optimization. Besides, for refining the Co-Kriging metamodels, a multi-objective adaptive infill sampling approach is developed to produce the infill sample points in terms of the expected improvement (EI) and the probability of feasibility (PF) functions. Optimization results show that the proposed optimization framework can significantly reduce the total mass of satellite system with a limited computational budget, which demonstrates the effectiveness and practicality of the multi-fidelity modeling and adaptive Co-Kriging-based optimization framework for all-electric GEO satellite systems design.},
  doi      = {10.1115/1.4044321},
  groups   = {Sequential model-based},
  url      = {https://doi.org/10.1115/1.4044321},
}

@Article{Shintani2023,
  author    = {Shintani, Kohei and Sugai, Tomotaka and Yamada, Takayuki},
  title     = {A set-based approach for hierarchical optimization problem using {B}ayesian active learning},
  journal   = {International Journal for Numerical Methods in Engineering},
  year      = {2023},
  volume    = {124},
  number    = {10},
  pages     = {2196--2214},
  month     = may,
  issn      = {0029-5981},
  abstract  = {Abstract In the development of complex engineering systems, many engineers from different disciplines collaborate to identify feasible designs that satisfy all system requirements. Analytical target cascading (ATC) is a method for the design optimization of hierarchical, multilevel systems and has been successfully employed in the design of complex engineering systems. In this paper, we propose a novel data-driven set-based ATC (SBATC) method for hierarchical design optimization problems using machine learning techniques. The proposed SBATC offers two advantages in engineering processes. First, it decomposes hierarchical design optimization problems into sets of suboptimization problems. The feasible regions of the suboptimization problems are explored and cascaded to lower-level optimization problems. Using the set-based approach, couplings between two adjacent levels in the optimization process are not required. Second, an efficient strategy is employed to determine feasible regions based on Bayesian active learning (BAL). In BAL, the Gaussian process (GP) of all cost functions is trained. An acquisition function that combines the probability of feasibility and entropy search is evaluated using posterior distributions of the trained GP. The acquisition function is maximized to generate new sampling points around the feasible regions by balancing the exploitation and exploration of the design space. To verify the effectiveness of the proposed method, numerical examples of hierarchical optimization problems are evaluated.},
  doi       = {10.1002/nme.7206},
  groups    = {Sequential model-based},
  keywords  = {analytical target cascading, Bayesian active learning, Gaussian process, multidisciplinary optimization, set-based design},
  publisher = {John Wiley \& Sons, Ltd},
  url       = {https://doi.org/10.1002/nme.7206},
}

@Article{Bonfiglio2018a,
  author    = {Bonfiglio, Luca and Perdikaris, Paris and del Águila, Jose and Karniadakis, George E.},
  title     = {A probabilistic framework for multidisciplinary design: Application to the hydrostructural optimization of supercavitating hydrofoils},
  journal   = {International Journal for Numerical Methods in Engineering},
  year      = {2018},
  volume    = {116},
  number    = {4},
  pages     = {246--269},
  month     = oct,
  issn      = {0029-5981},
  abstract  = {Summary The analysis and optimization of complex multiphysics systems presents a series of challenges that limit the practical use of computational tools. Specifically, the optimization of such systems involves multiple interconnected components with competing quantities of interest and high-dimensional spaces and necessitates the use of costly high-fidelity solvers to accurately simulate the coupled multiphysics. In this paper, we put forth a data-driven framework to address these challenges leveraging recent advances in machine learning. We combine multifidelity Gaussian process regression and Bayesian optimization to construct probabilistic surrogate models for given quantities of interest and explore high-dimensional design spaces in a cost-effective manner. The synergistic use of these computational tools gives rise to a tractable and general framework for tackling realistic multidisciplinary optimization problems. To demonstrate the specific merits of our approach, we have chosen a challenging large-scale application involving the hydrostructural optimization of three-dimensional supercavitating hydrofoils. To this end, we have developed an automated workflow for performing multiresolution simulations of turbulent multiphase flows and multifidelity structural mechanics (combining three-dimensional and one-dimensional finite element results), the results of which drive our machine learning analysis in pursuit of the optimal hydrofoil shape.},
  doi       = {10.1002/nme.5923},
  groups    = {Sequential model-based},
  keywords  = {Bayesian optimization, fluid-structure interaction, machine learning, multifidelity, multiphysics, supercavitation},
  publisher = {John Wiley \& Sons, Ltd},
  url       = {https://doi.org/10.1002/nme.5923},
}

@Article{Huang2023,
  author   = {Huang, Hanyan and Liu, Zecong and Zheng, Hongyu and Xu, Xiaoyu and Duan, Yanhui},
  title    = {A proportional expected improvement criterion-based multi-fidelity sequential optimization method},
  journal  = {Structural and Multidisciplinary Optimization},
  year     = {2023},
  volume   = {66},
  number   = {2},
  pages    = {30},
  issn     = {1615-1488},
  abstract = {Multi-fidelity surrogate models fusing data from different fidelity systems can significantly reduce the computational cost while ensuring the model accuracy. The focus of this paper is on the sequential design for multi-fidelity models for expensive black-box problem. A Co-kriging-based multi-fidelity sequential optimization method named proportional expected improvement (PEI) is proposed with the objection to be more efficient for global optimization and to be more reasonable to evaluate the costs and benefits of candidate points from different levels of fidelity. The PEI method is an extension of expected improvement (EI) and uses an integrated criterion to determine both location and fidelity level of the subsequent. In the integrated criterion, a proportional factor which is adaptively adjusted according to the sample density is added in EI to adjust the tendency between exploration and exploitation during the search process. Meanwhile, Kullback-Leibler divergence is used to measure the credibility of a point from system with different fidelities, and the cost and constraint of different fidelities are also considered. The effectiveness and advantage of the proposed method were demonstrated by seven analytical functions and then applied to the aerodynamic shape optimization of NACA0012 airfoil. Experiments show that the proportional factor makes the proposed algorithm better search for the global optimum, and the KL divergence can describe the relationship between high and low fidelity more significantly.},
  doi      = {10.1007/s00158-022-03484-7},
  groups   = {Sequential model-based},
  refid    = {Huang2023},
  url      = {https://doi.org/10.1007/s00158-022-03484-7},
}

@Article{Lin2023,
  author   = {Lin, Quan and Zheng, Anran and Hu, Jiexiang and Shu, Leshi and Zhou, Qi},
  title    = {A multi-objective {B}ayesian optimization approach based on variable-fidelity multi-output metamodeling},
  journal  = {Structural and Multidisciplinary Optimization},
  year     = {2023},
  volume   = {66},
  number   = {5},
  pages    = {100},
  issn     = {1615-1488},
  abstract = {Practical engineering problems are often involved multiple computationally expensive objectives. A promising strategy to alleviate the computational cost is the variable-fidelity metamodel-based multi-objective Bayesian optimization approach. However, the existing approaches are under the assumption of independent correlations across the multiple objectives by constructing a variable-fidelity metamodel for each objective, which may lose some useful information in a way. To facilitate the usage of the variable-fidelity metamodel-based multi-objective Bayesian optimization approach, a multi-objective Bayesian optimization approach based on variable-fidelity multi-output (VFMO) metamodeling is proposed in this paper. A variable-fidelity multi-output metamodeling approach is developed to model the multiple objectives jointly, which can capture the latent correlations across the multiple objectives and further enhance the optimization. Furthermore, a weighted expected hypervolume improvement acquisition function based on the VFMO metamodeling approach (VFMO-WEHVI) is proposed for multi-objective optimization. The weight coefficients are adaptively determined according to the information from the current metamodel, which allows a better tradeoff between global exploration and local exploitation. Moreover, the probability of feasibility is introduced to deal with multi-objective optimization problems with constraints. The effectiveness of the proposed approach is demonstrated using five analytical benchmark examples and the multi-objective optimization of a metamaterial vibration isolator. Results indicate that the proposed VFMO-WEHVI approach has the best overall performance compared with the state-of-the-art approaches.},
  doi      = {10.1007/s00158-023-03536-6},
  groups   = {Sequential model-based},
  refid    = {Lin2023},
  url      = {https://doi.org/10.1007/s00158-023-03536-6},
}

@Article{Cheng2022,
  author   = {Cheng, Ji and Lin, Qiao and Yi, Jiaxiang},
  title    = {An enhanced variable-fidelity optimization approach for constrained optimization problems and its parallelization},
  journal  = {Structural and Multidisciplinary Optimization},
  year     = {2022},
  volume   = {65},
  number   = {7},
  pages    = {188},
  issn     = {1615-1488},
  abstract = {In this paper, a variable-fidelity constrained lower confidence bound (VF-CLCB) criterion is presented for computationally expensive constrained optimization problems (COPs) with two levels of fidelity. In VF-CLCB, the hierarchical Kriging model is adopted to model the objective and inequality constraints. Two infill sampling functions are developed based on the objective and the constraints, respectively, and an adaptive selection strategy is set to select the elite sample points. Moreover, based on the VF-CLCB criterion, a parallel optimization method noted as PVF-CLCB is subsequently developed to accelerate the optimization process. In PVF-CLCB, a VF influence function is defined to approximately evaluate the estimation error of the hierarchical Kriging models, based on which multiple promising points can be determined at each iteration. In addition, an allocation strategy is proposed to distribute the computation resources between the objective- and constraint-oriented functions properly. Lastly, the proposed VF-CLCB and PVF-CLCB approaches are compared with the alternative methods on 12 benchmark numerical cases, and their significant superiority in solving computationally expensive COPs is verified. Furthermore, the proposed methods are employed to optimize the global stability of the stiffened cylindrical shell, and the optimum structure is yielded.},
  doi      = {10.1007/s00158-022-03283-0},
  groups   = {Sequential model-based},
  refid    = {Cheng2022},
  url      = {https://doi.org/10.1007/s00158-022-03283-0},
}

@Article{Fiore2021,
  author   = {Fiore, Di Francesco and Maggiore, Paolo and Mainini, Laura},
  title    = {Multifidelity domain-aware learning for the design of re-entry vehicles},
  journal  = {Structural and Multidisciplinary Optimization},
  year     = {2021},
  volume   = {64},
  number   = {5},
  pages    = {3017--3035},
  issn     = {1615-1488},
  abstract = {The multidisciplinary design optimization (MDO) of re-entry vehicles presents many challenges associated with the plurality of the domains that characterize the design problem and the multi-physics interactions. Aerodynamic and thermodynamic phenomena are strongly coupled and relate to the heat loads that affect the vehicle along the re-entry trajectory, which drive the design of the thermal protection system (TPS). The preliminary design and optimization of re-entry vehicles would benefit from accurate high-fidelity aerothermodynamic analysis, which are usually expensive computational fluid dynamic simulations. We propose an original formulation for multifidelity active learning that considers both the information extracted from data and domain-specific knowledge. Our scheme is developed for the design of re-entry vehicles and is demonstrated for the case of an Orion-like capsule entering the Earth atmosphere. The design process aims to minimize the mass of propellant burned during the entry maneuver, the mass of the TPS, and the temperature experienced by the TPS along the re-entry. The results demonstrate that our multifidelity strategy allows to achieve a sensitive improvement of the design solution with respect to the baseline. In particular, the outcomes of our method are superior to the design obtained through a single-fidelity framework, as a result of the principled selection of a limited number of high-fidelity evaluations.},
  doi      = {10.1007/s00158-021-03037-4},
  groups   = {Sequential model-based},
  refid    = {Di Fiore2021},
  url      = {https://doi.org/10.1007/s00158-021-03037-4},
}

@Article{Ruan2020,
  author   = {Ruan, Xiongfeng and Jiang, Ping and Zhou, Qi and Hu, Jiexiang and Shu, Leshi},
  title    = {Variable-fidelity probability of improvement method for efficient global optimization of expensive black-box problems},
  journal  = {Structural and Multidisciplinary Optimization},
  year     = {2020},
  volume   = {62},
  number   = {6},
  pages    = {3021--3052},
  issn     = {1615-1488},
  abstract = {Variable-fidelity (VF) surrogate models have attracted significant attention recently in simulation-based design because they can achieve a desirable accuracy at a reasonable cost by making use of the data from both low-fidelity (LF) and high-fidelity (HF) simulations. To facilitate the usage of VF surrogate models assisted efficient global optimization, there are still challenging issues on (1) how to construct the VF surrogate model for simulations with variable-fidelity levels under the non-nested sampling data, (2) how to determine the location and fidelity level of the samples simultaneously, and (3) how to handle constraints when VF surrogate models are also used for constraints. In this work, a variable-fidelity probability of improvement (VF-PI) method is proposed for computationally expensive black-box problems. First, a multi-level generalized Co-Kriging (GCK) model, which is extended from the two-level GCK model, is developed for VF surrogate modeling of simulations with three or more levels of fidelities under non-nested sampling data. Second, to determine the location and fidelity level of the sequential samples, an extended probability of improvement (EPI) function is developed. In EPI function, the model correlation and cost ratio between the LF and HF models, together with the sample density, are considered. Third, the probability of satisfying the constraints is introduced and combined with the EPI function, enabling the proposed approach to handle VF optimization problems with constraints. The comparison results illustrate that the proposed VF-PI method is more efficient and robust than the four compared methods on the illustrated cases.},
  doi      = {10.1007/s00158-020-02646-9},
  groups   = {Sequential model-based},
  refid    = {Ruan2020},
  url      = {https://doi.org/10.1007/s00158-020-02646-9},
}

@Article{Ghoreishi2019,
  author   = {Ghoreishi, Seyede Fatemeh and Allaire, Douglas},
  title    = {Multi-information source constrained {B}ayesian optimization},
  journal  = {Structural and Multidisciplinary Optimization},
  year     = {2019},
  volume   = {59},
  number   = {3},
  pages    = {977--991},
  issn     = {1615-1488},
  abstract = {Design decisions for complex systems often can be made or informed by a variety of information sources. When optimizing such a system, the evaluation of a quantity of interest is typically required at many different input configurations. For systems with expensive to evaluate available information sources, the optimization task can potentially be computationally prohibitive using traditional techniques. This paper presents an information-economic approach to the constrained optimization of a system with multiple available information sources. The approach rigorously quantifies the correlation between the discrepancies of different information sources, which enables the overcoming of information source bias. All information is exploited efficiently by fusing newly acquired information with that previously evaluated. Independent decision-makings are achieved by developing a two-step look-ahead utility policy and an information gain policy for objective function and constraints respectively. The approach is demonstrated on a one-dimensional example test problem and an aerodynamic design problem, where it is shown to perform well in comparison to traditional multi-information source techniques.},
  doi      = {10.1007/s00158-018-2115-z},
  groups   = {Sequential model-based},
  refid    = {Ghoreishi2019},
  url      = {https://doi.org/10.1007/s00158-018-2115-z},
}

@Article{Grassi2023,
  author    = {Grassi, Francesco and Manganini, Giorgio and Garraffa, Michele and Mainini, Laura},
  journal   = {AIAA Journal},
  title     = {{RAAL: Resource aware active learning for multifidelity efficient optimization}},
  year      = {2023},
  issn      = {0001-1452},
  month     = may,
  number    = {6},
  pages     = {2744--2753},
  volume    = {61},
  comment   = {doi: 10.2514/1.J061383},
  doi       = {10.2514/1.J061383},
  groups    = {Sequential model-based},
  publisher = {American Institute of Aeronautics and Astronautics},
  url       = {https://doi.org/10.2514/1.J061383},
}

@Article{Fiore2023,
  author    = {Fiore, Di Francesco and Mainini, Laura},
  title     = {{NM-MF}: Non-Myopic Multifidelity Framework for Constrained Multi-Regime Aerodynamic Optimization},
  journal   = {AIAA Journal},
  year      = {2023},
  volume    = {61},
  number    = {3},
  pages     = {1270--1280},
  month     = jan,
  issn      = {0001-1452},
  abstract  = {The exploration and trade-off analysis of different aerodynamic design configurations requires solving optimization problems. The major bottleneck to assess the optimal design is the large number of time-consuming evaluations of high-fidelity computational fluid dynamics (CFD) models, necessary to capture the non-linear phenomena and discontinuities that occur at higher Mach number regimes. To address this limitation, we introduce an original non-myopic multifidelity Bayesian framework aimed at including expensive high-fidelity CFD simulations for the optimization of the aerodynamic design. Our scheme proposes a novel two-step lookahead policy to maximize the improvement of the solution quality considering the rewards of future steps, and combines it with utility functions informed by the fluid dynamic regime and the information extracted from data, to wisely select the aerodynamic model to interrogate. We validate the proposed framework for the case of a constrained drag coefficient optimization problem of a NACA 0012 airfoil, and compare the results to other popular multifidelity and single-fidelity optimization frameworks. The results suggest that our strategy outperforms the other approaches, allowing to significantly reduce the drag coefficient through a principled selection of limited evaluations of the high-fidelity CFD model.},
  comment   = {doi: 10.2514/1.J062219},
  doi       = {10.2514/1.J062219},
  groups    = {Sequential model-based},
  publisher = {American Institute of Aeronautics and Astronautics},
  url       = {https://doi.org/10.2514/1.J062219},
}

@Article{Bailly2019,
  author    = {Bailly, Joëlle and Bailly, Didier},
  title     = {Multifidelity Aerodynamic Optimization of a Helicopter Rotor Blade},
  journal   = {AIAA Journal},
  year      = {2019},
  volume    = {57},
  number    = {8},
  pages     = {3132--3144},
  month     = apr,
  issn      = {0001-1452},
  abstract  = {A multifidelity optimization technique is applied to the design of a helicopter rotor blade to improve its performance in forward flight. This optimization technique is based on a surrogate model that replaces the high-fidelity computational fluid dynamics (CFD)/computational structural dynamics (CSD) simulations necessary to capture the three-dimensional unsteady effects generated in the flowfield of a complex blade geometry. The single low-fidelity model based on kriging methodology and generated by lifting-line simulations leads to a power benefit of 2.5%, which is not reproducible by an a posteriori high-fidelity CSD/CFD computation. The optimization procedure using cokriging surrogate models based on two levels of fidelity (lifting-line and CSD/CFD simulations) leads to a realistic blade planform, for which the power benefit is estimated at 2.2%. This optimized solution, obtained after a factor-of-six reduction in CPU time, shows the advantages of using a cokriging surrogate model (rather than a single-fidelity kriging model) for aerodynamic optimizations.},
  comment   = {doi: 10.2514/1.J056513},
  doi       = {10.2514/1.J056513},
  groups    = {Sequential model-based},
  publisher = {American Institute of Aeronautics and Astronautics},
  url       = {https://doi.org/10.2514/1.J056513},
}

@Article{Khatamsaz2021aiaa,
  author    = {Khatamsaz, Danial and Peddareddygari, Lalith and Friedman, Samuel and Allaire, Douglas},
  journal   = {AIAA Journal},
  title     = {Bayesian Optimization of Multiobjective Functions Using Multiple Information Sources},
  year      = {2021},
  issn      = {0001-1452},
  month     = mar,
  number    = {6},
  pages     = {1964--1974},
  volume    = {59},
  abstract  = {Multiobjective optimization is often a difficult task owing to the need to balance competing objectives. A typical approach to handling this is to estimate a Pareto frontier in objective space by identifying nondominated points. This task is typically computationally demanding owing to the need to incorporate information of high enough fidelity to be trusted in design and decision-making processes. In this work, we present a multi-information source framework for enabling efficient multiobjective optimization. The framework allows for the exploitation of all available information and considers both potential improvement and cost. The framework includes ingredients of model fusion, expected hypervolume improvement, and intermediate Gaussian process surrogates. The approach is demonstrated on a test problem and an aerostructural wing design problem.},
  comment   = {doi: 10.2514/1.J059803},
  doi       = {10.2514/1.J059803},
  groups    = {Sequential model-based},
  publisher = {American Institute of Aeronautics and Astronautics},
  url       = {https://doi.org/10.2514/1.J059803},
}

@Article{Perdikaris2016,
  author    = {Perdikaris, Paris and Karniadakis, George Em},
  title     = {Model inversion via multi-fidelity {B}ayesian optimization: a new paradigm for parameter estimation in haemodynamics, and beyond},
  journal   = {Journal of The Royal Society Interface},
  year      = {2016},
  volume    = {13},
  number    = {118},
  pages     = {20151107},
  month     = may,
  comment   = {doi: 10.1098/rsif.2015.1107},
  doi       = {10.1098/rsif.2015.1107},
  groups    = {Opti Algorithms, Sequential model-based},
  publisher = {Royal Society},
  url       = {https://doi.org/10.1098/rsif.2015.1107},
}

@Article{Winter2023,
  author   = {Winter, J. M. and Abaidi, R. and Kaiser, J. W. J. and Adami, S. and Adams, N. A.},
  title    = {Multi-fidelity {B}ayesian optimization to solve the inverse {S}tefan problem},
  journal  = {Computer Methods in Applied Mechanics and Engineering},
  year     = {2023},
  volume   = {410},
  pages    = {115946},
  issn     = {0045-7825},
  abstract = {In this work, we propose an efficient solution of the inverse Stefan problem by multi-fidelity Bayesian optimization. We construct a multi-fidelity Gaussian process surrogate model by combining many low-fidelity estimates of a solidification problem with only a few high-fidelity measurements. To solve the inverse problem, we employ the Gaussian process model in a Bayesian optimization approach based on a multi-fidelity knowledge gradient acquisition function. To account for the specific structure of the target function, we reformulate it as a composite function and thus significantly improve the stability of the optimization procedure. Target values can be switched easily, and previously obtained samples and surrogate models can be reused. The proposed method iteratively improves the recommended solution of the inverse problem. Explicitly adding recommended points of previous iterations to the solution procedure enhances the convergence properties of the algorithm. We demonstrate the applicability of the algorithm by solving the inverse problem for a planar solidification front in a single-fidelity setting. Process parameters are identified for targeted crystal-growth velocities during directional dendritic solidification. The relation between these velocities and process parameters, such as undercooling, thermal diffusivity, capillarity, and capillary anisotropy, defines the thermo-mechanical properties of the solidified part in metal-based additive manufacturing. Material design is based on the corresponding inverse problem. Cost-efficiency of solving the inverse problem is improved by introducing a fidelity hierarchy based on coarse-grid approximations of high-fidelity numerical simulations. The open-source simulation framework ALPACA for multiphase flows allows to generate data at all fidelities. We demonstrate the superior convergence properties of the presented multi-fidelity approach by comparison with an approach solely based on high-fidelity measurements of the tip velocity.},
  doi      = {10.1016/j.cma.2023.115946},
  groups   = {Sequential model-based},
  keywords = {Bayesian optimization, Multi-fidelity modeling, Inverse problem, Dendritic growth, Multiresolution},
  url      = {https://www.sciencedirect.com/science/article/pii/S0045782523000695},
}

@Article{Amrit2018,
  author   = {Amrit, Anand and Leifsson, Leifur and Koziel, Slawomir},
  title    = {Multi-fidelity aerodynamic design trade-off exploration using point-by-point {P}areto set identification},
  journal  = {Aerospace Science and Technology},
  year     = {2018},
  volume   = {79},
  pages    = {399--412},
  issn     = {1270-9638},
  abstract = {Aerodynamic design is inherently a multi-objective optimization (MOO) problem. Determining the best possible trade-offs between conflicting aerodynamic objectives can be computationally challenging when carried out directly at the level of high-fidelity computational fluid dynamics simulations. This paper presents a computationally cheap methodology for exploration of aerodynamic design trade-offs. In particular, point-by-point identification of a set of Pareto-optimal designs is executed starting in the neighborhood of a single-objective optimal design, and using a trust-region-based, multi-fidelity optimization algorithm as well as locally constructed response surface approximations (RSAs). In this work, the RSAs are constructed using second-order polynomials without mixed terms, multi-fidelity models, and adaptive corrections. The application of the point-by-point MOO algorithm is demonstrated through MOO of transonic airfoil shapes using the Reynold-Averaged Navier Stokes equations and the Spalart-Allmaras turbulence model. The results demonstrate that the Pareto front in the neighborhood of an initial design can be obtained at a low cost when considering up to 12 design variables. The results also indicate that the computational cost of the optimization process grows slowly with the number of the design variables, and the repeatability of the algorithm is very good when starting the search from different initial points.},
  doi      = {10.1016/j.ast.2018.05.023},
  groups   = {Sequential model-based},
  url      = {https://www.sciencedirect.com/science/article/pii/S1270963818304577},
}

@Article{WuY2021,
  author   = {Wu, Yuda and Lin, Quan and Zhou, Qi and Hu, Jiexiang and Wang, Shengyi and Peng, Yutong},
  journal  = {Aerospace Science and Technology},
  title    = {An adaptive space preselection method for the multi-fidelity global optimization},
  year     = {2021},
  issn     = {1270-9638},
  pages    = {106728},
  volume   = {113},
  abstract = {Multi-fidelity (MF) metamodels have been well applied to aerospace structure optimization problems to relieve the computation burden. However, most of the existing multi-fidelity optimization methods explore the optimum in the whole design space, which may lead to low-efficiency of the optimization search. In this paper, a space preselection-based multi-fidelity lower confidence bounding (SPMF-LCB) optimization method is proposed to solve this problem. Firstly, a bootstrap-assisted area selection algorithm is proposed, which can adaptively partition the whole design space and select the most potential area to facilitate the optimization process. Secondly, the lower confidence bounding (LCB) method is extended to the multi-fidelity level, which can adaptively determine both the fidelity level and the location of sample points, with the consideration of the low-fidelity (LF) simulations budget. Finally, the probability of feasible (POF) method is combined with the extended LCB method to handle the constrained optimization problems. Eight analytical examples and the optimization problem of the radome of the missile are utilized to illustrate the efficiency of the proposed SPMF-LCB method. The performance of the proposed approach is compared with four existing methods. Results show that the proposed SPMF-LCB method performs the best considering the efficiency and robustness.},
  doi      = {10.1016/j.ast.2021.106728},
  groups   = {Sequential model-based},
  keywords = {Multi-fidelity metamodel, Space preselection, Bootstrap, Constrained optimization, Lower confidence bounding},
  url      = {https://www.sciencedirect.com/science/article/pii/S1270963821002388},
}

@Article{He2021,
  author   = {He, Youwei and Sun, Jinju and Song, Peng and Wang, Xuesong},
  title    = {Variable-fidelity expected improvement based efficient global optimization of expensive problems in presence of simulation failures and its parallelization},
  journal  = {Aerospace Science and Technology},
  year     = {2021},
  volume   = {111},
  pages    = {106572},
  issn     = {1270-9638},
  abstract = {To further alleviate the computational cost of single-fidelity efficient global optimization (EGO) method, the variable-fidelity surrogate-based EGO method assisted by variable-fidelity expected improvement (VF-EI) criterion has emerged and demonstrated to be efficient. This method follows the EGO framework and inherits the sequential feature of EGO, which is, only one infill sample is obtained in each iteration to update the surrogate. Such sequential feature makes the method vulnerable in solving the engineering design optimization problems in which the simulator will possibly crash, especially in computational fluid dynamic involved geometry shape optimization problems. In this paper, a strategy to deal with the existence of simulation failures in variable-fidelity surrogate based sequential optimization method is proposed. By introducing additional Kriging model, which can be updated regardless of the simulation status of the newly selected infill point, to approximate the simulation success possibility, the iterative optimal search process of sequential method will not halt prematurely if a simulation failure of infill point happens. With the available simulation success possibility, new infill criteria based on VF-EI are developed to ensure the effectiveness and efficiency of the proposed method. Further, to accelerate the optimization process with the aid of parallel computation, the sequential method is parallelized by utilizing an influence function to prevent point clustering. Experiment results over analytic and engineering problems show that the proposed sequential method outperforms the method employing the penalty and imputation strategies to deal with simulation failures and the parallel method can accelerate the optimal search compared with the proposed sequential method.},
  doi      = {10.1016/j.ast.2021.106572},
  groups   = {Sequential model-based},
  keywords = {Variable-fidelity surrogate, Hierarchical Kriging, Expected improvement, Simulation failures, Parallel computing},
  url      = {https://www.sciencedirect.com/science/article/pii/S1270963821000833},
}

@Article{Renganathan2021,
  author   = {Renganathan, S. Ashwin and Maulik, Romit and Ahuja, Jai},
  title    = {Enhanced data efficiency using deep neural networks and {G}aussian processes for aerodynamic design optimization},
  journal  = {Aerospace Science and Technology},
  year     = {2021},
  volume   = {111},
  pages    = {106522},
  issn     = {1270-9638},
  abstract = {Adjoint-based optimization methods are attractive for aerodynamic shape design primarily due to their computational costs being independent of the dimensionality of the input space and their ability to generate high-fidelity gradients that can then be used in a gradient-based optimizer. This makes them very well suited for high-fidelity simulation based aerodynamic shape optimization of highly parametrized geometries such as aircraft wings. However, the development of adjoint-based solvers involve careful mathematical treatment and their implementation require detailed software development. Furthermore, they can become prohibitively expensive when multiple optimization problems are being solved, each requiring multiple restarts to circumvent local optima. In this work, we propose a machine learning enabled, surrogate-based framework that replaces the expensive adjoint solver, without compromising on predicting predictive accuracy. Specifically, we first train a deep neural network (DNN) from training data generated from evaluating the high-fidelity simulation model on a model-agnostic design of experiments on the geometry shape parameters. The optimum shape may then be computed by using a gradient-based optimizer coupled with the trained DNN. Subsequently, we also perform a gradient-free Bayesian optimization, where the trained DNN is used as the prior mean. We observe that the latter framework (DNN-BO) improves upon the DNN-only based optimization strategy for the same computational cost. Overall, this framework predicts the true optimum with very high accuracy, while requiring far fewer high-fidelity function calls compared to the adjoint-based method. Furthermore, we show that multiple optimization problems can be solved with the same machine learning model with high accuracy, to amortize the offline costs associated with constructing our models. Our methodology finds applications in the early stages of aerospace design.},
  doi      = {10.1016/j.ast.2021.106522},
  groups   = {Sequential model-based},
  keywords = {Machine learning, Gaussian processes, Bayesian optimization, Aerodynamic design optimization, Adjoint method},
  url      = {https://www.sciencedirect.com/science/article/pii/S1270963821000341},
}

@Article{Bonfiglio2018b,
  author   = {Bonfiglio, L. and Perdikaris, P. and Brizzolara, S. and Karniadakis, G. E.},
  title    = {Multi-fidelity optimization of super-cavitating hydrofoils},
  journal  = {Computer Methods in Applied Mechanics and Engineering},
  year     = {2018},
  volume   = {332},
  pages    = {63--85},
  issn     = {0045-7825},
  abstract = {We present an effective multi-fidelity framework for shape optimization of super-cavitating hydrofoils using viscous solvers. We employ state-of-the-art machine learning tools such as multi-fidelity Gaussian process regression and Bayesian optimization to synthesize data obtained from multi-resolution simulations, and efficiently identify optimal configurations in the design space. We validate our simulation results against experimental data, and showcase the efficiency of the proposed work-flow in a realistic design problem involving the shape optimization of a three-dimensional super-cavitating hydrofoil parametrized by 17 design variables.},
  doi      = {10.1016/j.cma.2017.12.009},
  groups   = {Sequential model-based},
  keywords = {Turbulent multi-phase flows, Multi-fidelity modeling, URANS, Bayesian optimization, Super-cavitating hydrofoils},
  url      = {https://www.sciencedirect.com/science/article/pii/S0045782517307612},
}

@Article{Bull2011,
  author  = {Bull, Adam D.},
  journal = {Journal of Machine Learning Research},
  title   = {Convergence rates of efficient global optimization algorithms.},
  year    = {2011},
  issn    = {1532-4435},
  number  = {10},
  pages   = {2879--2904},
  volume  = {12},
  groups  = {Bayesian optimization},
  url     = {https://www.jmlr.org/papers/v12/bull11a.html},
}

@InProceedings{Mockus1975,
  author    = {Močkus, Jonas},
  booktitle = {{Optimization Techniques IFIP Technical Conference: Novosibirsk, July 1-7, 1974}},
  title     = {{On Bayesian methods for seeking the extremum}},
  year      = {1975},
  pages     = {400--404},
  publisher = {Springer},
  doi       = {10.1007/3-540-07165-2_55},
  groups    = {Earlyworks, Bayesian optimization, Acquisition functions},
}

@Article{Knowles2006,
  author      = {Knowles, J.},
  title       = {{ParEGO}: a hybrid algorithm with on-line landscape approximation for expensive multiobjective optimization problems},
  journal     = {IEEE Transactions on Evolutionary Computation},
  year        = {2006},
  volume      = {10},
  number      = {1},
  pages       = {50--66},
  issn        = {1941-0026},
  call-number = {10},
  doi         = {10.1109/TEVC.2005.851274},
  groups      = {Multi-objective BO},
}

@Article{ZhangQ2010,
  author      = {Zhang, Q. and Liu, W. and Tsang, E. and Virginas, B.},
  journal     = {IEEE Transactions on Evolutionary Computation},
  title       = {Expensive Multiobjective Optimization by {MOEA/D} With {G}aussian Process Model},
  year        = {2010},
  issn        = {1941-0026},
  number      = {3},
  pages       = {456--474},
  volume      = {14},
  call-number = {14},
  doi         = {10.1109/TEVC.2009.2033671},
  groups      = {Multi-objective BO},
}

@Article{Couckuyt2014,
  author   = {Couckuyt, Ivo and Deschrijver, Dirk and Dhaene, Tom},
  title    = {Fast calculation of multiobjective probability of improvement and expected improvement criteria for Pareto optimization},
  journal  = {Journal of Global Optimization},
  year     = {2014},
  volume   = {60},
  number   = {3},
  pages    = {575--594},
  issn     = {1573-2916},
  abstract = {The use of surrogate based optimization (SBO) is widely spread in engineering design to reduce the number of computational expensive simulations. However, “real-world” problems often consist of multiple, conflicting objectives leading to a set of competitive solutions (the Pareto front). The objectives are often aggregated into a single cost function to reduce the computational cost, though a better approach is to use multiobjective optimization methods to directly identify a set of Pareto-optimal solutions, which can be used by the designer to make more efficient design decisions (instead of weighting and aggregating the costs upfront). Most of the work in multiobjective optimization is focused on multiobjective evolutionary algorithms (MOEAs). While MOEAs are well-suited to handle large, intractable design spaces, they typically require thousands of expensive simulations, which is prohibitively expensive for the problems under study. Therefore, the use of surrogate models in multiobjective optimization, denoted as multiobjective surrogate-based optimization, may prove to be even more worthwhile than SBO methods to expedite the optimization of computational expensive systems. In this paper, the authors propose the efficient multiobjective optimization (EMO) algorithm which uses Kriging models and multiobjective versions of the probability of improvement and expected improvement criteria to identify the Pareto front with a minimal number of expensive simulations. The EMO algorithm is applied on multiple standard benchmark problems and compared against the well-known NSGA-II, SPEA2 and SMS-EMOA multiobjective optimization methods.},
  doi      = {10.1007/s10898-013-0118-2},
  groups   = {Multi-objective BO},
  refid    = {Couckuyt2014},
  url      = {https://doi.org/10.1007/s10898-013-0118-2},
}

@Article{Bradford2018,
  author   = {Bradford, Eric and Schweidtmann, Artur M. and Lapkin, Alexei},
  title    = {Efficient multiobjective optimization employing Gaussian processes, spectral sampling and a genetic algorithm},
  journal  = {Journal of Global Optimization},
  year     = {2018},
  volume   = {71},
  number   = {2},
  pages    = {407--438},
  issn     = {1573-2916},
  abstract = {Many engineering problems require the optimization of expensive, black-box functions involving multiple conflicting criteria, such that commonly used methods like multiobjective genetic algorithms are inadequate. To tackle this problem several algorithms have been developed using surrogates. However, these often have disadvantages such as the requirement of a priori knowledge of the output functions or exponentially scaling computational cost with respect to the number of objectives. In this paper a new algorithm is proposed, TSEMO, which uses Gaussian processes as surrogates. The Gaussian processes are sampled using spectral sampling techniques to make use of Thompson sampling in conjunction with the hypervolume quality indicator and NSGA-II to choose a new evaluation point at each iteration. The reference point required for the hypervolume calculation is estimated within TSEMO. Further, a simple extension was proposed to carry out batch-sequential design. TSEMO was compared to ParEGO, an expected hypervolume implementation, and NSGA-II on nine test problems with a budget of 150 function evaluations. Overall, TSEMO shows promising performance, while giving a simple algorithm without the requirement of a priori knowledge, reduced hypervolume calculations to approach linear scaling with respect to the number of objectives, the capacity to handle noise and lastly the ability for batch-sequential usage.},
  doi      = {10.1007/s10898-018-0609-2},
  groups   = {Multi-objective BO},
  refid    = {Bradford2018},
  url      = {https://doi.org/10.1007/s10898-018-0609-2},
}

@InProceedings{Daulton2020,
  author    = {Daulton, Samuel and Balandat, Maximilian and Bakshy, Eytan},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Differentiable expected hypervolume improvement for parallel multi-objective {B}ayesian optimization},
  year      = {2020},
  pages     = {9851--9864},
  publisher = {NeurIPS 2020},
  volume    = {33},
  groups    = {Multi-objective BO},
  url       = {https://proceedings.neurips.cc/paper/2020/hash/6fec24eac8f18ed793f5eaad3dd7977c-Abstract.html},
}

@Article{Picheny2015,
  author   = {Picheny, Victor},
  journal  = {Statistics and Computing},
  title    = {Multiobjective optimization using {G}aussian process emulators via stepwise uncertainty reduction},
  year     = {2015},
  issn     = {1573-1375},
  number   = {6},
  pages    = {1265--1280},
  volume   = {25},
  abstract = {Optimization of expensive computer models with the help of Gaussian process emulators is now commonplace. However, when several (competing) objectives are considered, choosing an appropriate sampling strategy remains an open question. We present here a new algorithm based on stepwise uncertainty reduction principles. Optimization is seen as a sequential reduction of the volume of the excursion sets below the current best solutions (Pareto set), and our sampling strategy chooses the points that give the highest expected reduction. The method is tested on several numerical examples and on an agronomy problem, showing that it provides an efficient trade-off between exploration and intensification.},
  doi      = {10.1007/s11222-014-9477-x},
  groups   = {Multi-objective BO},
  refid    = {Picheny2015},
  url      = {https://doi.org/10.1007/s11222-014-9477-x},
}

@InProceedings{Daulton2022MOBO,
  author    = {Daulton, Samuel and Eriksson, David and Balandat, Maximilian and Bakshy, Eytan},
  booktitle = {Proceedings of the Thirty-Eighth Conference on Uncertainty in Artificial Intelligence},
  title     = {Multi-objective {B}ayesian optimization over high-dimensional search spaces},
  year      = {2022},
  pages     = {507--517},
  publisher = {PMLR},
  volume    = {180},
  groups    = {High-dimension BO, RBF},
  issn      = {2640-3498},
  journal   = {Uncertainty in Artificial Intelligence},
  url       = {https://proceedings.mlr.press/v180/daulton22a.html},
}

@InProceedings{Eriksson2021,
  author       = {Eriksson, David and Poloczek, Matthias},
  booktitle    = {Proceedings of The 24th International Conference on Artificial Intelligence and Statistics},
  title        = {Scalable constrained {B}ayesian optimization},
  year         = {2021},
  pages        = {730--738},
  publisher    = {PMLR},
  volume       = {130},
  comment-ruda = {Constrained BO},
  groups       = {High-dimension BO, RBF},
  issn         = {2640-3498},
  journal      = {International Conference on Artificial Intelligence and Statistics},
  keywords     = {constraints},
  url          = {https://proceedings.mlr.press/v130/eriksson21a.html},
}

@InProceedings{Snoek2015,
  author    = {Snoek, Jasper and Rippel, Oren and Swersky, Kevin and Kiros, Ryan and Satish, Nadathur and Sundaram, Narayanan and Patwary, Mostofa and Prabhat, Mr and Adams, Ryan},
  booktitle = {Proceedings of the 32nd International Conference on International Conference on Machine Learning},
  title     = {Scalable {B}ayesian optimization using deep neural networks},
  year      = {2015},
  pages     = {2171--2180},
  volume    = {37},
  groups    = {High-dimension BO, RBF},
  journal   = {International conference on machine learning},
  url       = {https://proceedings.mlr.press/v37/snoek15.html},
}

@Article{Forrester2006,
  author    = {Forrester, Alexander I. J. and Keane, Andy J. and Bressloff, Neil W.},
  title     = {Design and Analysis of "Noisy" Computer Experiments},
  journal   = {AIAA Journal},
  year      = {2006},
  volume    = {44},
  number    = {10},
  pages     = {2331--2339},
  month     = oct,
  issn      = {0001-1452},
  comment   = {doi: 10.2514/1.20068},
  doi       = {10.2514/1.20068},
  groups    = {BO uncertainty},
  publisher = {American Institute of Aeronautics and Astronautics},
  url       = {https://doi.org/10.2514/1.20068},
}

@Article{Scott2011,
  author    = {Scott, Warren and Frazier, Peter I. and Powell, Warren},
  journal   = {SIAM Journal on Optimization},
  title     = {The Correlated Knowledge Gradient for Simulation Optimization of Continuous Parameters using Gaussian Process Regression},
  year      = {2011},
  issn      = {1052-6234},
  month     = jul,
  number    = {3},
  pages     = {996--1026},
  volume    = {21},
  abstract  = {We extend the concept of the correlated knowledge-gradient policy for the ranking and selection of a finite set of alternatives to the case of continuous decision variables. We propose an approximate knowledge gradient for problems with continuous decision variables in the context of a Gaussian process regression model in a Bayesian setting, along with an algorithm to maximize the approximate knowledge gradient. In the problem class considered, we use the knowledge gradient for continuous parameters to sequentially choose where to sample an expensive noisy function in order to find the maximum quickly. We show that the knowledge gradient for continuous decisions is a generalization of the efficient global optimization algorithm proposed in [D. R. Jones, M. Schonlau and W. J. Welch, J. Global Optim., 13 (1998), pp. 455?492].},
  comment   = {doi: 10.1137/100801275},
  doi       = {10.1137/100801275},
  groups    = {BO uncertainty},
  publisher = {Society for Industrial and Applied Mathematics},
  url       = {https://doi.org/10.1137/100801275},
}

@InProceedings{Daulton2021,
  author    = {Daulton, Samuel and Balandat, Maximilian and Bakshy, Eytan},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Parallel {B}ayesian optimization of multiple noisy objectives with expected hypervolume improvement},
  year      = {2021},
  pages     = {2187--2200},
  publisher = {NeurIPS 2021},
  volume    = {34},
  groups    = {BO uncertainty},
  journal   = {Advances in Neural Information Processing Systems},
  url       = {https://proceedings.neurips.cc/paper/2021/hash/11704817e347269b7254e744b5e22dac-Abstract.html},
}

@InProceedings{Daulton2022RMOBO,
  author    = {Daulton, Samuel and Cakmak, Sait and Balandat, Maximilian and Osborne, Michael A. and Zhou, Enlu and Bakshy, Eytan},
  booktitle = {Proceedings of the 39th International Conference on Machine Learning},
  title     = {Robust multi-objective {B}ayesian optimization under input noise},
  year      = {2022},
  pages     = {4831--4866},
  publisher = {PMLR},
  groups    = {BO uncertainty},
  issn      = {2640-3498},
  journal   = {International Conference on Machine Learning},
  url       = {https://proceedings.mlr.press/v162/daulton22a.html},
}

@InCollection{Frazier2015,
  author    = {Frazier, Peter I. and Wang, Jialei},
  booktitle = {Information Science for Materials Discovery and Design},
  publisher = {Springer},
  title     = {Bayesian optimization for materials design},
  year      = {2015},
  pages     = {45--75},
  doi       = {10.1007/978-3-319-23871-5_3},
  groups    = {BO applications},
}

@Article{ZhangYi2020,
  author   = {Zhang, Yichi and Apley, Daniel W. and Chen, Wei},
  journal  = {Scientific Reports},
  title    = {Bayesian Optimization for Materials Design with Mixed Quantitative and Qualitative Variables},
  year     = {2020},
  issn     = {2045-2322},
  number   = {1},
  pages    = {4924},
  volume   = {10},
  abstract = {Although Bayesian Optimization (BO) has been employed for accelerating materials design in computational materials engineering, existing works are restricted to problems with quantitative variables. However, real designs of materials systems involve both qualitative and quantitative design variables representing material compositions, microstructure morphology, and processing conditions. For mixed-variable problems, existing Bayesian Optimization (BO) approaches represent qualitative factors by dummy variables first and then fit a standard Gaussian process (GP) model with numerical variables as the surrogate model. This approach is restrictive theoretically and fails to capture complex correlations between qualitative levels. We present in this paper the integration of a novel latent-variable (LV) approach for mixed-variable GP modeling with the BO framework for materials design. LVGP is a fundamentally different approach that maps qualitative design variables to underlying numerical LV in GP, which has strong physical justification. It provides flexible parameterization and representation of qualitative factors and shows superior modeling accuracy compared to the existing methods. We demonstrate our approach through testing with numerical examples and materials design examples. The chosen materials design examples represent two different scenarios, one on concurrent materials selection and microstructure optimization for optimizing the light absorption of a quasi-random solar cell, and another on combinatorial search of material constitutes for optimal Hybrid Organic-Inorganic Perovskite (HOIP) design. It is found that in all test examples the mapped LVs provide intuitive visualization and substantial insight into the nature and effects of the qualitative factors. Though materials designs are used as examples, the method presented is generic and can be utilized for other mixed variable design optimization problems that involve expensive physics-based simulations.},
  doi      = {10.1038/s41598-020-60652-9},
  groups   = {BO applications},
  refid    = {Zhang2020},
  url      = {https://doi.org/10.1038/s41598-020-60652-9},
}

@Article{Yamashita2018,
  author    = {Yamashita, Tomoki and Sato, Nobuya and Kino, Hiori and Miyake, Takashi and Tsuda, Koji and Oguchi, Tamio},
  journal   = {Physical Review Materials},
  title     = {Crystal structure prediction accelerated by {B}ayesian optimization},
  year      = {2018},
  month     = jan,
  number    = {1},
  pages     = {013803},
  volume    = {2},
  doi       = {10.1103/PhysRevMaterials.2.013803},
  groups    = {BO applications},
  publisher = {American Physical Society},
  refid     = {10.1103/PhysRevMaterials.2.013803},
  url       = {https://link.aps.org/doi/10.1103/PhysRevMaterials.2.013803},
}

@Article{WangK2022,
  author   = {Wang, Ke and Dowling, Alexander W.},
  journal  = {Current Opinion in Chemical Engineering},
  title    = {Bayesian optimization for chemical products and functional materials},
  year     = {2022},
  issn     = {2211-3398},
  pages    = {100728},
  volume   = {36},
  abstract = {The design of chemical-based products and functional materials is vital to modern technologies, yet remains expensive and slow. Artificial intelligence and machine learning offer new approaches to leverage data to overcome these challenges. This review focuses on recent applications of Bayesian optimization (BO) to chemical products and materials including molecular design, drug discovery, molecular modeling, electrolyte design, and additive manufacturing. Numerous examples show how BO often requires an order of magnitude fewer experiments than Edisonian search. The essential equations for BO are introduced in a self-contained primer specifically written for chemical engineers and others new to the area. Finally, the review discusses four current research directions for BO and their relevance to product and materials design.},
  doi      = {10.1016/j.coche.2021.100728},
  groups   = {BO applications},
  url      = {https://www.sciencedirect.com/science/article/pii/S2211339821000605},
}

@Article{Vangelatos2021,
  author    = {Vangelatos, Zacharias and Sheikh, Haris Moazam and Marcus, Philip S. and Grigoropoulos, Costas P. and Lopez, Victor Z. and Flamourakis, George and Farsari, Maria},
  title     = {{Strength through defects: A novel Bayesian approach for the optimization of architected materials}},
  journal   = {Science Advances},
  year      = {2021},
  volume    = {7},
  number    = {41},
  pages     = {eabk2218},
  month     = aug,
  abstract  = {A substantial improvement in the strength of a material is made with a new optimization method applicable to many science problems. We use a previously unexplored Bayesian optimization framework, ?evolutionary Monte Carlo sampling,? to systematically design the arrangement of defects in an architected microlattice to maximize its strain energy density before undergoing catastrophic failure. Our algorithm searches a design space with billions of 4 ? 4 ? 5 3D lattices, yet it finds the global optimum with only 250 cost function evaluations. Our optimum has a normalized strain energy density 12,464 times greater than its commonly studied defect-free counterpart. Traditional optimization is inefficient for this microlattice because (i) the design space has discrete, qualitative parameter states as input variables, (ii) the cost function is computationally expensive, and (iii) the design space is large. Our proposed framework is useful for architected materials and for many optimization problems in science and elucidates how defects can enhance the mechanical performance of architected materials.},
  comment   = {doi: 10.1126/sciadv.abk2218},
  doi       = {10.1126/sciadv.abk2218},
  groups    = {BO applications},
  publisher = {American Association for the Advancement of Science},
  url       = {https://doi.org/10.1126/sciadv.abk2218},
}

@Article{Ueno2016,
  author   = {Ueno, Tsuyoshi and Rhone, Trevor David and Hou, Zhufeng and Mizoguchi, Teruyasu and Tsuda, Koji},
  title    = {{COMBO: An efficient Bayesian optimization library for materials science}},
  journal  = {Materials Discovery},
  year     = {2016},
  volume   = {4},
  pages    = {18--21},
  issn     = {2352-9245},
  abstract = {In many subfields of chemistry and physics, numerous attempts have been made to accelerate scientific discovery using data-driven experimental design algorithms. Among them, Bayesian optimization has been proven to be an effective tool. A standard implementation (e.g., scikit-learn), however, can accommodate only small training data. We designed an efficient protocol for Bayesian optimization that employs Thompson sampling, random feature maps, one-rank Cholesky update and automatic hyperparameter tuning, and implemented it as an open-source python library called COMBO (COMmon Bayesian Optimization library). Promising results using COMBO to determine the atomic structure of a crystalline interface are presented. COMBO is available at https://github.com/tsudalab/combo.},
  doi      = {10.1016/j.md.2016.04.001},
  groups   = {BO applications},
  keywords = {Bayesian optimization, Python library, Global optimization, Materials design},
  url      = {https://www.sciencedirect.com/science/article/pii/S2352924516300035},
}

@Article{Tran2019,
  author   = {Tran, Anh and Tran, Minh and Wang, Yan},
  title    = {{Constrained mixed-integer Gaussian mixture Bayesian optimization and its applications in designing fractal and auxetic metamaterials}},
  journal  = {Structural and Multidisciplinary Optimization},
  year     = {2019},
  volume   = {59},
  number   = {6},
  pages    = {2131--2154},
  issn     = {1615-1488},
  abstract = {Bayesian optimization (BO) is a global optimization method that has the potential for design optimization. However, in classical BO algorithm, the variables are considered as continuous. In real-world engineering problems, both continuous and discrete variables are present. In this work, an efficient approach to incorporate discrete variables to BO is proposed. In the proposed constrained mixed-integer BO method, the sample set is decomposed into smaller clusters during sequential sampling, where each cluster corresponds to a unique ordered set of discrete variables, and a Gaussian process regression (GP) metamodel is constructed for each cluster. The model prediction is formed as the Gaussian mixture model, where the weights are computed based on the pair-wise Wasserstein distance between clusters and gradually converge to an independent GP as the optimization process advances. The definition of neighborhood can be flexibly and manually defined to account for independence between clusters, such as in the case of categorical variables. Theoretical results are provided in concert with two numerical and engineering examples, and two examples for metamaterial developments, including one fractal and one auxetic metamaterials, where the effective properties depend on both the geometry and the bulk material properties.},
  doi      = {10.1007/s00158-018-2182-1},
  groups   = {BO applications},
  refid    = {Tran2019},
  url      = {https://doi.org/10.1007/s00158-018-2182-1},
}

@Article{Torun2018,
  author      = {Torun, H. M. and Swaminathan, M. and Davis, A. Kavungal and Bellaredj, M. L. F.},
  title       = {A Global {B}ayesian Optimization Algorithm and Its Application to Integrated System Design},
  journal     = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
  year        = {2018},
  volume      = {26},
  number      = {4},
  pages       = {792--802},
  issn        = {1557-9999},
  call-number = {26},
  doi         = {10.1109/TVLSI.2017.2784783},
  groups      = {BO applications},
}

@Article{ShiR2021,
  author   = {Shi, Rui and Xu, Xinyue and Li, Jianmin and Li, Yanqiu},
  journal  = {Applied Soft Computing},
  title    = {{Prediction and analysis of train arrival delay based on XGBoost and Bayesian optimization}},
  year     = {2021},
  issn     = {1568-4946},
  pages    = {107538},
  volume   = {109},
  abstract = {Accurate train arrival delay prediction is critical for real-time train dispatching and for the improvement of the transportation service. This study proposes a data-driven method that combines eXtreme Gradient Boosting (XGBoost) and a Bayesian optimization (BO) algorithm to predict train arrival delays. First, eleven characteristics that may affect the train arrival time at the next scheduled station are identified as independent variables. Second, an XGBoost prediction model that captures the relation between the train arrival delays and various railway system characteristics is established. Third, the BO algorithm is applied to the hyperparameter optimization of the XGBoost model to improve the prediction accuracy. Subsequently, case studies using data from two high-speed railway (HSR) lines in China are performed to analyze the prediction efficiency and accuracy of the proposed model for different delay bins and at different stations. The results on two HSR lines demonstrate that the proposed method outperforms other benchmark models regarding the performance metrics of the determination coefficient (0.9889/0.9905), root-mean-squared error (2.686/1.887), and mean absolute error (0.896/ 0.802). In addition, the statistical test is carried out using Friedman Test (FT) and Wilcoxon Signed Rank Test (WSRT) to validate the efficacy of the proposed method. Furthermore, the train arrival delays at different abnormal events can also be accurately forecasted using the proposed method; the results indicate that the proposed method outperforms other benchmark methods, especially in the prediction of long delays caused by specific abnormal events.},
  doi      = {10.1016/j.asoc.2021.107538},
  groups   = {BO applications},
  keywords = {High-speed railway, Train arrival delay prediction, Extreme gradient boosting machine, Bayesian optimization, Abnormal events},
  url      = {https://www.sciencedirect.com/science/article/pii/S1568494621004610},
}

@Article{Roussel2021,
  author    = {Roussel, Ryan and Hanuka, Adi and Edelen, Auralee},
  journal   = {Physical Review Accelerators and Beams},
  title     = {{Multiobjective Bayesian optimization for online accelerator tuning}},
  year      = {2021},
  month     = jun,
  pages     = {062801},
  volume    = {24},
  doi       = {10.1103/PhysRevAccelBeams.24.062801},
  groups    = {BO applications},
  issue     = {6},
  numpages  = {14},
  publisher = {American Physical Society},
  url       = {https://link.aps.org/doi/10.1103/PhysRevAccelBeams.24.062801},
}

@Article{Priem2020,
  author   = {Priem, R. and Bartoli, N. and Diouane, Y. and Sgueglia, A.},
  title    = {Upper trust bound feasibility criterion for mixed constrained {B}ayesian optimization with application to aircraft design},
  journal  = {Aerospace Science and Technology},
  year     = {2020},
  volume   = {105},
  pages    = {105980},
  issn     = {1270-9638},
  abstract = {Bayesian optimization methods have been successfully applied to black box optimization problems that are expensive to evaluate. In this paper, we adapt the so-called super efficient global optimization algorithm to solve more accurately mixed constrained problems. The proposed approach handles constraints by means of upper trust bound, the latter encourages exploration of the feasible domain by combining the mean prediction and the associated uncertainty function given by the Gaussian processes. On top of that, a refinement procedure, based on a learning rate criterion, is introduced to enhance the exploitation and exploration trade-off. We show the good potential of the approach on a set of numerical experiments. Finally, we present an application to conceptual aircraft configuration upon which we show the superiority of the proposed approach compared to a set of the state-of-the-art black box optimization solvers.},
  doi      = {10.1016/j.ast.2020.105980},
  groups   = {BO applications},
  keywords = {Global optimization, Mixed constrained optimization, Black box optimization, Bayesian optimization, Gaussian process},
  url      = {https://www.sciencedirect.com/science/article/pii/S1270963820306623},
}

@Article{Mathern2021,
  author   = {Mathern, Alexandre and Steinholtz, Olof Skogby and Sjöberg, Anders and Önnheim, Magnus and Ek, Kristine and Rempling, Rasmus and Gustavsson, Emil and Jirstrand, Mats},
  title    = {{Multi-objective constrained Bayesian optimization for structural design}},
  journal  = {Structural and Multidisciplinary Optimization},
  year     = {2021},
  volume   = {63},
  number   = {2},
  pages    = {689--701},
  issn     = {1615-1488},
  abstract = {The planning and design of buildings and civil engineering concrete structures constitutes a complex problem subject to constraints, for instance, limit state constraints from design codes, evaluated by expensive computations such as finite element (FE) simulations. Traditionally, the focus has been on minimizing costs exclusively, while the current trend calls for good trade-offs of multiple criteria such as sustainability, buildability, and performance, which can typically be computed cheaply from the design parameters. Multi-objective methods can provide more relevant design strategies to find such trade-offs. However, the potential of multi-objective optimization methods remains unexploited in structural concrete design practice, as the expensiveness of structural design problems severely limits the scope of applicable algorithms. Bayesian optimization has emerged as an efficient approach to optimizing expensive functions, but it has not been, to the best of our knowledge, applied to constrained multi-objective optimization of structural concrete design problems. In this work, we develop a Bayesian optimization framework explicitly exploiting the features inherent to structural design problems, that is, expensive constraints and cheap objectives. The framework is evaluated on a generic case of structural design of a reinforced concrete (RC) beam, taking into account sustainability, buildability, and performance objectives, and is benchmarked against the well-known Non-dominated Sorting Genetic Algorithm II (NSGA-II) and a random search procedure. The results show that the Bayesian algorithm performs considerably better in terms of rate-of-improvement, final solution quality, and variance across repeated runs, which suggests it is well-suited for multi-objective constrained optimization problems in structural design.},
  doi      = {10.1007/s00158-020-02720-2},
  groups   = {BO applications},
  refid    = {Mathern2021},
  url      = {https://doi.org/10.1007/s00158-020-02720-2},
}

@Article{Manheim2019,
  author    = {Manheim, Derek C. and Detwiler, Russell L.},
  title     = {Accurate and reliable estimation of kinetic parameters for environmental engineering applications: A global, multi objective, {B}ayesian optimization approach},
  journal   = {Methods X},
  year      = {2019},
  volume    = {6},
  pages     = {1398--1414},
  month     = jan,
  issn      = {2215-0161},
  comment   = {doi: 10.1016/j.mex.2019.05.035},
  doi       = {10.1016/j.mex.2019.05.035},
  groups    = {BO applications},
  publisher = {Elsevier},
  url       = {https://doi.org/10.1016/j.mex.2019.05.035},
}

@Article{QLiang2021,
  author   = {Liang, Qiaohao and Gongora, Aldair E. and Ren, Zekun and Tiihonen, Armi and Liu, Zhe and Sun, Shijing and Deneault, James R. and Bash, Daniil and Mekki-Berrada, Flore and Khan, Saif A. and Hippalgaonkar, Kedar and Maruyama, Benji and Brown, Keith A. and Fisher III, John and Buonassisi, Tonio},
  title    = {Benchmarking the performance of {B}ayesian optimization across multiple experimental materials science domains},
  journal  = {npj Computational Materials},
  year     = {2021},
  volume   = {7},
  number   = {1},
  pages    = {188},
  issn     = {2057-3960},
  abstract = {Bayesian optimization (BO) has been leveraged for guiding autonomous and high-throughput experiments in materials science. However, few have evaluated the efficiency of BO across a broad range of experimental materials domains. In this work, we quantify the performance of BO with a collection of surrogate model and acquisition function pairs across five diverse experimental materials systems. By defining acceleration and enhancement metrics for materials optimization objectives, we find that surrogate models such as Gaussian Process (GP) with anisotropic kernels and Random Forest (RF) have comparable performance in BO, and both outperform the commonly used GP with isotropic kernels. GP with anisotropic kernels has demonstrated the most robustness, yet RF is a close alternative and warrants more consideration because it is free from distribution assumptions, has smaller time complexity, and requires less effort in initial hyperparameter selection. We also raise awareness about the benefits of using GP with anisotropic kernels in future materials optimization campaigns.},
  doi      = {10.1038/s41524-021-00656-9},
  groups   = {BO applications},
  refid    = {Liang2021},
  url      = {https://doi.org/10.1038/s41524-021-00656-9},
}

@Article{Kuhn2022,
  author   = {Kuhn, Jannick and Spitz, Jonathan and Sonnweber-Ribic, Petra and Schneider, Matti and Böhlke, Thomas},
  title    = {Identifying material parameters in crystal plasticity by {B}ayesian optimization},
  journal  = {Optimization and Engineering},
  year     = {2022},
  volume   = {23},
  number   = {3},
  pages    = {1489--1523},
  issn     = {1573-2924},
  abstract = {In this work, we advocate using Bayesian techniques for inversely identifying material parameters for multiscale crystal plasticity models. Multiscale approaches for modeling polycrystalline materials may significantly reduce the effort necessary for characterizing such material models experimentally, in particular when a large number of cycles is considered, as typical for fatigue applications. Even when appropriate microstructures and microscopic material models are identified, calibrating the individual parameters of the model to some experimental data is necessary for industrial use, and the task is formidable as even a single simulation run is time consuming (although less expensive than a corresponding experiment). For solving this problem, we investigate Gaussian process based Bayesian optimization, which iteratively builds up and improves a surrogate model of the objective function, at the same time accounting for uncertainties encountered during the optimization process. We describe the approach in detail, calibrating the material parameters of a high-strength steel as an application. We demonstrate that the proposed method improves upon comparable approaches based on an evolutionary algorithm and performing derivative-free methods.},
  doi      = {10.1007/s11081-021-09663-7},
  groups   = {BO applications},
  refid    = {Kuhn2022},
  url      = {https://doi.org/10.1007/s11081-021-09663-7},
}

@InProceedings{Klein2017,
  author    = {Klein, Aaron and Falkner, Stefan and Bartels, Simon and Hennig, Philipp and Hutter, Frank},
  booktitle = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics},
  title     = {Fast {B}ayesian optimization of machine learning hyperparameters on large datasets},
  year      = {2017},
  pages     = {528--536},
  publisher = {PMLR},
  volume    = {54},
  groups    = {BO applications},
  issn      = {2640-3498},
  url       = {https://proceedings.mlr.press/v54/klein17a.html},
}

@Article{Jim2021,
  author    = {Jim, Timothy M. S. and Faza, Ghifari A. and Palar, Pramudita S. and Shimoyama, Koji},
  title     = {Bayesian Optimization of a Low-Boom Supersonic Wing Planform},
  journal   = {AIAA Journal},
  year      = {2021},
  volume    = {59},
  number    = {11},
  pages     = {4514--4529},
  month     = jul,
  issn      = {0001-1452},
  abstract  = {A surrogate-assisted methodology to accelerate the design space exploration process and multi-objective optimization of a notional low-drag, low-boom supersonic transport planform using Euler computational fluid dynamics with an empirical parasite drag addition and an augmented Burgers equation solver is demonstrated. The use of Kriging-based surrogate models, coupled with expected hypervolume improvement in a Kriging believer framework, allows efficient global optimization of the selected wing design parameters. The use of effective nondominated sampling is used to further improve the solutions found. The effects of parameterizing the planform of a baseline model (NASA?s 69 deg wing?body) with 6- and 11-variables are explored. Genetic and local optimizers are used to search the Kriging surrogates. The results show the tradeoff between planform shapes that efficiently use compression lift to increase the lift-to-drag ratio and a smooth undertrack pressure signature to reduce ground-level noise. Furthermore, correctly positioned wingtips may also be used to smooth the undertrack signature, reducing the sonic boom.},
  comment   = {doi: 10.2514/1.J060225},
  doi       = {10.2514/1.J060225},
  groups    = {BO applications},
  publisher = {American Institute of Aeronautics and Astronautics},
  url       = {https://doi.org/10.2514/1.J060225},
}

@Article{Do2022,
  author   = {Do, Bach and Ohsaki, Makoto},
  title    = {Proximal-exploration multi-objective {B}ayesian optimization for inverse identification of cyclic constitutive law of structural steels},
  journal  = {Structural and Multidisciplinary Optimization},
  year     = {2022},
  volume   = {65},
  number   = {7},
  pages    = {199},
  issn     = {1615-1488},
  abstract = {Despite its importance in seismic response analysis, solving an inverse problem to identify the cyclic elastoplastic parameters for structural steels using conventional optimization algorithms still demands a substantial computational cost of repeatedly carrying out many nonlinear analyses. The parameters are commonly identified based on experimental measures from a single loading history, leading them to be biased and giving inaccurate predictions of structural behavior under other loading histories. To address these issues, we formulate a multi-objective inverse problem that simultaneously minimizes the error functions representing the differences between simulated responses and those measured experimentally from various cyclic tests of a steel specimen or a structural component. We then propose proximal-exploration multi-objective Bayesian optimization (MOBO) for solving the formulated inverse problem, resulting in an approximate Pareto front of parameters while limiting the number of costly simulations. MOBO sorts an initial Pareto front and constructs Gaussian process (GP) models for the error functions from a training dataset. It then relies on the hypervolume of the current solutions, the GP models, and a proximal exploration surrounding the current best compromise parameters to formulate an acquisition function that guides the improvement of the current solutions intelligently. Two identification examples show that the parameters obtained from the multi-objective inverse problem can reduce the bias induced using a single loading history for identification. The robustness of MOBO as well as a good prediction performance of the best compromise solution of identified parameters are demonstrated.},
  doi      = {10.1007/s00158-022-03297-8},
  groups   = {BO applications},
  refid    = {Do2022},
  url      = {https://doi.org/10.1007/s00158-022-03297-8},
}

@Article{Do2021,
  author   = {Do, Bach and Ohsaki, Makoto and Yamakawa, Makoto},
  title    = {Bayesian optimization for robust design of steel frames with joint and individual probabilistic constraints},
  journal  = {Engineering Structures},
  year     = {2021},
  volume   = {245},
  pages    = {112859},
  issn     = {0141-0296},
  abstract = {This work proposes a Bayesian optimization (BO) method for solving multi-objective robust design optimization (RDO) problems of steel frames under aleatory uncertainty in external loads and material properties. Joint and individual probabilistic constrained RDO problems are formulated to consider two different ways the frame reaches its collapse state. Each problem involves three conflicting objective functions, namely, the total mass of the frame, the mean and variance of the maximum inter-story drift. Since the uncertain objective and probabilistic constraint functions of both problems are implicit within a finite element analysis program and the computation of the probabilistic constraints is an NP-hard problem, BO is used to guide the optimization process toward better solutions after it completes an iteration and offers a set of near Pareto-optimal solutions when it terminates. Specifically, Bayesian regression models called Gaussian processes (GPs) serve as surrogates for the structural responses. Two acquisition functions are then developed for the two RDO problems and a maximization problem of these functions is formulated as a mixed-integer nonlinear programming (MINLP) problem. A new random search coupled with simulated annealing is devised to solve the MINLP problem, thereby locating the most promising point in the input variable space at which the current solutions maximize their chance to be improved and the GP models are refined before the BO starts a new iteration. A test problem and two design examples show that exact or good Pareto-optimal solutions to the RDO problems can be found by the proposed method with 20 iterations.},
  doi      = {10.1016/j.engstruct.2021.112859},
  groups   = {BO applications},
  keywords = {Bayesian optimization, Robust design optimization, Probabilistic constraints, Steel frames},
  url      = {https://www.sciencedirect.com/science/article/pii/S0141029621010099},
}

@InProceedings{Bergstra2011,
  author    = {Bergstra, James and Bardenet, Rémi and Bengio, Yoshua and Kégl, Balázs},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Algorithms for hyper-parameter optimization},
  year      = {2011},
  pages     = {2546--2554},
  publisher = {NIPS 2011},
  volume    = {24},
  groups    = {BO applications},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2011/hash/86e8f7ab32cfd12577bc2619bc635690-Abstract.html},
}

@Article{Greenhill2020,
  author      = {Greenhill, S. and Rana, S. and Gupta, S. and Vellanki, P. and Venkatesh, S.},
  title       = {Bayesian Optimization for Adaptive Experimental Design: A Review},
  journal     = {IEEE Access},
  year        = {2020},
  volume      = {8},
  pages       = {13937--13948},
  issn        = {2169-3536},
  call-number = {8},
  doi         = {10.1109/ACCESS.2020.2966228},
  groups      = {BO applications},
}

@Article{Lei2021,
  author   = {Lei, Bowen and Kirk, Tanner Quinn and Bhattacharya, Anirban and Pati, Debdeep and Qian, Xiaoning and Arroyave, Raymundo and Mallick, Bani K.},
  title    = {Bayesian optimization with adaptive surrogate models for automated experimental design},
  journal  = {npj Computational Materials},
  year     = {2021},
  volume   = {7},
  number   = {1},
  pages    = {194},
  issn     = {2057-3960},
  abstract = {Bayesian optimization (BO) is an indispensable tool to optimize objective functions that either do not have known functional forms or are expensive to evaluate. Currently, optimal experimental design is always conducted within the workflow of BO leading to more efficient exploration of the design space compared to traditional strategies. This can have a significant impact on modern scientific discovery, in particular autonomous materials discovery, which can be viewed as an optimization problem aimed at looking for the maximum (or minimum) point for the desired materials properties. The performance of BO-based experimental design depends not only on the adopted acquisition function but also on the surrogate models that help to approximate underlying objective functions. In this paper, we propose a fully autonomous experimental design framework that uses more adaptive and flexible Bayesian surrogate models in a BO procedure, namely Bayesian multivariate adaptive regression splines and Bayesian additive regression trees. They can overcome the weaknesses of widely used Gaussian process-based methods when faced with relatively high-dimensional design space or non-smooth patterns of objective functions. Both simulation studies and real-world materials science case studies demonstrate their enhanced search efficiency and robustness.},
  doi      = {10.1038/s41524-021-00662-x},
  groups   = {BO applications},
  refid    = {Lei2021},
  url      = {https://doi.org/10.1038/s41524-021-00662-x},
}

@Article{Park2018,
  author   = {Park, Seongeon and Na, Jonggeol and Kim, Minjun and Lee, Jong Min},
  title    = {Multi-objective {B}ayesian optimization of chemical reactor design using computational fluid dynamics},
  journal  = {Computers {\&} Chemical Engineering},
  year     = {2018},
  volume   = {119},
  pages    = {25--37},
  issn     = {0098-1354},
  abstract = {This study presents a computational fluid dynamics (CFD) based optimal design tool for chemical reactors, in which multi-objective Bayesian optimization (MBO) is utilized to reduce the number of required CFD runs. Detailed methods used to automate the process by connecting CFD with MBO are also proposed. The developed optimizer was applied to minimize the power consumption and maximize the gas holdup in a gas-sparged stirred tank reactor, which has six design variables: the aspect ratio of the tank, the diameter and clearance of each of the two impellers, and the gas sparger. The saturated Pareto front is obtained after 100 iterations. The resulting Pareto front consists of many near-optimal designs with significantly enhanced performances compared to conventional reactors reported in the literature. We anticipate that this design approach can be applied to any process unit design problems that require a large number of CFD simulation runs.},
  doi      = {10.1016/j.compchemeng.2018.08.005},
  groups   = {BO applications},
  keywords = {Multi-objective optimization, Bayesian optimization, Computational fluid dynamics, CFD-based optimization, Reactor design, Machine learning},
  url      = {https://www.sciencedirect.com/science/article/pii/S0098135418301236},
}

@Article{Deshwal2021,
  author    = {Deshwal, Aryan and Simon, Cory M. and Doppa, Janardhan Rao},
  title     = {Bayesian optimization of nanoporous materials},
  journal   = {Molecular Systems Design {\&} Engineering},
  year      = {2021},
  volume    = {6},
  number    = {12},
  pages     = {1066--1086},
  abstract  = {Nanoporous materials (NPMs) could be used to store, capture, and sense many different gases. Given an adsorption task, we often wish to search a library of NPMs for the one with the optimal adsorption property. The high cost of NPM synthesis and gas adsorption measurements, whether these experiments are in the lab or in a simulation, often precludes exhaustive search. We explain, demonstrate, and advocate Bayesian optimization (BO) to actively search for the optimal NPM in a library of NPMs--and find it using the fewest experiments. The two ingredients of BO are a surrogate model and an acquisition function. The surrogate model is a probabilistic model reflecting our beliefs about the NPM-structure-property relationship based on observations from past experiments. The acquisition function uses the surrogate model to score each NPM according to the utility of picking it for the next experiment. It balances two competing goals: (a) exploitation of our current approximation of the structure-property relationship to pick the NPM we believe [under uncertainty] will be the highest-performing, and (b) exploration of regions of NPM space we have not visited, to pick an NPM we are uncertain about and improve our approximation of the structure-property relationship. We demonstrate BO by searching an open database of ∼70 000 hypothetical covalent organic frameworks (COFs) for the COF with the highest simulated methane deliverable capacity (pertinent for vehicular adsorbed natural gas storage). BO finds the optimal COF and acquires ∼30% of the top 100 highest-ranked COFs after evaluating only ∼140 COFs. More, BO searches more efficiently than evolutionary and one-shot supervised machine learning approaches.},
  doi       = {10.1039/D1ME00093D},
  groups    = {BO applications},
  publisher = {The Royal Society of Chemistry},
  url       = {http://dx.doi.org/10.1039/D1ME00093D},
}

@Article{Peng2023,
  author   = {Peng, Xuhao and Kou, Jiaqing and Zhang, Weiwei},
  title    = {Multi-fidelity nonlinear unsteady aerodynamic modeling and uncertainty estimation based on {H}erarchical {K}riging},
  journal  = {Applied Mathematical Modelling},
  year     = {2023},
  volume   = {122},
  pages    = {1--21},
  issn     = {0307-904X},
  abstract = {By fusing aerodynamic data from multiple sources, multi-fidelity methods can well balance model accuracy and computational cost. To extend multi-fidelity models for predicting unsteady aerodynamics with uncertainty estimation, an improved modelling framework based on the Hierarchical Kriging (HK) is proposed for nonlinear aerodynamic reduced-order modelling. This aerodynamic modelling framework is achieved by a two-stage modelling strategy. In the first stage, using the current and time-delayed low-fidelity aerodynamic load and pitching motion as input, a linear regression model is built to achieve preliminary aerodynamic prediction. In the second stage, the current and time-delayed preliminary prediction, as well as pitching motion history are used as the input of the Hierarchical Kriging model to predict the high-fidelity aerodynamic loads. The generalization capability of this improved modelling framework is firstly verified by several analytical examples. Furthermore, with three sets of training samples taken from harmonic pitching motions of an airfoil in transonic flow, the proposed model achieves accurate prediction on unsteady aerodynamics of other harmonic and random motions, and provides reliable uncertainty estimation of the predicted unsteady aerodynamics. In addition, the computational cost and prediction errors of multi-fidelity and the single-fidelity aerodynamic model are compared. Even with the same computational cost, the proposed multi-fidelity model still shows improved accuracy. Finally, using the uncertainty estimation of the proposed model, an active learning task is performed, which actively selects and incrementally adds the required training samples. With active learning, a good balance between model accuracy and cost of data acquisition is maintained, while avoiding over-fitting problems of the proposed model.},
  doi      = {10.1016/j.apm.2023.05.031},
  groups   = {Additive},
  keywords = {Data fusion, Unsteady aerodynamics, Transonic flow, Hierarchical Kriging, Uncertainty estimation},
  url      = {https://www.sciencedirect.com/science/article/pii/S0307904X23002329},
}

@Article{March2011,
  author    = {March, A. and Willcox, K. and Wang, Q.},
  journal   = {The Aeronautical Journal},
  title     = {Gradient-based multifidelity optimisation for aircraft design using {B}ayesian model calibration},
  year      = {2011},
  issn      = {0001-9240},
  number    = {1174},
  pages     = {729--738},
  volume    = {115},
  abstract  = {Optimisation of complex systems frequently requires evaluating a computationally expensive high-fidelity function to estimate a system metric of interest. Although design sensitivities may be available through either direct or adjoint methods, the use of formal optimisation methods may remain too costly. Incorporating low-fidelity performance estimates can substantially reduce the cost of the high-fidelity optimisation. In this paper we present a provably convergent multifidelity optimisation method that uses Cokriging Bayesian model calibration and first-order consistent trust regions. The technique is compared with a single-fidelity sequential quadratic programming method and a conventional first-order trust-region method on both a two-dimensional structural optimisation and an aerofoil design problem. In both problems adjoint formulations are used to provide inexpensive sensitivity information.},
  database  = {Cambridge Core},
  doi       = {10.1017/S0001924000006473},
  groups    = {Gradient-based},
  publisher = {Cambridge University Press},
  url       = {https://www.cambridge.org/core/article/gradientbased-multifidelity-optimisation-for-aircraft-design-using-bayesian-model-calibration/DD380C0E3F755279C3ECDE1987D7E99C},
}

@Article{De2020,
  author   = {De, Subhayan and Maute, Kurt and Doostan, Alireza},
  title    = {Bi-fidelity stochastic gradient descent for structural optimization under uncertainty},
  journal  = {Computational Mechanics},
  year     = {2020},
  volume   = {66},
  number   = {4},
  pages    = {745--771},
  issn     = {1432-0924},
  abstract = {The presence of uncertainty in material properties and geometry of a structure is ubiquitous. The design of robust engineering structures, therefore, needs to incorporate uncertainty in the optimization process. Stochastic gradient descent (SGD) method can alleviate the cost of optimization under uncertainty, which includes statistical moments of quantities of interest in the objective and constraints. However, the design may change considerably during the initial iterations of the optimization process which impedes the convergence of the traditional SGD method and its variants. In this paper, we present two SGD based algorithms, where the computational cost is reduced by employing a low-fidelity model in the optimization process. In the first algorithm, most of the stochastic gradient calculations are performed on the low-fidelity model and only a handful of gradients from the high-fidelity model is used per iteration, resulting in an improved convergence. In the second algorithm, we use gradients from the low-fidelity models to be used as control variate, a variance reduction technique, to reduce the variance in the search direction. These two bi-fidelity algorithms are illustrated first with a conceptual example. Then, the convergence of the proposed bi-fidelity algorithms is studied with two numerical examples of shape and topology optimization and compared to popular variants of the SGD method that do not use low-fidelity models. The results show that the proposed use of a bi-fidelity approach for the SGD method can improve the convergence. Two analytical proofs are also provided that show linear convergence of these two algorithms under appropriate assumptions.},
  doi      = {10.1007/s00466-020-01870-w},
  groups   = {Gradient-based},
  refid    = {De2020},
  url      = {https://doi.org/10.1007/s00466-020-01870-w},
}

@Article{Nachar2020,
  author   = {Nachar, Stéphane and Boucard, Pierre-Alain and Néron, David and Rey, Christian},
  title    = {Multi-fidelity {B}ayesian optimization using model-order reduction for viscoplastic structures},
  journal  = {Finite Elements in Analysis and Design},
  year     = {2020},
  volume   = {176},
  pages    = {103400},
  issn     = {0168-874X},
  abstract = {One of the main issues when dealing with the numerical optimization of mechanical structures is the balance between computation time and model accuracy. The work presented herein aims at accelerating global optimization by using the framework of Bayesian optimization on a quantity of interest together with multiple levels of fidelity. These multi-fidelity data are generated from a model-order reduction framework: the LATIN Proper Generalized Decomposition. Within this framework, a reduced-order basis is generated on-the-fly and re-exploited to reduce the computational cost of observations. This strategy is illustrated on two elasto-viscoplastic test-cases for which significant speedups can be observed.},
  doi      = {10.1016/j.finel.2020.103400},
  groups   = {Sequential model-based},
  keywords = {Bayesian optimization, Multi-fidelity kriging, Reduced-order models},
  url      = {https://www.sciencedirect.com/science/article/pii/S0168874X20300809},
}

@Article{WuW2021,
  author   = {Wu, Wensi and Bonneville, Christophe and Earls, Christopher},
  journal  = {Finite Elements in Analysis and Design},
  title    = {A principled approach to design using high fidelity fluid-structure interaction simulations},
  year     = {2021},
  issn     = {0168-874X},
  pages    = {103562},
  volume   = {194},
  abstract = {A high fidelity fluid-structure interaction simulation may require many days to run, on hundreds of cores. This poses a serious burden, both in terms of time and economic considerations, when repetitions of such simulations may be required (e.g. for the purpose of design optimization). In this paper we present strategies based on (constrained) Bayesian optimization (BO) to alleviate this burden. BO is a numerical optimization technique based on Gaussian processes (GP) that is able to efficiently (with minimal calls to the expensive FSI models) converge towards some globally working admissible design, as gauged using a black box objective function. In this study we present a principled design evolution that moves from FSI model verification, through a series of Bridge Simulations (bringing the verification case incrementally closer to the application), in order that we may identify material properties for an underwater, unmanned, autonomous vehicle (UUAV) sail plane. We are able to achieve fast convergence towards an working admissible design, using a small number of FSI simulations (a dozen at most), even when selecting over several design parameters, and while respecting optimization constraints.},
  doi      = {10.1016/j.finel.2021.103562},
  groups   = {Sequential model-based},
  keywords = {Bayesian optimization, Fluid-structure interaction, Gaussian process, Machine learning, Design optimization},
  url      = {https://www.sciencedirect.com/science/article/pii/S0168874X21000469},
}

@Article{Wiangkham2023,
  author   = {Wiangkham, Attasit and Ariyarit, Atthaphon and Timtong, Anantaya and Aengchuan, Prasert},
  title    = {{Multi-fidelity model using GRNN and ANFIS algorithms-based fracture criterion for predicting mixed-mode I-II of sugarcane leaves/epoxy composite}},
  journal  = {Theoretical and Applied Fracture Mechanics},
  year     = {2023},
  volume   = {125},
  pages    = {103892},
  issn     = {0167-8442},
  abstract = {Artificial intelligence plays a huge role in solving engineering problems. In the fracture mechanics field, artificial intelligence is used to predict fracture behavior or parameters, based on testing data, which is a destructive type of testing, so it is difficult to get a large amount of data for the learning modeling process. Therefore, this study presents artificial intelligence modeling to predict the fracture toughness by reducing the use of the actual testing fracture toughness data, while replacing the data with a sufficient quantity of data by fracture toughness prediction from the fracture criterion, for which artificial intelligence was modeled by the concept called the “multi-fidelity model”. The concept of the modeling begins with calculating and predicting the difference between the actual data set and the criterion data set, then combining the data sets, which is the data from the criterion performed mathematically with the data from the above difference prediction to the actual data set to create a model based on the increased data volume of combining the two data sets. In this study, the mixed-mode I-II fracture toughness of sugarcane leaves/epoxy composite with different mixing conditions was performed on an inclined crack specimen via the three-point bending test configuration used as the actual data set, while the generalized maximum tangential stress criterion (GMTS), which is preliminary fracture toughness, was used as criterion data. The multi-fidelity model is proposed via different artificial intelligence algorithms, namely the general regression neural network (GRNN) and the adaptive neuro-fuzzy inference system (ANFIS). Once the modeling process was complete, the performance metrics showed a clear increase in the efficiency of multi-fidelity models compared to the original artificial intelligence models.},
  doi      = {10.1016/j.tafmec.2023.103892},
  groups   = {Additive},
  keywords = {Mixed-mode I-II, Composite, Fracture criteria, Artificial intelligence, Multi-fidelity model},
  url      = {https://www.sciencedirect.com/science/article/pii/S0167844223001556},
}

@Article{Do2023,
  author   = {Do, Bach and Ohsaki, Makoto},
  title    = {Bayesian optimization-assisted approximate {B}ayesian computation and its application to identifying cyclic constitutive law of structural steels},
  journal  = {Computers {\&} Structures},
  year     = {2023},
  volume   = {286},
  pages    = {107111},
  issn     = {0045-7949},
  abstract = {A costly finite element model discourages Bayesian inference of the underlying parameters from noise-corrupted experimental datasets. This arises because the likelihood of such a model is often computationally intractable, leading to the impossibility of performing a large number of costly simulations. This study presents Bayesian optimization-assisted approximate Bayesian computation (BO-assisted ABC) and showcases its application to identifying the approximate posteriors of parameters for known statistical models and of cyclic elastoplastic parameters for structural steels. ABC bypasses likelihood evaluations by generating prior samples that are assigned as samples constituting the posterior if discrepancies between the experimental dataset and the corresponding simulated datasets do not exceed a small, positive threshold. With a modest number of costly simulations, BO facilitates ABC by intelligently constructing a Gaussian process model that approximates the discrepancy mean function. Identification results from illustrative examples show that the approximate posteriors by the proposed approach not only reproduce a given posterior with acceptable accuracy but also capture the true nominal parameters of a statistical model. Moreover, the approximate posteriors of material parameters can simulate the cyclic elastoplastic behavior of a steel specimen under different loading conditions. The dependence of identification results on definitions of BO acquisition function is also investigated.},
  doi      = {10.1016/j.compstruc.2023.107111},
  groups   = {BO applications},
  keywords = {Cyclic constitutive law, Bayesian inference, Approximate Bayesian computation, Bayesian optimization, Parameter identification, Structural steels},
  url      = {https://www.sciencedirect.com/science/article/pii/S0045794923001414},
}

@Article{Yoo2021,
  author   = {Yoo, Kwangkyu and Bacarreza, Omar and Aliabadi, M. H. Ferri},
  journal  = {Computers {\&} Structures},
  title    = {Multi-fidelity probabilistic optimisation of composite structures under thermomechanical loading using {G}aussian processes},
  year     = {2021},
  issn     = {0045-7949},
  pages    = {106655},
  volume   = {257},
  abstract = {A multi-fidelity probabilistic optimisation method for the design of composite structures subjected to thermomechanical loading is introduced in this work for the first time. The proposed multi-fidelity approach offers considerable computation efficiency as well as sufficient accuracy, enabling probabilistic optimisation to include more design variables in the early design phase. This approach incorporates both nonlinear information fusion algorithms and multi-level optimisation to achieve increased accuracy and computation time savings. In this optimisation process, a High-Fidelity Model (HFM) covers only a part of the entire design space with information collected uniformly while providing high-fidelity information of other design spaces sparsely without causing extra computational cost. Simultaneously, a Low-Fidelity Model (LFM) explores the whole design space to compensate lack of high-fidelity information. In this manner, the number of high-fidelity information to construct a multi-fidelity model is dramatically reduced. The Reliability-Based Design Optimisation (RBDO) demonstrated the proposed multi-fidelity method of a mono-stringer stiffened composite panel under thermomechanical loading using Gaussian Processes (GPs).},
  doi      = {10.1016/j.compstruc.2021.106655},
  groups   = {MF optimization under uncertainty},
  keywords = {Multi-fidelity, Composites, Gaussian processes, Reliability-based design optimisation, Thermomechanical load},
  url      = {https://www.sciencedirect.com/science/article/pii/S0045794921001772},
}

@Article{Werner2023,
  author    = {Werner, Steffen W. R. and Overton, Michael L. and Peherstorfer, Benjamin},
  title     = {Multifidelity Robust Controller Design with Gradient Sampling},
  journal   = {SIAM Journal on Scientific Computing},
  year      = {2023},
  volume    = {45},
  number    = {2},
  pages     = {A933--A957},
  month     = apr,
  issn      = {1064-8275},
  abstract  = {Abstract. Robust controllers that stabilize dynamical systems even under disturbances and noise are often formulated as solutions of nonsmooth, nonconvex optimization problems. While methods such as gradient sampling can handle the nonconvexity and nonsmoothness, the costs of evaluating the objective function may be substantial, making robust control challenging for dynamical systems with high-dimensional state spaces. In this work, we introduce multifidelity variants of gradient sampling that leverage low-cost, low-fidelity models with low-dimensional state spaces for speeding up the optimization process while nonetheless providing convergence guarantees for a high-fidelity model of the system of interest, which is primarily accessed in the last phase of the optimization process. Our first multifidelity method initiates gradient sampling on higher-fidelity models with starting points obtained from cheaper, lower-fidelity models. Our second multifidelity method relies on ensembles of gradients that are computed from low- and high-fidelity models. Numerical experiments with controlling the cooling of a steel rail profile and laminar flow in a cylinder wake demonstrate that our new multifidelity gradient sampling methods achieve up to two orders of magnitude speedup compared to the single-fidelity gradient sampling method that relies on the high-fidelity model alone.},
  comment   = {doi: 10.1137/22M1500137},
  doi       = {10.1137/22M1500137},
  groups    = {MF optimization under uncertainty},
  publisher = {Society for Industrial and Applied Mathematics},
  url       = {https://doi.org/10.1137/22M1500137},
}

@Article{Lin2022,
  author   = {Lin, Quan and Gong, Lintao and Zhang, Yunlong and Kou, Meng and Zhou, Qi},
  title    = {A probability of improvement-based multi-fidelity robust optimization approach for aerospace products design},
  journal  = {Aerospace Science and Technology},
  year     = {2022},
  volume   = {128},
  pages    = {107764},
  issn     = {1270-9638},
  abstract = {Uncertainty is inevitable in aerospace products design optimization, which may lead to the undesired performance of aerospace products and even infeasible designs. Robust design optimization (RDO) aims to obtain a solution with a desirable performance mean and is insensitive to uncertainty. However, it is computationally intensive during the RDO process, which is unaffordable for aerospace product design optimization. Multi-fidelity (MF) surrogate modeling is a widely used strategy to alleviate the heavy computational burden in RDO problems. The existing MF surrogate modeling-based RDO approaches, however, generate the samples in a one-shot way, which requires sufficient samples distributed in the entire design space. In this work, a probability of improvement-based adaptive sampling approach is proposed for multi-fidelity robust design optimization. The multi-level hierarchical Kriging (MHK) model is used for multi-fidelity modeling, which allows the proposed approach to deal with data of multiple fidelities. Both design variable uncertainty and interpolation uncertainty are considered within the approach. An extended PI (EPI) function is developed to simultaneously select the design location and fidelity level of the updated sample. To deal with RDO problems with constraints, the proposed EPI function is extended by combining with the probability of feasibility functions. The proposed approach is demonstrated using four numerical examples and an engineering example of robust design optimization for a micro-aerial vehicle fuselage.},
  doi      = {10.1016/j.ast.2022.107764},
  groups   = {MF optimization under uncertainty},
  keywords = {Robust design optimization, Multi-fidelity surrogate, Adaptive sampling, Probability of improvement},
  url      = {https://www.sciencedirect.com/science/article/pii/S1270963822004382},
}

@Article{TaoJ2019,
  author   = {Tao, Jun and Sun, Gang},
  journal  = {Aerospace Science and Technology},
  title    = {Application of deep learning based multi-fidelity surrogate model to robust aerodynamic design optimization},
  year     = {2019},
  issn     = {1270-9638},
  pages    = {722--737},
  volume   = {92},
  abstract = {In the present work, a multi-fidelity surrogate-based optimization framework is proposed, and then applied to the robust optimizations for airfoil and wing under uncertainty of Mach number. DBN (deep belief network) is employed as the low-fidelity model, and the k-step contrastive divergence algorithm is used for training the network. By virtue of the well trained DBN model and high-fidelity data, a linear regression multi-fidelity surrogate model is established. Verification results indicate that the multi-fidelity surrogate model obtains more accurate predictions than the DBN model and is highly reliable as a prediction model. The multi-fidelity surrogate model is embedded into an improved PSO (particle swarm optimization) algorithm framework, and is updated in each iteration of the robust optimization processes for both airfoil and wing. Comparisons between multi-fidelity surrogate predictions and CFD results indicate that, the multi-fidelity surrogate predictions tend to approach the CFD results as the iteration number increases. The robust optimization results of airfoil and wing demonstrate that, the multi-fidelity surrogate model performs very well as a prediction model, and improves the optimization efficiency obviously.},
  doi      = {10.1016/j.ast.2019.07.002},
  groups   = {MF optimization under uncertainty},
  keywords = {Multi-fidelity surrogate model, Deep learning, Deep belief network, Improved PSO algorithm, Aerodynamic design optimization},
  url      = {https://www.sciencedirect.com/science/article/pii/S1270963819306686},
}

@Article{Shah2015,
  author   = {Shah, Harsheel and Hosder, Serhat and Koziel, Slawomir and Tesfahunegn, Yonatan A. and Leifsson, Leifur},
  title    = {Multi-fidelity robust aerodynamic design optimization under mixed uncertainty},
  journal  = {Aerospace Science and Technology},
  year     = {2015},
  volume   = {45},
  pages    = {17--29},
  issn     = {1270-9638},
  abstract = {The objective of this paper is to present a robust optimization algorithm for computationally efficient airfoil design under mixed (inherent and epistemic) uncertainty using a multi-fidelity approach. This algorithm exploits stochastic expansions derived from the Non-Intrusive Polynomial Chaos (NIPC) technique to create surrogate models utilized in the optimization process. A combined NIPC expansion approach is used, where both the design and the mixed uncertain parameters are the independent variables of the surrogate model. To reduce the computational cost, the high-fidelity Computational Fluid Dynamics (CFD) model is replaced by a suitably corrected low-fidelity one, the latter being evaluated using the same CFD solver but with a coarser mesh. The model correction is implemented to the low-fidelity CFD solutions utilized for the construction of stochastic surrogate by using multi-point Output Space Mapping (OSM) technique. The proposed algorithm is applied to the design of NACA 4-digit airfoils with four deterministic design variables (the airfoil shape parameters and the angle of attack), one aleatory uncertain variable (the Mach number) and one epistemic variable (β, a geometry parameter) to demonstrate robust optimization under mixed uncertainties. In terms of computational cost, the proposed technique outperforms the conventional approach that exclusively uses the high-fidelity model to create the surrogates. The design cost reduces to only 34 equivalent high-fidelity model evaluations versus 168 obtained with the conventional method.},
  doi      = {10.1016/j.ast.2015.04.011},
  groups   = {MF optimization under uncertainty},
  keywords = {Multi-fidelity modeling, Polynomial chaos, Computational fluid dynamics, Aerodynamic shape optimization, Robust design, Output space mapping},
  url      = {https://www.sciencedirect.com/science/article/pii/S1270963815001261},
}

@Article{Chakraborty2017,
  author   = {Chakraborty, Souvik and Chatterjee, Tanmoy and Chowdhury, Rajib and Adhikari, Sondipon},
  title    = {A surrogate based multi-fidelity approach for robust design optimization},
  journal  = {Applied Mathematical Modelling},
  year     = {2017},
  volume   = {47},
  pages    = {726--744},
  issn     = {0307-904X},
  abstract = {Robust design optimization (RDO) is a field of optimization in which certain measure of robustness is sought against uncertainty. Unlike conventional optimization, the number of function evaluations in RDO is significantly more which often renders it time consuming and computationally cumbersome. This paper presents two new methods for solving the RDO problems. The proposed methods couple differential evolution algorithm (DEA) with polynomial correlated function expansion (PCFE). While DEA is utilized for solving the optimization problem, PCFE is utilized for calculating the statistical moments. Three examples have been presented to illustrate the performance of the proposed approaches. Results obtained indicate that the proposed approaches provide accurate and computationally efficient estimates of the RDO problems. Moreover, the proposed approaches outperforms popular RDO techniques such as tensor product quadrature, Taylor’s series and Kriging. Finally, the proposed approaches have been utilized for robust hydroelectric flow optimization, demonstrating its capability in solving large scale problems.},
  doi      = {10.1016/j.apm.2017.03.040},
  groups   = {MF optimization under uncertainty},
  keywords = {Robust design optimization, Polynomial correlated function expansion, Differential evolution algorithm, Stochastic computation},
  url      = {https://www.sciencedirect.com/science/article/pii/S0307904X17301890},
}

@Article{Fusi2015,
  author    = {Fusi, Francesca and Guardone, Alberto and Quaranta, Giuseppe and Congedo, Pietro M.},
  title     = {Multifidelity Physics-Based Method for Robust Optimization Applied to a Hovering Rotor Airfoil},
  journal   = {AIAA Journal},
  year      = {2015},
  volume    = {53},
  number    = {11},
  pages     = {3448--3465},
  month     = jul,
  issn      = {0001-1452},
  abstract  = {The paper presents a multifidelity robust optimization technique with application to the design of rotor blade airfoils in hover. A genetic algorithm is coupled with a non-intrusive uncertainty propagation technique based on polynomial chaos expansion to determine the robust optimal airfoils that maximize the mean value of the lift-to-drag ratio while minimizing the variance, under uncertain operating conditions. Uncertainties on the blade pitch angle and induced velocity are considered. To deal with the variable operating conditions induced by the considered uncertainties and to alleviate the computational cost of the optimization procedure, a multifidelity strategy is developed that exploits two aerodynamic models of different fidelity. The two models correspond to different physical descriptions of the flowfield around the airfoil; thus, the multifidelity method employs the low-fidelity model in regions of the stochastic space where the physics of the problem is well captured by the model, and it switches to high-fidelity estimates only where needed. The proposed robust optimization technique is compared with the robust optimization based on the high-fidelity aerodynamic model and the deterministic optimization, to assess the capability of finding a consistent Pareto set and to evaluate the numerical efficiency. The results obtained show how the robust multifidelity approach is effective in reducing the sensitivity of the designed airfoils with respect to variation in the operating conditions.},
  comment   = {doi: 10.2514/1.J053952},
  doi       = {10.2514/1.J053952},
  groups    = {MF optimization under uncertainty},
  publisher = {American Institute of Aeronautics and Astronautics},
  url       = {https://doi.org/10.2514/1.J053952},
}

@Article{Ng2014,
  author    = {Ng, Leo W. T. and Willcox, Karen},
  title     = {Multifidelity approaches for optimization under uncertainty},
  journal   = {International Journal for Numerical Methods in Engineering},
  year      = {2014},
  volume    = {100},
  number    = {10},
  pages     = {746--772},
  month     = dec,
  issn      = {0029-5981},
  abstract  = {SUMMARYIt is important to design robust and reliable systems by accounting for uncertainty and variability in the design process. However, performing optimization in this setting can be computationally expensive, requiring many evaluations of the numerical model to compute statistics of the system performance at every optimization iteration. This paper proposes a multifidelity approach to optimization under uncertainty that makes use of inexpensive, low-fidelity models to provide approximate information about the expensive, high-fidelity model. The multifidelity estimator is developed based on the control variate method to reduce the computational cost of achieving a specified mean square error in the statistic estimate. The method optimally allocates the computational load between the two models based on their relative evaluation cost and the strength of the correlation between them. This paper also develops an information reuse estimator that exploits the autocorrelation structure of the high-fidelity model in the design space to reduce the cost of repeatedly estimating statistics during the course of optimization. Finally, a combined estimator incorporates the features of both the multifidelity estimator and the information reuse estimator. The methods demonstrate 90% computational savings in an acoustic horn robust optimization example and practical design turnaround time in a robust wing optimization problem. Copyright ? 2014 John Wiley \& Sons, Ltd.},
  doi       = {10.1002/nme.4761},
  groups    = {MF optimization under uncertainty},
  keywords  = {multifidelity, design under uncertainty, model reduction, optimization, probabilistic methods, stochastic problems},
  publisher = {John Wiley \& Sons, Ltd},
  url       = {https://doi.org/10.1002/nme.4761},
}

@Book{Zhou2023,
  author    = {Zhou, Qi and Zhao, Min and Hu, Jiexiang and Ma, Mengying},
  publisher = {Springer},
  title     = {{Multi-Fidelity Surrogates: Modeling, Optimization and Applications}},
  year      = {2023},
  address   = {Singapore},
  isbn      = {9811972109},
  groups    = {Books},
}

@Article{Chaudhuri2021,
  author   = {Chaudhuri, Anirban and Marques, Alexandre N. and Willcox, Karen},
  title    = {{mfEGRA: Multifidelity efficient global reliability analysis through active learning for failure boundary location}},
  journal  = {Structural and Multidisciplinary Optimization},
  year     = {2021},
  volume   = {64},
  number   = {2},
  pages    = {797--811},
  issn     = {1615-1488},
  abstract = {This paper develops mfEGRA, a multifidelity active learning method using data-driven adaptively refined surrogates for failure boundary location in reliability analysis. This work addresses the issue of prohibitive cost of reliability analysis using Monte Carlo sampling for expensive-to-evaluate high-fidelity models by using cheaper-to-evaluate approximations of the high-fidelity model. The method builds on the efficient global reliability analysis (EGRA) method, which is a surrogate-based method that uses adaptive sampling for refining Gaussian process surrogates for failure boundary location using a single- fidelity model. Our method introduces a two-stage adaptive sampling criterion that uses a multifidelity Gaussian process surrogate to leverage multiple information sources with different fidelities. The method combines expected feasibility criterion from EGRA with one-step lookahead information gain to refine the surrogate around the failure boundary. The computational savings from mfEGRA depends on the discrepancy between the different models, and the relative cost of evaluating the different models as compared to the high-fidelity model. We show that accurate estimation of reliability using mfEGRA leads to computational savings of $\sim $46% for an analytic multimodal test problem and 24% for a three-dimensional acoustic horn problem, when compared to single-fidelity EGRA. We also show the effect of using a priori drawn Monte Carlo samples in the implementation for the acoustic horn problem, where mfEGRA leads to computational savings of 45% for the three-dimensional case and 48% for a rarer event four-dimensional case as compared to single-fidelity EGRA.},
  doi      = {10.1007/s00158-021-02892-5},
  groups   = {MF reliability analysis},
  refid    = {Chaudhuri2021},
  url      = {https://doi.org/10.1007/s00158-021-02892-5},
}

@Article{ZhangC2022,
  author   = {Zhang, Chi and Song, Chaolin and Shafieezadeh, Abdollah},
  journal  = {Structural Safety},
  title    = {Adaptive reliability analysis for multi-fidelity models using a collective learning strategy},
  year     = {2022},
  issn     = {0167-4730},
  pages    = {102141},
  volume   = {94},
  abstract = {In many fields of science and engineering, models with different fidelities are available. Physical experiments or detailed simulations that accurately capture the behavior of the system are regarded as high-fidelity models with low model uncertainty; however, they are expensive to run. On the other hand, simplified physical experiments or numerical models are seen as low-fidelity models that are cheaper to evaluate. Although low-fidelity models are often not suitable for direct use in reliability analysis due to their low accuracy, they can offer information about the trend of the high-fidelity model thus providing the opportunity to explore the design space at a low cost. This study presents a new approach called adaptive multi-fidelity Gaussian process for reliability analysis (AMGPRA). Contrary to selecting training points and information sources in two separate stages as done in state-of-the-art mfEGRA method, the proposed approach finds the optimal training point and information source simultaneously using the novel collective learning function (CLF). CLF is able to assess the global impact of a candidate training point from an information source and it accommodates any learning function that satisfies a certain profile. In this context, CLF provides a new direction for quantifying the impact of new training points and can be easily extended with new learning functions to adapt to different reliability problems. The performance of the proposed method is demonstrated by three mathematical examples and one engineering problem concerning the wind reliability of transmission towers. It is shown that the proposed method achieves similar or higher accuracy with reduced computational costs compared to state-of-the-art single and multi-fidelity methods. A key application of AMGPRA is high-fidelity fragility modeling using complex and costly physics-based computational models.},
  doi      = {10.1016/j.strusafe.2021.102141},
  groups   = {MF reliability analysis},
  keywords = {Multi-fidelity models, Reliability analysis, Surrogate modelling, Adaptive Kriging, Gaussian process},
  url      = {https://www.sciencedirect.com/science/article/pii/S0167473021000643},
}

@Article{Patsialis2021,
  author   = {Patsialis, D. and Taflanidis, A. A.},
  title    = {{Multi-fidelity Monte Carlo for seismic risk assessment applications}},
  journal  = {Structural Safety},
  year     = {2021},
  volume   = {93},
  pages    = {102129},
  issn     = {0167-4730},
  abstract = {Modern probabilistic seismic risk assessment practices frequently require a considerable number of nonlinear time-history analyses of the structural behavior for estimating risk. Use of high-fidelity finite element models (FEMs) in this setting has the potential to accurately capture all essential features of the inelastic/hysteretic structural behavior but introduces a large associated computational burden. This study examines a multi-fidelity Monte Carlo (MC) implementation to alleviate this burden. Approach leverages low computational cost, biased evaluations from a low-fidelity numerical model to accelerate the MC estimation process, and uses simulations of the computationally expensive, high-fidelity FEM, to establish an unbiased MC estimation. It specifically exploits the correlation between the two models following a control variate formulation. The number of simulations from each of the models is optimally selected to minimize the variability of the MC-based risk predictions for the given computational budget. Since seismic risk assessment requires simultaneous estimation of the risk for multiple quantities of interest, related to different engineering demand parameters or different thresholds describing performance, guidelines for achieving satisfactory computational savings across all of them are discussed. As low-fidelity model, the reduced order modeling framework recently proposed by the authors is adopted in this study, though implementation can support any other modeling approach. The computational savings and accuracy improvement established by using the multi-fidelity estimator, when compared against the use of only the high- or low-fidelity models, is examined within an illustrative implementation that considers two structures, corresponding to different heights and materials, with high-fidelity FEMs developed in OpenSees.},
  doi      = {10.1016/j.strusafe.2021.102129},
  groups   = {MF reliability analysis},
  keywords = {Multi-fidelity Monte Carlo, Efficient seismic risk assessment, Hysteretic structural response, Reduced order modeling},
  url      = {https://www.sciencedirect.com/science/article/pii/S0167473021000539},
}

@Article{Skandalos2022,
  author   = {Skandalos, Konstantinos and Chakraborty, Souvik and Tesfamariam, Solomon},
  title    = {Seismic reliability analysis using a multi-fidelity surrogate model: Example of base-isolated buildings},
  journal  = {Structural Safety},
  year     = {2022},
  volume   = {97},
  pages    = {102222},
  issn     = {0167-4730},
  abstract = {An approach is presented for conducting simulation-based seismic reliability analysis using a multi-fidelity surrogate model. To illustrate the approach, a base-isolated building is considered comprising a reinforced concrete frame for primary building and lead-rubber bearings for isolation devices. The high-fidelity model is a nonlinear finite-element model used in time history analysis. Ground motions are generated from a parametric power spectral density function that is compatible with a target response spectrum at the site of interest. The low-fidelity model uses a linear-elastic stick model for the superstructure, while stochastic linearization is used to capture the nonlinearity of the base isolation. Cokriging is used as a multi-fidelity surrogate model for fusing a large number of low-fidelity model evaluations with few high-fidelity model evaluations, in order to attain high-fidelity model accuracy while mitigating the computational cost. After training the Cokriging model, Monte Carlo simulation is carried out by sampling the cheap multi-fidelity surrogate, and the probability of failure is subsequently evaluated for different response thresholds. The relative error in failure probability prediction of Cokriging is shown to be in the range of 6% with a maximum of 10%, for a critical range of sample response variable threshold values, in contrast to errors as high as 60% provided by the low-fidelity model.},
  doi      = {10.1016/j.strusafe.2022.102222},
  groups   = {MF reliability analysis},
  keywords = {Multi-fidelity modelling, Cokriging, Seismic reliability analysis, Base isolation},
  url      = {https://www.sciencedirect.com/science/article/pii/S0167473022000340},
}

@Article{Alsup2023,
  author    = {Alsup, Terrence and Peherstorfer, Benjamin},
  title     = {Context-Aware Surrogate Modeling for Balancing Approximation and Sampling Costs in Multifidelity Importance Sampling and Bayesian Inverse Problems},
  journal   = {SIAM/ASA Journal on Uncertainty Quantification},
  year      = {2023},
  volume    = {11},
  number    = {1},
  pages     = {285--319},
  month     = mar,
  abstract  = {Abstract. Multifidelity methods leverage low-cost surrogate models to speed up computations and make occasional recourse to expensive high-fidelity models to establish accuracy guarantees. Because surrogate and high-fidelity models are used together, poor predictions by surrogate models can be compensated with frequent recourse to high-fidelity models. Thus, there is a trade-off between investing computational resources to improve the accuracy of surrogate models versus simply making more frequent recourse to expensive high-fidelity models; however, this trade-off is ignored by traditional modeling methods that construct surrogate models that are meant to replace high-fidelity models rather than being used together with high-fidelity models. This work considers multifidelity importance sampling and theoretically and computationally trades off increasing the fidelity of surrogate models for constructing more accurate biasing densities and the numbers of samples that are required from the high-fidelity models to compensate poor biasing densities. Numerical examples demonstrate that such context-aware surrogate models for multifidelity importance sampling have lower fidelity than what typically is set as tolerance in traditional model reduction, leading to runtime speedups of up to one order of magnitude in the presented examples.},
  comment   = {doi: 10.1137/21M1445594},
  doi       = {10.1137/21M1445594},
  groups    = {MF_UQ},
  publisher = {Society for Industrial and Applied Mathematics},
  url       = {https://doi.org/10.1137/21M1445594},
}

@Article{Qian2018,
  author    = {Qian, E. and Peherstorfer, B. and O'Malley, D. and Vesselinov, V. V. and Willcox, K.},
  title     = {Multifidelity Monte Carlo Estimation of Variance and Sensitivity Indices},
  journal   = {SIAM/ASA Journal on Uncertainty Quantification},
  year      = {2018},
  volume    = {6},
  number    = {2},
  pages     = {683--706},
  month     = jan,
  abstract  = {Variance-based sensitivity analysis provides a quantitative measure of how uncertainty in a model input contributes to uncertainty in the model output. Such sensitivity analyses arise in a wide variety of applications and are typically computed using Monte Carlo estimation, but the many samples required for Monte Carlo to be sufficiently accurate can make these analyses intractable when the model is expensive. This work presents a multifidelity approach for estimating sensitivity indices that leverages cheaper low-fidelity models to reduce the cost of sensitivity analysis while retaining accuracy guarantees via recourse to the original, expensive model. This paper develops new multifidelity estimators for variance and for the Sobol' main and total effect sensitivity indices. We discuss strategies for dividing limited computational resources among models and specify a recommended strategy. Results are presented for the Ishigami function and a convection-diffusion-reaction model that demonstrate up to $10\times$ speedups for fixed convergence levels. For the problems tested, the multifidelity approach allows inputs to be definitively ranked in importance when Monte Carlo alone fails to do so.},
  comment   = {doi: 10.1137/17M1151006},
  doi       = {10.1137/17M1151006},
  groups    = {MF_UQ},
  publisher = {Society for Industrial and Applied Mathematics},
  url       = {https://doi.org/10.1137/17M1151006},
}

@Article{Peherstorfer2019,
  author    = {Peherstorfer, Benjamin},
  title     = {Multifidelity Monte Carlo Estimation with Adaptive Low-Fidelity Models},
  journal   = {SIAM/ASA Journal on Uncertainty Quantification},
  year      = {2019},
  volume    = {7},
  number    = {2},
  pages     = {579--603},
  month     = jan,
  abstract  = {Multifidelity Monte Carlo (MFMC) estimation combines low- and high-fidelity models to speed up the estimation of statistics of the high-fidelity model outputs. MFMC optimally samples the low- and high-fidelity models such that the MFMC estimator has minimal mean-squared error (MSE) for a given computational budget. In the setup of MFMC, the low-fidelity models are static; i.e., they are given and fixed and cannot be changed and adapted. We introduce the adaptive MFMC (AMFMC) method that splits the computational budget between adapting the low-fidelity models to improve their approximation quality and sampling the low- and high-fidelity models to reduce the MSE of the estimator. Our AMFMC approach derives the quasi-optimal balance between adaptation and sampling in the sense that our approach minimizes an upper bound of the MSE, instead of the error directly. We show that the quasi-optimal number of adaptations of the low-fidelity models is bounded even in the limit of an infinite budget. This shows that adapting low-fidelity models in MFMC beyond a certain approximation accuracy is unnecessary and can even be wasteful. Our AMFMC approach trades off adaptation and sampling and so avoids overadaptation of the low-fidelity models. Besides the costs of adapting low-fidelity models, our AMFMC approach can also take into account the costs of the initial construction of the low-fidelity models (``offline costs''), which is critical if low-fidelity models are computationally expensive to build such as reduced models and data-fit surrogate models. Numerical results demonstrate that our adaptive approach can achieve orders of magnitude speedups compared to MFMC estimators with static low-fidelity models and compared to Monte Carlo estimators that use the high-fidelity model alone.},
  comment   = {doi: 10.1137/17M1159208},
  doi       = {10.1137/17M1159208},
  groups    = {MF_UQ},
  publisher = {Society for Industrial and Applied Mathematics},
  url       = {https://doi.org/10.1137/17M1159208},
}

@Article{Perry2019,
  author    = {Perry, Daniel J. and Kirby, Robert M. and Narayan, Akil and Whitaker, Ross T.},
  title     = {Allocation Strategies for High Fidelity Models in the Multifidelity Regime},
  journal   = {SIAM/ASA Journal on Uncertainty Quantification},
  year      = {2019},
  volume    = {7},
  number    = {1},
  pages     = {203--231},
  month     = jan,
  abstract  = {We propose a novel approach to allocating resources for expensive simulations of high fidelity models when used in a multifidelity framework. Allocation decisions that distribute computational resources across several simulation models become extremely important in situations where only a small number of expensive high fidelity simulations can be run. We identify this allocation decision as a problem in optimal subset selection and subsequently regularize this problem so that solutions can be computed. Our regularized formulation yields a type of group lasso problem that has been studied in the literature to accomplish subset selection. Our numerical results compare the performance of algorithms that solve the group lasso problem for algorithmic allocation against a variety of other strategies, including those based on classical linear algebraic pivoting routines and those derived from more modern machine learning--based methods. We demonstrate on well-known synthetic problems and more difficult real-world simulations that this group lasso solution to the relaxed optimal subset selection problem performs better than the alternatives.},
  comment   = {doi: 10.1137/17M1144714},
  doi       = {10.1137/17M1144714},
  groups    = {MF_UQ},
  publisher = {Society for Industrial and Applied Mathematics},
  url       = {https://doi.org/10.1137/17M1144714},
}

@Article{Pham2022,
  author    = {Pham, Trung and Gorodetsky, Alex A.},
  title     = {Ensemble Approximate Control Variate Estimators: Applications to MultiFidelity Importance Sampling},
  journal   = {SIAM/ASA Journal on Uncertainty Quantification},
  year      = {2022},
  volume    = {10},
  number    = {3},
  pages     = {1250--1292},
  month     = sep,
  abstract  = {Abstract. The recent growth in multifidelity uncertainty quantification has given rise to a large set of variance reduction techniques that leverage information from model ensembles to provide variance reduction for estimates of the statistics of a high-fidelity model. In this paper we provide two contributions: (1) we utilize an ensemble estimator to account for uncertainties in the optimal weights of approximate control variate (ACV) approaches and derive lower bounds on the number of samples required to guarantee variance reduction; and (2) we extend an existing multifidelity importance sampling (MFIS) scheme to leverage control variates. Our approach directly addresses a limitation of many multifidelity sampling strategies that require the usage of pilot samples to estimate covariances. As such we make significant progress towards both increasing the practicality of approximate control variates?for instance, by accounting for the effect of pilot samples?and using multifidelity approaches more effectively for estimating low-probability events. The numerical results indicate our hybrid MFIS-ACV estimator achieves up to 50% improvement in variance reduction over the existing state-of-the-art MFIS estimator, which had already shown an outstanding convergence rate compared to the Monte Carlo method, on several problems of computational mechanics.},
  comment   = {doi: 10.1137/21M1390426},
  doi       = {10.1137/21M1390426},
  groups    = {MF_UQ},
  publisher = {Society for Industrial and Applied Mathematics},
  url       = {https://doi.org/10.1137/21M1390426},
}

@Article{Prescott2020,
  author    = {Prescott, Thomas P. and Baker, Ruth E.},
  title     = {Multifidelity Approximate Bayesian Computation},
  journal   = {SIAM/ASA Journal on Uncertainty Quantification},
  year      = {2020},
  volume    = {8},
  number    = {1},
  pages     = {114--138},
  month     = jan,
  abstract  = {A vital stage in the mathematical modeling of real-world systems is to calibrate a model's parameters to observed data. Likelihood-free parameter inference methods, such as approximate Bayesian computation (ABC), build Monte Carlo samples of the uncertain parameter distribution by comparing the data with large numbers of model simulations. However, the computational expense of generating these simulations forms a significant bottleneck in the practical application of such methods. We identify how simulations of corresponding cheap, low-fidelity models have been used separately in two complementary ways to reduce the computational expense of building these samples, at the cost of introducing additional variance to the resulting parameter estimates. We explore how these approaches can be unified so that cost and benefit are optimally balanced, and we characterize the optimal choice of how often to simulate from cheap, low-fidelity models in place of expensive, high-fidelity models in Monte Carlo ABC algorithms. The resulting early accept/reject multifidelity ABC algorithm that we propose is shown to give improved performance over existing multifidelity and high-fidelity approaches.},
  comment   = {doi: 10.1137/18M1229742},
  doi       = {10.1137/18M1229742},
  groups    = {MF_UQ},
  publisher = {Society for Industrial and Applied Mathematics},
  url       = {https://doi.org/10.1137/18M1229742},
}

@Article{Prescott2021,
  author    = {Prescott, Thomas P. and Baker, Ruth E.},
  title     = {Multifidelity Approximate Bayesian Computation with Sequential Monte Carlo Parameter Sampling},
  journal   = {SIAM/ASA Journal on Uncertainty Quantification},
  year      = {2021},
  volume    = {9},
  number    = {2},
  pages     = {788--817},
  month     = jan,
  abstract  = {Multifidelity approximate Bayesian computation (MF-ABC) is a likelihood-free technique for parameter inference that exploits model approximations to significantly increase the speed of ABC algorithms [T. P. Prescott and R. E. Baker, SIAM/ASA J. Uncertain. Quantif., 8 (2020), pp. 114--138]. Previous work has considered MF-ABC only in the context of rejection sampling, which does not explore parameter space particularly efficiently. In this work, we integrate the multifidelity approach with the ABC sequential Monte Carlo (ABC-SMC) algorithm into a new MF-ABC-SMC algorithm. We show that the improvements generated by each of ABC-SMC and MF-ABC to the efficiency of generating Monte Carlo samples and estimates from the ABC posterior are amplified when the two techniques are used together.},
  comment   = {doi: 10.1137/20M1316160},
  doi       = {10.1137/20M1316160},
  groups    = {MF_UQ},
  publisher = {Society for Industrial and Applied Mathematics},
  url       = {https://doi.org/10.1137/20M1316160},
}

@InProceedings{WuJ2020,
  author    = {Wu, Jian and Toscano-Palmerin, Saul and Frazier, Peter I. and Wilson, Andrew Gordon},
  booktitle = {Proceedings of The 35th Uncertainty in Artificial Intelligence Conference},
  title     = {{Practical multi-fidelity Bayesian optimization for hyperparameter tuning}},
  year      = {2020},
  pages     = {788--798},
  publisher = {PMLR},
  groups    = {Applications of MF modeling, MF BO applications},
  issn      = {2640-3498},
  url       = {https://proceedings.mlr.press/v115/wu20a.html},
}

@Article{Konrad2022,
  author   = {Konrad, Julia and Farcaş, Ionuţ-Gabriel and Peherstorfer, Benjamin and Di Siena, Alessandro and Jenko, Frank and Neckel, Tobias and Bungartz, Hans-Joachim},
  title    = {{Data-driven low-fidelity models for multi-fidelity Monte Carlo sampling in plasma micro-turbulence analysis}},
  journal  = {Journal of Computational Physics},
  year     = {2022},
  volume   = {451},
  pages    = {110898},
  issn     = {0021-9991},
  abstract = {The linear micro-instabilities driving turbulent transport in magnetized fusion plasmas (as well as the respective nonlinear saturation mechanisms) are known to be sensitive with respect to various physical parameters characterizing the background plasma and the magnetic equilibrium. Therefore, uncertainty quantification is essential for achieving predictive numerical simulations of plasma turbulence. However, the high computational costs of the required gyrokinetic simulations and the large number of parameters render standard Monte Carlo techniques intractable. To address this problem, we propose a multi-fidelity Monte Carlo approach in which we employ data-driven low-fidelity models that exploit the structure of the underlying problem such as low intrinsic dimension and anisotropic coupling of the stochastic inputs. The low-fidelity models are efficiently constructed via sensitivity-driven dimension-adaptive sparse grid interpolation using both the full set of uncertain inputs and subsets comprising only selected, important parameters. We illustrate the power of this method by applying it to two plasma turbulence problems with up to 14 stochastic parameters, demonstrating that it is up to four orders of magnitude more efficient than standard Monte Carlo methods measured in single-core performance, which translates into a runtime reduction from around eight days to one hour on 240 cores on parallel machines.},
  doi      = {10.1016/j.jcp.2021.110898},
  groups   = {MF_UQ},
  keywords = {Multi-fidelity Monte Carlo sampling, Plasma micro-turbulence, Sensitivity-driven adaptivity, Reduced-dimension low-fidelity models},
  url      = {https://www.sciencedirect.com/science/article/pii/S0021999121007932},
}

@Article{Fairbanks2020,
  author   = {Fairbanks, Hillary R. and Jofre, Lluís and Geraci, Gianluca and Iaccarino, Gianluca and Doostan, Alireza},
  title    = {Bi-fidelity approximation for uncertainty quantification and sensitivity analysis of irradiated particle-laden turbulence},
  journal  = {Journal of Computational Physics},
  year     = {2020},
  volume   = {402},
  pages    = {108996},
  issn     = {0021-9991},
  abstract = {Particle-laden turbulent flows subject to radiative heating are relevant in many applications, for example concentrated solar power receivers. Efficient and accurate simulations provide valuable insights and enable optimization of such systems. However, as there are many uncertainties inherent in such flows, uncertainty quantification is fundamental to improve the predictive capabilities of the numerical simulations. For large-scale, multi-physics problems exhibiting high-dimensional uncertainty, characterizing the stochastic solution presents a significant computational challenge as most strategies require a large number of high-fidelity solves. This requirement might result in an infeasible number of simulations when a typical converged high-fidelity simulation requires intensive computational resources. To reduce the cost of quantifying high-dimensional uncertainties, we investigate the application of a non-intrusive, bi-fidelity approximation to estimate statistics of quantities of interest associated with an irradiated particle-laden turbulent flow. This method exploits the low-rank structure of the solution to accelerate the stochastic sampling and approximation processes by means of cheaper-to-run, lower fidelity representations. The application of this bi-fidelity approximation results in accurate estimates of the quantities of interest statistics, while requiring a small number of high-fidelity model evaluations.},
  doi      = {10.1016/j.jcp.2019.108996},
  groups   = {MF_UQ},
  keywords = {Bi-fidelity approximation, Irradiated particle-laden turbulence, Low-rank approximation, Non-intrusive, Predictive computational science, Uncertainty quantification},
  url      = {https://www.sciencedirect.com/science/article/pii/S0021999119307016},
}

@Article{LYan2019,
  author   = {Yan, Liang and Zhou, Tao},
  title    = {Adaptive multi-fidelity polynomial chaos approach to {B}ayesian inference in inverse problems},
  journal  = {Journal of Computational Physics},
  year     = {2019},
  volume   = {381},
  pages    = {110--128},
  issn     = {0021-9991},
  abstract = {The polynomial chaos (PC) expansion has been widely used as a surrogate model in the Bayesian inference to speed up the Markov chain Monte Carlo (MCMC) calculations. However, the use of a PC surrogate introduces the modeling error, that may severely distort the estimate of the posterior distribution. This error can be corrected by increasing the order of the PC expansion, but the cost for building the surrogate may increase dramatically. In this work, we seek to address this challenge by proposing an adaptive procedure to construct a multi-fidelity PC surrogate. This new strategy combines (a large number of) low-fidelity surrogate model evaluations and (a small number of) high-fidelity model evaluations, yielding a locally adaptive multi-fidelity approach. Here the low-fidelity surrogate is chosen as the prior-based PC surrogate, while the high-fidelity model refers to the true forward model. The key idea is to construct and refine the multi-fidelity approach over a sequence of samples adaptively determined from data so that the approximation can eventually concentrate on the posterior distribution. We illustrate the performance of the proposed strategy through two nonlinear inverse problems. It is shown that the proposed adaptive multi-fidelity approach can improve significantly the accuracy, yet without a dramatic increase in computational complexity. The numerical results also indicate that our new algorithm can enhance the efficiency by several orders of magnitude compared to a standard MCMC approach using only the true forward model.},
  doi      = {10.1016/j.jcp.2018.12.025},
  groups   = {Additive},
  keywords = {Bayesian inverse problems, Multi-fidelity polynomial chaos, Surrogate modeling, Markov chain Monte Carlo},
  url      = {https://www.sciencedirect.com/science/article/pii/S0021999119300063},
}

@Article{Parussini2017,
  author   = {Parussini, L. and Venturi, D. and Perdikaris, P. and Karniadakis, G. E.},
  title    = {{Multi-fidelity Gaussian process regression for prediction of random fields}},
  journal  = {Journal of Computational Physics},
  year     = {2017},
  volume   = {336},
  pages    = {36--50},
  issn     = {0021-9991},
  abstract = {We propose a new multi-fidelity Gaussian process regression (GPR) approach for prediction of random fields based on observations of surrogate models or hierarchies of surrogate models. Our method builds upon recent work on recursive Bayesian techniques, in particular recursive co-kriging, and extends it to vector-valued fields and various types of covariances, including separable and non-separable ones. The framework we propose is general and can be used to perform uncertainty propagation and quantification in model-based simulations, multi-fidelity data fusion, and surrogate-based optimization. We demonstrate the effectiveness of the proposed recursive GPR techniques through various examples. Specifically, we study the stochastic Burgers equation and the stochastic Oberbeck-Boussinesq equations describing natural convection within a square enclosure. In both cases we find that the standard deviation of the Gaussian predictors as well as the absolute errors relative to benchmark stochastic solutions are very small, suggesting that the proposed multi-fidelity GPR approaches can yield highly accurate results.},
  doi      = {10.1016/j.jcp.2017.01.047},
  groups   = {Hybrid},
  keywords = {Gaussian random fields, Multi-fidelity modeling, Recursive co-kriging, Uncertainty quantification},
  url      = {https://www.sciencedirect.com/science/article/pii/S0021999117300633},
}

@Article{AshwinRenganathan2023,
  author   = {Ashwin Renganathan, S. and Rao, Vishwas and Navon, Ionel M.},
  title    = {{CAMERA: A method for cost-aware, adaptive, multifidelity, efficient reliability analysis}},
  journal  = {Journal of Computational Physics},
  year     = {2023},
  volume   = {472},
  pages    = {111698},
  issn     = {0021-9991},
  abstract = {Estimating probability of failure in aerospace systems is a critical requirement for flight certification and qualification. Failure probability estimation involves resolving tails of probability distributions, and Monte Carlo sampling methods are intractable when expensive high-fidelity simulations have to be queried. We propose a method to use models of multiple fidelities that trade accuracy for computational efficiency. Specifically, we propose the use of multifidelity Gaussian process models to efficiently fuse models at multiple fidelity, thereby offering a cheap surrogate model that emulates the original model at all fidelities. Furthermore, we propose a novel sequential acquisition function based experiment design framework that can automatically select samples from appropriate fidelity models to make predictions about quantities of interest at the highest fidelity. We use our proposed approach in an importance sampling setting and demonstrate our method on the failure level set and probability estimation on synthetic test functions and two real-world applications, namely, the reliability analysis of a gas turbine engine blade using a finite element method and a transonic aerodynamic wing test case using Reynolds-averaged Navier-Stokes equations. We show that our method predicts the failure boundary and probability more accurately and at a fraction of the computational cost compared with using just a single expensive high-fidelity model. Finally, we show that our sequential approach is guaranteed to asymptotically converge to the true failure boundary with high probability.},
  doi      = {10.1016/j.jcp.2022.111698},
  groups   = {MF reliability analysis},
  keywords = {Gaussian process regression, Sequential experiment design, Reliability analysis, Aircraft design and certification},
  url      = {https://www.sciencedirect.com/science/article/pii/S0021999122007616},
}

@Article{Wang2023,
  author     = {Wang, Xilu and Jin, Yaochu and Schmitt, Sebastian and Olhofer, Markus},
  journal    = {ACM Computing Surveys},
  title      = {Recent advances in {B}ayesian optimization},
  year       = {2023},
  issn       = {0360-0300},
  month      = {jul},
  number     = {13s},
  volume     = {55},
  abstract   = {Bayesian optimization has emerged at the forefront of expensive black-box optimization due to its data efficiency. Recent years have witnessed a proliferation of studies on the development of new Bayesian optimization algorithms and their applications. Hence, this article attempts to provide a comprehensive and updated survey of recent advances in Bayesian optimization that are mainly based on Gaussian processes and identify challenging open problems. We categorize the existing work on Bayesian optimization into nine main groups according to the motivations and focus of the proposed algorithms. For each category, we present the main advances with respect to the construction of surrogate models and adaptation of the acquisition functions. Finally, we discuss the open questions and suggest promising future research directions, in particular with regard to heterogeneity, privacy preservation, and fairness in distributed and federated optimization systems.},
  address    = {New York, NY, USA},
  articleno  = {287},
  doi        = {10.1145/3582078},
  groups     = {Bayesian optimization},
  issue_date = {December 2023},
  keywords   = {Gaussian process, acquisition function, Bayesian optimization},
  numpages   = {36},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi-org.ezproxy.lib.uh.edu/10.1145/3582078},
}

@Book{Oberkampf2010,
  author    = {Oberkampf, William L. and Roy, Christopher J.},
  publisher = {Cambridge University Press},
  title     = {Verification and validation in scientific computing},
  year      = {2010},
  address   = {Cambridge, UK},
  isbn      = {1139491768},
  groups    = {Books},
}

@Article{Sacks1989,
  author  = {Sacks, Jerome and Welch, William J. and Mitchell, Toby J. and Wynn, Henry P.},
  title   = {Design and Analysis of Computer Experiments},
  journal = {Statistical Science},
  year    = {1989},
  volume  = {4},
  number  = {4},
  pages   = {409--423},
  month   = nov,
  doi     = {10.1214/ss/1177012413},
  groups  = {Earlyworks, Others},
  url     = {https://doi.org/10.1214/ss/1177012413},
}

@InCollection{Garland2020,
  author    = {Garland, Nicolas and Le Riche, Rodolphe and Richet, Yann and Durrande, Nicolas and Brevault, Loïc and Balesdent, Mathieu and Morio, Jérôme},
  booktitle = {Aerospace System Analysis and Optimization in Uncertainty},
  publisher = {Springer International Publishing},
  title     = {{Multi-fidelity for MDO using Gaussian processes}},
  year      = {2020},
  address   = {Cham},
  isbn      = {978-3-030-39126-3},
  pages     = {295--320},
  abstract  = {The challenges of handling uncertainties within an MDO process have been discussed in Chapters 6and 7. Related concepts to multi-fidelity are introduced in this chapter. Indeed, high-fidelity models are used to represent the behavior of a system with an acceptable accuracy. However, these models are computationally intensive and they cannot be repeatedly evaluated, as required in MDO. Low-fidelity models are more suited to the early design phases as they are cheaper to evaluate. But they are often less accurate because of simplifications such as linearization, restrictive physical assumptions, dimensionality reduction, etc. Multi-fidelity models aim at combining models of different fidelities to achieve the desired accuracy at a lower computational cost. In Section 8.2, the connection between MDO, multi-fidelity, and cokriging is made through a review of past works and system representations of code architectures.},
  doi       = {10.1007/978-3-030-39126-3_8},
  groups    = {Additive},
  refid     = {Garland2020},
  url       = {https://doi.org/10.1007/978-3-030-39126-3_8},
}

@InProceedings{Damianou2013,
  author    = {Damianou, Andreas and Lawrence, Neil D.},
  booktitle = {Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics},
  title     = {{Deep Gaussian processes}},
  year      = {2013},
  month     = apr,
  pages     = {207--215},
  publisher = {PMLR},
  volume    = {31},
  abstract  = {In this paper we introduce deep Gaussian process (GP) models. Deep GPs are a deep belief network based on Gaussian process mappings. The data is modeled as the output of a multivariate GP. The inputs to that Gaussian process are then governed by another GP. A single layer model is equivalent to a standard GP or the GP latent variable model (GP-LVM). We perform inference in the model by approximate variational marginalization. This results in a strict lower bound on the marginal likelihood of the model which we use for model selection (number of layers and nodes per layer). Deep belief networks are typically applied to relatively large data sets using stochastic gradient descent for optimization. Our fully Bayesian treatment allows for the application of deep models even when data is scarce. Model selection by our variational bound shows that a five layer hierarchy is justified even when modelling a digit data set containing only 150 examples.},
  groups    = {Others},
  refid     = {pmlr-v31-damianou13a},
  url       = {https://proceedings.mlr.press/v31/damianou13a.html},
}

@InProceedings{Salimbeni2017,
  author    = {Salimbeni, Hugh and Deisenroth, Marc},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {{Doubly stochastic variational inference for deep Gaussian processes}},
  year      = {2017},
  pages     = {4588--4599},
  publisher = {NIPS 2017},
  volume    = {30},
  groups    = {Others},
  url       = {https://proceedings.neurips.cc/paper/2017/hash/8208974663db80265e9bfe7b222dcb18-Abstract.html},
}

@Article{Hebbal2021oe,
  author   = {Hebbal, Ali and Brevault, Loïc and Balesdent, Mathieu and Talbi, El-Ghazali and Melab, Nouredine},
  journal  = {Optimization and Engineering},
  title    = {{Bayesian optimization using deep Gaussian processes with applications to aerospace system design}},
  year     = {2021},
  issn     = {1573-2924},
  number   = {1},
  pages    = {321--361},
  volume   = {22},
  abstract = {Bayesian Optimization using Gaussian Processes is a popular approach to deal with optimization involving expensive black-box functions. However, because of the assumption on the stationarity of the covariance function defined in classic Gaussian Processes, this method may not be adapted for non-stationary functions involved in the optimization problem. To overcome this issue, Deep Gaussian Processes can be used as surrogate models instead of classic Gaussian Processes. This modeling technique increases the power of representation to capture the non-stationarity by considering a functional composition of stationary Gaussian Processes, providing a multiple layer structure. This paper investigates the application of Deep Gaussian Processes within Bayesian Optimization context. The specificities of this optimization method are discussed and highlighted with academic test cases. The performance of Bayesian Optimization with Deep Gaussian Processes is assessed on analytical test cases and aerospace design optimization problems and compared to the state-of-the-art stationary and non-stationary Bayesian Optimization approaches.},
  doi      = {10.1007/s11081-020-09517-8},
  groups   = {BO applications},
  refid    = {Hebbal2021},
  url      = {https://doi.org/10.1007/s11081-020-09517-8},
}

@Article{ZhangY2020,
  author    = {Zhang, Yichi and Tao, Siyu and Chen, Wei and Apley, Daniel W.},
  journal   = {Technometrics},
  title     = {A Latent Variable Approach to {G}aussian Process Modeling with Qualitative and Quantitative Factors},
  year      = {2020},
  issn      = {0040-1706},
  month     = jul,
  number    = {3},
  pages     = {291--302},
  volume    = {62},
  comment   = {doi: 10.1080/00401706.2019.1638834},
  doi       = {10.1080/00401706.2019.1638834},
  groups    = {Latent map},
  publisher = {Taylor \& Francis},
  url       = {https://doi.org/10.1080/00401706.2019.1638834},
}

@InProceedings{Poloczek2017,
  author       = {Poloczek, Matthias and Wang, Jialei and Frazier, Peter I.},
  booktitle    = {Advances in Neural Information Processing Systems},
  title        = {Multi-Information Source Optimization},
  year         = {2017},
  pages        = {4288--4298},
  publisher    = {NIPS 2017},
  volume       = {30},
  comment-ruda = {Categorical variable for models, HFM and discrepancies as independent GPs; acquisition function, maximize improvement in best expected objective value per cost.},
  file         = {:Poloczek2017.pdf:PDF},
  groups       = {Others},
  url          = {https://proceedings.neurips.cc/paper_files/paper/2017/file/df1f1d20ee86704251795841e6a9405a-Paper.pdf},
}

@Article{Xiao2018,
  author   = {Xiao, Manyu and Zhang, Guohua and Breitkopf, Piotr and Villon, Pierre and Zhang, Weihong},
  title    = {Extended Co-{K}riging interpolation method based on multi-fidelity data},
  journal  = {Applied Mathematics and Computation},
  year     = {2018},
  volume   = {323},
  pages    = {120--131},
  issn     = {0096-3003},
  abstract = {The common issue of surrogate models is to make good use of sampling data. In theory, the higher the fidelity of sampling data provided, the more accurate the approximation model built. However, in practical engineering problems, high-fidelity data may be less available, and such data may also be computationally expensive. On the contrary, we often obtain low-fidelity data under certain simplifications. Although low-fidelity data is less accurate, such data still contains much information about the real system. So, combining both high and low multi-fidelity data in the construction of a surrogate model may lead to better representation of the physical phenomena. Co-Kriging is a method based on a two-level multi-fidelity data. In this work, a Co-Kriging method which expands the usual two-level to multi-level multi-fidelity is proposed to improve the approximation accuracy. In order to generate the different fidelity data, the POD model reduction is used with varying number of the basis vectors. Three numerical examples are tested to illustrate not only the feasibility and effectiveness of the proposed method but also the better accuracy when compared with Kriging and classical Co-Kriging.},
  doi      = {10.1016/j.amc.2017.10.055},
  groups   = {Additive},
  keywords = {Multi-level multi-fidelity, Co-Kriging, Kriging, Surrogate model, POD},
  url      = {https://www.sciencedirect.com/science/article/pii/S0096300317307646},
}

@Article{Pelamatti2021,
  author    = {Pelamatti, Julien and Brevault, Loïc and Balesdent, Mathieu and Talbi, El-Ghazali and Guerin, Yannick},
  title     = {Mixed Variable {G}aussian Process-Based Surrogate Modeling Techniques: Application to Aerospace Design},
  journal   = {Journal of Aerospace Information Systems},
  year      = {2021},
  volume    = {18},
  number    = {11},
  pages     = {813--837},
  month     = aug,
  issn      = {1940-3151},
  abstract  = {Within the framework of complex system analyses, such as aircraft and launch vehicles, the presence of computationally intensive models (e.g., finite element models and multidisciplinary analyses) coupled with the dependence on discrete and unordered technological design choices results in challenging modeling problems. In this paper, the use of Gaussian process surrogate modeling of mixed continuous/discrete functions and the associated challenges are extensively discussed. A unifying formalism is proposed in order to facilitate the description and comparison between the existing covariance kernels allowing to adapt Gaussian processes to the presence of discrete unordered variables. Furthermore, the modeling performances of these various kernels are tested and compared on a set of analytical and aerospace-engineering-design-related benchmarks with different characteristics and parameterizations. Eventually, general tendencies and recommendations for such types of modeling problem using Gaussian process are highlighted.},
  comment   = {doi: 10.2514/1.I010965},
  doi       = {10.2514/1.I010965},
  groups    = {Latent map},
  publisher = {American Institute of Aeronautics and Astronautics},
  url       = {https://doi.org/10.2514/1.I010965},
}

@Article{WangL2021,
  author   = {Wang, Liwei and Tao, Siyu and Zhu, Ping and Chen, Wei},
  journal  = {Journal of Mechanical Design},
  title    = {Data-Driven Topology Optimization With Multiclass Microstructures Using Latent Variable {G}aussian Process},
  year     = {2021},
  issn     = {1050-0472},
  month    = nov,
  number   = {3},
  volume   = {143},
  abstract = {The data-driven approach is emerging as a promising method for the topological design of multiscale structures with greater efficiency. However, existing data-driven methods mostly focus on a single class of microstructures without considering multiple classes to accommodate spatially varying desired properties. The key challenge is the lack of an inherent ordering or “distance” measure between different classes of microstructures in meeting a range of properties. To overcome this hurdle, we extend the newly developed latent-variable Gaussian process (LVGP) models to create multi-response LVGP (MR-LVGP) models for the microstructure libraries of metamaterials, taking both qualitative microstructure concepts and quantitative microstructure design variables as mixed-variable inputs. The MR-LVGP model embeds the mixed variables into a continuous design space based on their collective effects on the responses, providing substantial insights into the interplay between different geometrical classes and material parameters of microstructures. With this model, we can easily obtain a continuous and differentiable transition between different microstructure concepts that can render gradient information for multiscale topology optimization. We demonstrate its benefits through multiscale topology optimization with aperiodic microstructures. Design examples reveal that considering multiclass microstructures can lead to improved performance due to the consistent load-transfer paths for micro- and macro-structures.},
  doi      = {10.1115/1.4048628},
  groups   = {Latent map},
  url      = {https://doi.org/10.1115/1.4048628},
}

@Article{Kushner1964,
  author   = {Kushner, Harold J.},
  title    = {{A new method of locating the maximum point of an arbitrary multipeak curve in the presence of noise}},
  journal  = {Journal Basic Engineering},
  year     = {1964},
  volume   = {86},
  number   = {1},
  pages    = {97--106},
  month    = mar,
  issn     = {0021-9223},
  abstract = {A versatile and practical method of searching a parameter space is presented. Theoretical and experimental results illustrate the usefulness of the method for such problems as the experimental optimization of the performance of a system with a very general multipeak performance function when the only available information is noise-distributed samples of the function. At present, its usefulness is restricted to optimization with respect to one system parameter. The observations are taken sequentially; but, as opposed to the gradient method, the observation may be located anywhere on the parameter interval. A sequence of estimates of the location of the curve maximum is generated. The location of the next observation may be interpreted as the location of the most likely competitor (with the current best estimate) for the location of the curve maximum. A Brownian motion stochastic process is selected as a model for the unknown function, and the observations are interpreted with respect to the model. The model gives the results a simple intuitive interpretation and allows the use of simple but efficient sampling procedures. The resulting process possesses some powerful convergence properties in the presence of noise; it is nonparametric and, despite its generality, is efficient in the use of observations. The approach seems quite promising as a solution to many of the problems of experimental system optimization.},
  doi      = {10.1115/1.3653121},
  groups   = {Acquisition functions},
  url      = {https://doi.org/10.1115/1.3653121},
}

@InProceedings{Agrawal2012,
  author    = {Agrawal, Shipra and Goyal, Navin},
  booktitle = {25th Annual Conference on Learning Theory},
  title     = {Analysis of {T}hompson sampling for the multi-armed bandit problem},
  year      = {2012},
  pages     = {39.1--39.26},
  publisher = {JMLR Workshop and Conference Proceedings},
  volume    = {23},
  groups    = {Acquisition functions},
  url       = {https://proceedings.mlr.press/v23/agrawal12.html},
}

@Article{Frazier2008,
  author    = {Frazier, Peter I. and Powell, Warren B. and Dayanik, Savas},
  title     = {A Knowledge-Gradient Policy for Sequential Information Collection},
  journal   = {SIAM Journal on Control and Optimization},
  year      = {2008},
  volume    = {47},
  number    = {5},
  pages     = {2410--2439},
  month     = jan,
  issn      = {0363-0129},
  abstract  = {In a sequential Bayesian ranking and selection problem with independent normal populations and common known variance, we study a previously introduced measurement policy which we refer to as the knowledge-gradient policy. This policy myopically maximizes the expected increment in the value of information in each time period, where the value is measured according to the terminal utility function. We show that the knowledge-gradient policy is optimal both when the horizon is a single time period and in the limit as the horizon extends to infinity. We show furthermore that, in some special cases, the knowledge-gradient policy is optimal regardless of the length of any given fixed total sampling horizon. We bound the knowledge-gradient policy's suboptimality in the remaining cases, and show through simulations that it performs competitively with or significantly better than other policies.},
  comment   = {doi: 10.1137/070693424},
  doi       = {10.1137/070693424},
  groups    = {Acquisition functions},
  publisher = {Society for Industrial and Applied Mathematics},
  url       = {https://doi.org/10.1137/070693424},
}

@InProceedings{HernandezLobato2014,
  author    = {Hernández-Lobato, José Miguel and Hoffman, Matthew W. and Ghahramani, Zoubin},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Predictive entropy search for efficient global optimization of black-box functions},
  year      = {2014},
  pages     = {918--926},
  publisher = {NIPS 2014},
  volume    = {27},
  groups    = {Acquisition functions},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2014/hash/069d3bb002acd8d7dd095917f9efe4cb-Abstract.html},
}

@Article{Hennig2012,
  author  = {Hennig, Philipp and Schuler, Christian J.},
  journal = {Journal of Machine Learning Research},
  title   = {Entropy Search for Information-Efficient Global Optimization.},
  year    = {2012},
  issn    = {1532-4435},
  number  = {6},
  pages   = {1809--1837},
  volume  = {13},
  groups  = {Acquisition functions},
  url     = {https://www.jmlr.org/papers/v13/hennig12a.html},
}

@Article{Saves2023,
  author   = {Saves, P. and Diouane, Y. and Bartoli, N. and Lefebvre, T. and Morlier, J.},
  title    = {A mixed-categorical correlation kernel for {G}aussian process},
  journal  = {Neurocomputing},
  year     = {2023},
  volume   = {550},
  pages    = {126472},
  issn     = {0925-2312},
  abstract = {Recently, there has been a growing interest for mixed-categorical meta-models based on Gaussian process (GP) surrogates. In this setting, several existing approaches use different strategies either by using continuous kernels (e.g., continuous relaxation and Gower distance based GP) or by using a direct estimation of the correlation matrix. In this paper, we present a kernel-based approach that extends continuous exponential kernels to handle mixed-categorical variables. The proposed kernel leads to a new GP surrogate that generalizes both the continuous relaxation and the Gower distance based GP models. We demonstrate, on both analytical and engineering problems, that our proposed GP model gives a higher likelihood and a smaller residual error than the other kernel-based state-of-the-art models. Our method is available in the open-source software SMT.},
  doi      = {10.1016/j.neucom.2023.126472},
  groups   = {Latent map},
  keywords = {Gaussian process, Mixed-categorical, Continuous relaxation, Hypersphere decomposition, Bayesian optimization, Surrogate modeling toolbox},
  url      = {https://www.sciencedirect.com/science/article/pii/S0925231223005957},
}

@Article{Deng2017,
  author    = {Deng, X. and Lin, C. Devon and Liu, K.-W. and Rowe, R. K.},
  title     = {Additive Gaussian Process for Computer Models With Qualitative and Quantitative Factors},
  journal   = {Technometrics},
  year      = {2017},
  volume    = {59},
  number    = {3},
  pages     = {283--292},
  month     = jul,
  issn      = {0040-1706},
  comment   = {doi: 10.1080/00401706.2016.1211554},
  doi       = {10.1080/00401706.2016.1211554},
  groups    = {Latent map},
  publisher = {Taylor \& Francis},
  url       = {https://doi.org/10.1080/00401706.2016.1211554},
}

@Article{GarridoMerchan2020,
  author   = {Garrido-Merchán, Eduardo C. and Hernández-Lobato, Daniel},
  title    = {Dealing with categorical and integer-valued variables in {B}ayesian Optimization with {G}aussian processes},
  journal  = {Neurocomputing},
  year     = {2020},
  volume   = {380},
  pages    = {20--35},
  issn     = {0925-2312},
  abstract = {Some optimization problems are characterized by an objective that is very expensive, that lacks an analytical expression, and whose evaluations can be contaminated by noise. Bayesian Optimization (BO) methods can be used to solve these problems efficiently. BO relies on a probabilistic model of the objective, which is typically a Gaussian process (GP). This model is used to compute an acquisition function that estimates the expected utility (for solving the optimization problem) of evaluating the objective at each potential new point. A problem with GPs is, however, that they assume real-valued input variables and cannot easily deal with categorical or integer-valued values. Common methods to account for these variables, before evaluating the objective, include assuming they are real and then using a one-hot encoding, for categorical variables, or rounding to the closest integer, for integer-valued variables. We show that this leads to suboptimal results and introduce a novel approach to tackle categorical or integer-valued input variables within the context of BO with GPs. Several synthetic and real-world experiments support our hypotheses and show that our approach outperforms the results of standard BO using GPs on problems with categorical or integer-valued input variables.},
  doi      = {10.1016/j.neucom.2019.11.004},
  groups   = {Latent map, Discrete BO},
  keywords = {Parameter tuning, Bayesian optimization, Gaussian processes, Integer-valued variables, Categorical variables},
  url      = {https://www.sciencedirect.com/science/article/pii/S0925231219315619},
}

@Article{Sobester2005,
  author   = {András Sóbester and Leary, Stephen J. and Keane, Andy J.},
  title    = {On the Design of Optimization Strategies Based on Global Response Surface Approximation Models},
  journal  = {Journal of Global Optimization},
  year     = {2005},
  volume   = {33},
  number   = {1},
  pages    = {31--59},
  issn     = {1573-2916},
  abstract = {Striking the correct balance between global exploration of search spaces and local exploitation of promising basins of attraction is one of the principal concerns in the design of global optimization algorithms. This is true in the case of techniques based on global response surface approximation models as well. After constructing such a model using some initial database of designs it is far from obvious how to select further points to examine so that the appropriate mix of exploration and exploitation is achieved. In this paper we propose a selection criterion based on the expected improvement measure, which allows relatively precise control of the scope of the search. We investigate its behavior through a set of artificial test functions and two structural optimization problems. We also look at another aspect of setting up search heuristics of this type: the choice of the size of the database that the initial approximation is built upon.},
  doi      = {10.1007/s10898-004-6733-1},
  groups   = {Acquisition functions},
  refid    = {Sóbester2005},
  url      = {https://doi.org/10.1007/s10898-004-6733-1},
}

@Article{Ath2021,
  author     = {De Ath, George and Everson, Richard M. and Rahat, Alma A. M. and Fieldsend, Jonathan E.},
  title      = {Greed Is Good: Exploration and Exploitation Trade-Offs in {B}ayesian Optimisation},
  journal    = {ACM Transactions on Evolutionary Learning and Optimization},
  year       = {2021},
  volume     = {1},
  number     = {1},
  month      = {apr},
  issn       = {2688-299X},
  abstract   = {The performance of acquisition functions for Bayesian optimisation to locate the global optimum of continuous functions is investigated in terms of the Pareto front between exploration and exploitation. We show that Expected Improvement (EI) and the Upper Confidence Bound (UCB) always select solutions to be expensively evaluated on the Pareto front, but Probability of Improvement is not guaranteed to do so and Weighted Expected Improvement does so only for a restricted range of weights. We introduce two novel -greedy acquisition functions. Extensive empirical evaluation of these together with random search, purely exploratory, and purely exploitative search on 10 benchmark problems in 1 to 10 dimensions shows that -greedy algorithms are generally at least as effective as conventional acquisition functions (e.g., EI and UCB), particularly with a limited budget. In higher dimensions, -greedy approaches are shown to have improved performance over conventional approaches. These results are borne out on a real-world computational fluid dynamics optimisation problem and a robotics active learning problem. Our analysis and experiments suggest that the most effective strategy, particularly in higher dimensions, is to be mostly greedy, occasionally selecting a random exploratory solution.},
  address    = {New York, NY, USA},
  articleno  = {1},
  doi        = {10.1145/3425501},
  groups     = {Acquisition functions},
  issue_date = {June 2021},
  keywords   = {Bayesian optimisation, ε-greedy, infill criteria, exploration-exploitation trade-off, acquisition function},
  numpages   = {22},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi-org.ezproxy.lib.uh.edu/10.1145/3425501},
}

@InProceedings{Contal2014,
  author    = {Contal, Emile and Perchet, Vianney and Vayatis, Nicolas},
  booktitle = {Proceedings of the 31st International Conference on International Conference on Machine Learning - Volume 32},
  title     = {Gaussian Process Optimization with Mutual Information},
  year      = {2014},
  address   = {Beijing, China},
  pages     = {253--261},
  publisher = {ICML 14},
  abstract  = {In this paper, we analyze a generic algorithm scheme for sequential global optimization using Gaussian processes. The upper bounds we derive on the cumulative regret for this generic algorithm improve by an exponential factor the previously known bounds for algorithms like GP-UCB. We also introduce the novel Gaussian Process Mutual Information algorithm (GP-MI), which significantly improves further these upper bounds for the cumulative regret. We confirm the efficiency of this algorithm on synthetic and real tasks against the natural competitor, GP-UCB, and also the Expected Improvement heuristic.},
  groups    = {Acquisition functions},
  url       = {https://proceedings.mlr.press/v32/contal14.html},
}

@Article{Lai1985,
  author  = {Lai, T. L. and Robbins, Herbert},
  title   = {Asymptotically efficient adaptive allocation rules},
  journal = {Advances in Applied Mathematics},
  year    = {1985},
  volume  = {6},
  number  = {1},
  pages   = {4--22},
  issn    = {0196-8858},
  doi     = {10.1016/0196-8858(85)90002-8},
  groups  = {Acquisition functions},
  url     = {https://www.sciencedirect.com/science/article/pii/0196885885900028},
}

@Article{Auer2002,
  author  = {Auer, Peter},
  journal = {Journal of Machine Learning Research},
  title   = {Using confidence bounds for exploitation-exploration trade-offs},
  year    = {2002},
  number  = {Nov},
  pages   = {397--422},
  volume  = {3},
  groups  = {Acquisition functions},
  url     = {https://www.jmlr.org/papers/v3/auer02a.html},
}

@Article{Russo2018,
  author     = {Russo, Daniel J. and Van Roy, Benjamin and Kazerouni, Abbas and Osband, Ian and Wen, Zheng},
  title      = {A Tutorial on {T}hompson Sampling},
  journal    = {Foundations and Trends in Machine Learning},
  year       = {2018},
  volume     = {11},
  number     = {1},
  pages      = {1–96},
  month      = {jul},
  issn       = {1935-8237},
  abstract   = {Thompson sampling is an algorithm for online decision problems where actions are taken sequentially in a manner that must balance between exploiting what is known to maximize immediate performance and investing to accumulate new information that may improve future performance. The algorithm addresses a broad range of problems in a computationally efficient manner and is therefore enjoying wide use. This tutorial covers the algorithm and its application, illustrating concepts through a range of examples, including Bernoulli bandit problems, shortest path problems, product recommendation, assortment, active learning with neural networks, and reinforcement learning in Markov decision processes. Most of these problems involve complex information structures, where information revealed by taking an action informs beliefs about other actions. We will also discuss when and why Thompson sampling is or is not effective and relations to alternative algorithms.},
  address    = {Hanover, MA, USA},
  doi        = {10.1561/2200000070},
  groups     = {Acquisition functions},
  issue_date = {Jul 2018},
  keywords   = {Exploration, Bandit learning},
  numpages   = {99},
  publisher  = {Now Publishers Inc.},
  url        = {https://doi.org/10.1561/2200000070},
}

@Article{Bijl2016,
  author  = {Bijl, Hildo and Schön, Thomas B. and van Wingerden, Jan-Willem and Verhaegen, Michel},
  title   = {{A sequential Monte Carlo approach to Thompson sampling for Bayesian optimization}},
  journal = {arXiv preprint arXiv:1604.00169},
  year    = {2016},
  doi     = {10.48550/arXiv.1604.00169},
  groups  = {Acquisition functions},
}

@InProceedings{Kandasamy2018,
  author    = {Kandasamy, Kirthevasan and Krishnamurthy, Akshay and Schneider, Jeff and Poczos, Barnabas},
  booktitle = {Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics},
  title     = {Parallelised {B}ayesian Optimisation via {T}hompson Sampling},
  year      = {2018},
  month     = {09--11 Apr},
  pages     = {133--142},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {84},
  abstract  = {We design and analyse variations of the classical Thompson sampling (TS) procedure for Bayesian optimisation (BO) in settings where function evaluations are expensive but can be performed in parallel. Our theoretical analysis shows that a direct application of the sequential Thompson sampling algorithm in either synchronous or asynchronous parallel settings yields a surprisingly powerful result: making $n$ evaluations distributed among $M$ workers is essentially equivalent to performing $n$ evaluations in sequence. Further, by modelling the time taken to complete a function evaluation, we show that, under a time constraint, asynchronous parallel TS achieves asymptotically lower regret than both the synchronous and sequential versions. These results are complemented by an experimental analysis, showing that asynchronous TS outperforms a suite of existing parallel BO algorithms in simulations and in an application involving tuning hyper-parameters of a convolutional neural network. In addition to these, the proposed procedure is conceptually much simpler than existing work for parallel BO.},
  groups    = {Acquisition functions},
  pdf       = {http://proceedings.mlr.press/v84/kandasamy18a/kandasamy18a.pdf},
  url       = {https://proceedings.mlr.press/v84/kandasamy18a.html},
}

@Article{Russo2014,
  author    = {Russo, Daniel J. and Van Roy, Benjamin},
  title     = {Learning to Optimize via Posterior Sampling},
  journal   = {Mathematics of Operations Research},
  year      = {2014},
  volume    = {39},
  number    = {4},
  pages     = {1221--1243},
  month     = apr,
  issn      = {0364-765X},
  abstract  = {This paper considers the use of a simple posterior sampling algorithm to balance between exploration and exploitation when learning to optimize actions such as in multiarmed bandit problems. The algorithm, also known as Thompson Sampling and as probability matching, offers significant advantages over the popular upper confidence bound (UCB) approach, and can be applied to problems with finite or infinite action spaces and complicated relationships among action rewards. We make two theoretical contributions. The first establishes a connection between posterior sampling and UCB algorithms. This result lets us convert regret bounds developed for UCB algorithms into Bayesian regret bounds for posterior sampling. Our second theoretical contribution is a Bayesian regret bound for posterior sampling that applies broadly and can be specialized to many model classes. This bound depends on a new notion we refer to as the eluder dimension, which measures the degree of dependence among action rewards. Compared to UCB algorithm Bayesian regret bounds for specific model classes, our general bound matches the best available for linear models and is stronger than the best available for generalized linear models. Further, our analysis provides insight into performance advantages of posterior sampling, which are highlighted through simulation results that demonstrate performance surpassing recently proposed UCB algorithms.},
  comment   = {doi: 10.1287/moor.2014.0650},
  doi       = {10.1287/moor.2014.0650},
  groups    = {Acquisition functions},
  publisher = {INFORMS},
  url       = {https://doi.org/10.1287/moor.2014.0650},
}

@Article{Roustant2020,
  author    = {Roustant, Olivier and Padonou, Espéran and Deville, Yves and Clément, Aloïs and Perrin, Guillaume and Giorla, Jean and Wynn, Henry},
  title     = {Group Kernels for {G}aussian Process Metamodels with Categorical Inputs},
  journal   = {SIAM/ASA Journal on Uncertainty Quantification},
  year      = {2020},
  volume    = {8},
  number    = {2},
  pages     = {775--806},
  month     = jan,
  abstract  = {Gaussian processes (GPs) are widely used as a metamodel for emulating time-consuming computer codes. We focus on problems involving categorical inputs, with a potentially large number $L$ of levels (typically several tens), partitioned in $G \ll L$ groups of various sizes. Parsimonious covariance functions, or kernels, can then be defined by block covariance matrices $T$ with constant covariances between pairs of blocks and within blocks. We study the positive definiteness of such matrices to encourage their practical use. The hierarchical group/level structure, equivalent to a nested Bayesian linear model, provides a parameterization of valid block matrices $T$. The same model can then be used when the assumption within blocks is relaxed, giving a flexible parametric family of valid covariance matrices with constant covariances between pairs of blocks. The positive definiteness of ${T}$ is equivalent to the positive definiteness of a smaller matrix of size $G$, obtained by averaging each block. The model is applied to a problem in nuclear waste analysis, where one of the categorical inputs is atomic number, which has more than 90 levels.},
  comment   = {doi: 10.1137/18M1209386},
  doi       = {10.1137/18M1209386},
  groups    = {Latent map},
  publisher = {Society for Industrial and Applied Mathematics},
  url       = {https://doi.org/10.1137/18M1209386},
}

@Article{Villemonteix2009,
  author   = {Villemonteix, Julien and Vazquez, Emmanuel and Walter, Eric},
  title    = {An informational approach to the global optimization of expensive-to-evaluate functions},
  journal  = {Journal of Global Optimization},
  year     = {2009},
  volume   = {44},
  number   = {4},
  pages    = {509--534},
  issn     = {1573-2916},
  abstract = {In many global optimization problems motivated by engineering applications, the number of function evaluations is severely limited by time or cost. To ensure that each evaluation contributes to the localization of good candidates for the role of global minimizer, a sequential choice of evaluation points is usually carried out. In particular, when Kriging is used to interpolate past evaluations, the uncertainty associated with the lack of information on the function can be expressed and used to compute a number of criteria accounting for the interest of an additional evaluation at any given point. This paper introduces minimizers entropy as a new Kriging-based criterion for the sequential choice of points at which the function should be evaluated. Based on stepwise uncertainty reduction, it accounts for the informational gain on the minimizer expected from a new evaluation. The criterion is approximated using conditional simulations of the Gaussian process model behind Kriging, and then inserted into an algorithm similar in spirit to the Efficient Global Optimization (EGO) algorithm. An empirical comparison is carried out between our criterion and expected improvement, one of the reference criteria in the literature. Experimental results indicate major evaluation savings over EGO. Finally, the method, which we call IAGO (for Informational Approach to Global Optimization), is extended to robust optimization problems, where both the factors to be tuned and the function evaluations are corrupted by noise.},
  doi      = {10.1007/s10898-008-9354-2},
  groups   = {Latent map},
  refid    = {Villemonteix2009},
  url      = {https://doi.org/10.1007/s10898-008-9354-2},
}

@InProceedings{WangZ2017,
  author    = {Wang, Zi and Jegelka, Stefanie},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  title     = {Max-value entropy search for efficient {B}ayesian optimization},
  year      = {2017},
  pages     = {3627--3635},
  publisher = {PMLR},
  groups    = {Acquisition functions},
  issn      = {2640-3498},
  journal   = {International Conference on Machine Learning},
  url       = {https://proceedings.mlr.press/v70/wang17e.html},
}

@InProceedings{Houlsby2012,
  author    = {Houlsby, Neil and Huszar, Ferenc and Ghahramani, Zoubin and Hern\'{a}ndez-lobato, Jose},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Collaborative {G}aussian Processes for Preference Learning},
  year      = {2012},
  pages     = {2096--2104},
  publisher = {NIPS 2012},
  volume    = {25},
  groups    = {Acquisition functions},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2012/file/afdec7005cc9f14302cd0474fd0f3c96-Paper.pdf},
}

@InProceedings{Lam2016,
  author    = {Lam, Remi and Willcox, Karen and Wolpert, David H.},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Bayesian Optimization with a Finite Budget: An Approximate Dynamic Programming Approach},
  year      = {2016},
  pages     = {883--891},
  publisher = {NIPS 2016},
  volume    = {29},
  groups    = {Acquisition functions},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2016/file/5ea1649a31336092c05438df996a3e59-Paper.pdf},
}

@InProceedings{Gonzalez2016,
  author    = {González, Javier and Osborne, Michael and Lawrence, Neil},
  booktitle = {Proceedings of the 19th International Conference on Artificial Intelligence and Statistics},
  title     = {{GLASSES: Relieving the myopia of Bayesian optimisation}},
  year      = {2016},
  pages     = {790--799},
  publisher = {PMLR},
  groups    = {Acquisition functions},
  url       = {https://proceedings.mlr.press/v51/gonzalez16b.html},
}

@Book{Hennig2022,
  author    = {Hennig, Philipp and Osborne, Michael A. and Kersting, Hans P.},
  publisher = {Cambridge University Press},
  title     = {{Probabilistic numerics: Computation as machine learning}},
  year      = {2022},
  isbn      = {1316730336},
  groups    = {Acquisition functions},
}

@InProceedings{Ginsbourger2010,
  author    = {Ginsbourger, David and Le Riche, Rodolphe},
  booktitle = {{mODa 9 - Advances in Model-Oriented Design and Analysis}},
  title     = {Towards {G}aussian Process-based Optimization with Finite Time Horizon},
  year      = {2010},
  address   = {Heidelberg},
  editor    = {Giovagnoli, Alessandra and Atkinson, Anthony C. and Torsney, Bernard and May, Caterina},
  pages     = {89--96},
  publisher = {Physica-Verlag HD},
  abstract  = {During the last decade, Kriging-based sequential optimization algorithms have become standard methods in computer experiments. These algorithms rely on the iterative maximization of sampling criteria such as the Expected Improvement (EI), which takes advantage of Kriging conditional distributions to make an explicit trade-off between promising and uncertain points in the search space. We have recently worked on a multipoint EI criterion meant to choose simultaneously several points for synchronous parallel computation. The results presented in this article concern sequential procedures with a fixed number of iterations. We show that maximizing the usual EI at each iteration is suboptimal. In essence, the latter amounts to considering the current iteration as the last one. This work formulates the problem of optimal strategy for finite horizon sequential optimization, provides the solution to this problem in terms of a new multipoint EI, and illustrates the suboptimality of maximizing the 1-point EI at each iteration on the basis of a first counter-example.},
  groups    = {Acquisition functions},
  isbn      = {978-3-7908-2410-0},
  refid     = {10.1007/978-3-7908-2410-0_12},
}

@Article{Streltsov1999,
  author   = {Streltsov, Simon and Vakili, Pirooz},
  title    = {A Non-myopic Utility Function for Statistical Global Optimization Algorithms},
  journal  = {Journal of Global Optimization},
  year     = {1999},
  volume   = {14},
  number   = {3},
  pages    = {283--298},
  issn     = {1573-2916},
  abstract = {The high cost of providing “worst-case” solutions to global optimization problems has motivated the development of ”average-case“ algorithms that rely on a statistical model of the objective function. The critical role of the statistical model is to guide the search for the optimum. The standard approach is to define a utility function u(x) that in a certain sense reflects the benefit of evaluating the function at x. A proper utility function needs to strike a balance between the immediate benefit of evaluating the function at x - a myopic consideration; and the overall effect of this choice on the performance of the algorithm - a global criterion. The utility functions currently used in this context are heuristically modified versions of some myopic utility functions. We propose using a new utility function that is provably a globally optimal utility function in a non-adaptive context (where the model of the function values remains unchanged). In the adaptive context, this utility function is not necessarily optimal, however, given its global nature, we expect that its use will lead to the improved performance of statistical global optimization algorithms. To illustrate the approach, and to test the above assertion, we apply this utility function to an existing adaptive multi-dimensional statistical global optimization algorithm and provide experimental comparisons with the original algorithm.},
  doi      = {10.1023/A:1008284229931},
  groups   = {Acquisition functions},
  refid    = {Streltsov1999},
  url      = {https://doi.org/10.1023/A:1008284229931},
}

@InProceedings{Lee2020,
  author    = {Lee, Eric and Eriksson, David and Bindel, David and Cheng, Bolong and Mccourt, Mike},
  booktitle = {Proceedings of the 36th Conference on Uncertainty in Artificial Intelligence},
  title     = {Efficient rollout strategies for {B}ayesian optimization},
  year      = {2020},
  pages     = {260--269},
  publisher = {PMLR},
  groups    = {Acquisition functions},
  issn      = {2640-3498},
  url       = {https://proceedings.mlr.press/v124/lee20a.html},
}

@InProceedings{Paulson2022,
  author    = {Paulson, J. A. and Sorouifar, F. and Chakrabarty, A.},
  title     = {Efficient Multi-Step Lookahead {B}ayesian Optimization with Local Search Constraints},
  booktitle = {2022 IEEE 61st Conference on Decision and Control (CDC)},
  year      = {2022},
  pages     = {123--129},
  doi       = {10.1109/CDC51059.2022.9992943},
  groups    = {Acquisition functions},
  issn      = {2576-2370},
  journal   = {2022 IEEE 61st Conference on Decision and Control (CDC)},
}

@Article{Kandasamy2019,
  author  = {Kandasamy, Kirthevasan and Dasarathy, Gautam and Oliva, Junier and Schneider, Jeff and Poczos, Barnabas},
  title   = {{Multi-fidelity Gaussian process bandit optimisation}},
  journal = {Journal of Artificial Intelligence Research},
  year    = {2019},
  volume  = {66},
  pages   = {151--196},
  issn    = {1076-9757},
  doi     = {10.1613/jair.1.11288},
  groups  = {Acquisition functions},
}

@InProceedings{Shahriari2014,
  author    = {Shahriari, Bobak and Wang, Ziyu and Hoffman, Matthew W. and Bouchard-Côté, Alexandre and de Freitas, Nando},
  booktitle = {{NIPS Workshop on Bayesian Optimization}},
  title     = {An entropy search portfolio for {B}ayesian optimization},
  year      = {2014},
  note      = {BayesOpt 2014.},
  eprint    = {https://arxiv.org/abs/1406.4625},
  groups    = {Acquisition functions},
  url       = {https://bayesopt.github.io/papers/2014/paper15.pdf},
}

@InProceedings{Hoffman2014,
  author    = {Hoffman, Matthew and Shahriari, Bobak and Freitas, Nando},
  booktitle = {Proceedings of the 17th International Conference on Artificial Intelligence and Statistic},
  title     = {On correlation and budget constraints in model-based bandit optimization with application to automatic machine learning},
  year      = {2014},
  pages     = {365--374},
  publisher = {PMLR},
  groups    = {Acquisition functions},
  url       = {https://proceedings.mlr.press/v33/hoffman14.html},
}

@InProceedings{Gardner2014,
  author    = {Gardner, Jacob R. and Kusner, Matt J. and Xu, Zhixiang Eddie and Weinberger, Kilian Q. and Cunningham, John P.},
  booktitle = {Proceedings of the 31st International Conference on Machine Learning},
  title     = {Bayesian optimization with inequality constraints},
  year      = {2014},
  pages     = {937--945},
  publisher = {JMLR},
  volume    = {2014},
  groups    = {Constrained BO},
  url       = {https://proceedings.mlr.press/v32/gardner14.html},
}

@Article{Sobester2014,
  author   = {András Sóbester and Forrester, Alexander I. J. and Toal, David J. J. and Tresidder, Es and Tucker, Simon},
  title    = {Engineering design applications of surrogate-assisted optimization techniques},
  journal  = {Optimization and Engineering},
  year     = {2014},
  volume   = {15},
  number   = {1},
  pages    = {243--265},
  issn     = {1573-2924},
  abstract = {The construction of models aimed at learning the behaviour of a system whose responses to inputs are expensive to measure is a branch of statistical science that has been around for a very long time. Geostatistics has pioneered a drive over the last half century towards a better understanding of the accuracy of such ‘surrogate’ models of the expensive function. Of particular interest to us here are some of the even more recent advances related to exploiting such formulations in an optimization context. While the classic goal of the modelling process has been to achieve a uniform prediction accuracy across the domain, an economical optimization process may aim to bias the distribution of the learning budget towards promising basins of attraction. This can only happen, of course, at the expense of the global exploration of the space and thus finding the best balance may be viewed as an optimization problem in itself. We examine here a selection of the state-of-the-art solutions to this type of balancing exercise through the prism of several simple, illustrative problems, followed by two ‘real world’ applications: the design of a regional airliner wing and the multi-objective search for a low environmental impact house.},
  doi      = {10.1007/s11081-012-9199-x},
  groups   = {Constrained BO},
  refid    = {Sóbester2014},
  url      = {https://doi.org/10.1007/s11081-012-9199-x},
}

@InCollection{Gramacy2011,
  author    = {Gramacy, Robert B. and Lee, Herbert K. H.},
  booktitle = {Bayesian Statistics 9},
  publisher = {Oxford University Press},
  title     = {Optimization Under Unknown Constraints},
  year      = {2011},
  editor    = {Bernardo, José M. and Bayarri, M. J. and Berger, James O. and Dawid, A. P. and Heckerman, David and Smith, Adrian F. M. and West, Mike},
  isbn      = {9780199694587},
  month     = oct,
  pages     = {229--256},
  abstract  = {Optimization of complex functions, such as the output of computer simulators, is a difficult task that has received much attention in the literature. A less studied problem is that of optimization under unknown constraints, i.e., when the simulator must be invoked both to determine the typical real‐valued response and to determine if a constraint has been violated, either for physical or policy reasons. We develop a statistical approach based on Gaussian processes and Bayesian learning to both approximate the unknown function and estimate the probability of meeting the constraints. A new integrated improvement criterion is proposed to recognize that responses from inputs that violate the constraint may still be informative about the function, and thus could potentially be useful in the optimization. The new criterion is illustrated on synthetic data, and on a motivating optimization problem from health care policy.},
  doi       = {10.1093/acprof:oso/9780199694587.003.0008},
  groups    = {Constrained BO},
  url       = {https://doi.org/10.1093/acprof:oso/9780199694587.003.0008},
}

@InProceedings{Picheny2014,
  author    = {Picheny, Victor},
  booktitle = {Proceedings of the 17th International Conference on Artificial Intelligence and Statistics},
  title     = {A stepwise uncertainty reduction approach to constrained global optimization},
  year      = {2014},
  pages     = {787--795},
  publisher = {PMLR},
  groups    = {Constrained BO},
  journal   = {Artificial intelligence and statistics},
  url       = {https://proceedings.mlr.press/v33/picheny14.html},
}

@Article{Kontogiannis2020b,
  author   = {Kontogiannis, Spyridon G. and Savill, Mark A.},
  title    = {A generalized methodology for multidisciplinary design optimization using surrogate modelling and multifidelity analysis},
  journal  = {Optimization and Engineering},
  year     = {2020},
  volume   = {21},
  number   = {3},
  pages    = {723--759},
  issn     = {1573-2924},
  abstract = {The advantages of multidisciplinary design are well understood, but not yet fully adopted by the industry where methods should be both fast and reliable. For such problems, minimum computational cost while providing global optimality and extensive design information at an early conceptual stage is desired. However, such a complex problem consisting of various objectives and interacting disciplines is associated with a challenging design space. This provides a large pool of possible designs, requiring an efficient exploration scheme with the ability to provide sufficient feedback early in the design process. This paper demonstrates a generalized optimization framework with rapid design space exploration capabilities in which a Multifidelity approach is directly adjusted to the emerging needs of the design. The methodology is developed to be easily applicable and efficient in computationally expensive multidisciplinary problems. To accelerate such a demanding process, Surrogate Based Optimization methods in the form of both Radial Basis Function and Kriging models are employed. In particular, a modification of the standard Kriging approach to account for Multifidelity data inputs is proposed, aiming to increasing its accuracy without increasing its training cost. The surrogate optimization problem is solved by a Particle Swarm Optimization algorithm and two constraint handling methods are implemented. The surrogate model modifications are visually demonstrated in a 1D and 2D test case, while the Rosenbrock and Sellar functions are used to examine the scalability and adaptability behaviour of the method. Our particular Multiobjective formulation is demonstrated in the common RAE2822 airfoil design problem. In this paper, the framework assessment focuses on our infill sampling approach in terms of design and objective space exploration for a given computational cost.},
  doi      = {10.1007/s11081-020-09504-z},
  groups   = {Constrained BO},
  refid    = {Kontogiannis2020},
  url      = {https://doi.org/10.1007/s11081-020-09504-z},
}

@Article{Spagnol2019,
  author    = {Spagnol, Adrien and Riche, Rodolphe Le and Veiga, Sébastien Da},
  title     = {Global Sensitivity Analysis for Optimization with Variable Selection},
  journal   = {SIAM/ASA Journal on Uncertainty Quantification},
  year      = {2019},
  volume    = {7},
  number    = {2},
  pages     = {417--443},
  month     = jan,
  abstract  = {The optimization of high dimensional functions is a key issue in engineering problems but it frequently comes at a cost that is not acceptable since it usually involves a complex and expensive computer code. Engineers often overcome this limitation by first identifying which parameters drive the function variations the most: noninfluential variables are set to a fixed value and the optimization procedure is carried out with the remaining influential variables. Such variable selection is performed through influence measures that are meaningful for regression problems. However it does not account for the specific structure of optimization problems where we would like to identify which variables most lead to constraints satisfaction and low values of the objective function. In this paper, we propose a new sensitivity analysis that accounts for the specific aspects of optimization problems. In particular, we introduce an influence measure based on the Hilbert--Schmidt independence criterion to characterize whether a design variable matters to reach low values of the objective function and to satisfy the constraints. This sensitivity measure makes it possible to sort the inputs and reduce the problem dimension. We compare a random and a greedy strategies to set the values of the noninfluential variables before conducting a local optimization. Applications to several test cases show that this variable selection and the greedy strategy significantly reduce the number of function evaluations at a limited cost in terms of solution performance.},
  comment   = {doi: 10.1137/18M1167978},
  doi       = {10.1137/18M1167978},
  groups    = {High-dimension BO},
  publisher = {Society for Industrial and Applied Mathematics},
  url       = {https://doi.org/10.1137/18M1167978},
}

@InProceedings{Letham2020,
  author    = {Letham, Ben and Calandra, Roberto and Rai, Akshara and Bakshy, Eytan},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Re-examining linear embeddings for high-dimensional {B}ayesian optimization},
  year      = {2020},
  pages     = {1546--1558},
  publisher = {NeurIPS 2020},
  volume    = {33},
  groups    = {High-dimension BO},
  url       = {https://proceedings.neurips.cc/paper/2020/hash/10fb6cfa4c990d2bad5ddef4f70e8ba2-Abstract.html},
}

@InProceedings{Kandasamy2015,
  author    = {Kandasamy, Kirthevasan and Schneider, Jeff and P\'{o}czos, Barnab\'{a}s},
  booktitle = {Proceedings of the 32nd International Conference on International Conference on Machine Learning},
  title     = {High Dimensional {B}ayesian Optimisation and Bandits via Additive Models},
  year      = {2015},
  address   = {Lille, France},
  pages     = {295--304},
  publisher = {JMLR},
  volume    = {37},
  abstract  = {Bayesian Optimisation (BO) is a technique used in optimising a D-dimensional function which is typically expensive to evaluate. While there have been many successes for BO in low dimensions, scaling it to high dimensions has been notoriously difficult. Existing literature on the topic are under very restrictive settings. In this paper, we identify two key challenges in this endeavour. We tackle these challenges by assuming an additive structure for the function. This setting is substantially more expressive and contains a richer class of functions than previous work. We prove that, for additive functions the regret has only linear dependence on D even though the function depends on all D dimensions. We also demonstrate several other statistical and computational benefits in our framework. Via synthetic examples, a scientific simulation and a face detection problem we demonstrate that our method outperforms naive BO on additive functions and on several examples where the function is not additive.},
  groups    = {High-dimension BO},
  numpages  = {10},
  url       = {https://proceedings.mlr.press/v37/kandasamy15.html},
}

@InProceedings{Gardner2017,
  author    = {Gardner, Jacob and Guo, Chuan and Weinberger, Kilian and Garnett, Roman and Grosse, Roger},
  booktitle = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics},
  title     = {{Discovering and exploiting additive structure for Bayesian optimization}},
  year      = {2017},
  month     = {20--22 Apr},
  pages     = {1311--1319},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {54},
  abstract  = {Bayesian optimization has proven invaluable for black-box optimization of expensive functions. Its main limitation is its exponential complexity with respect to the dimensionality of the search space using typical kernels. Luckily, many objective functions can be decomposed into additive subproblems, which can be optimized independently. We investigate how to automatically discover such (typically unknown) additive structure while simultaneously exploiting it through Bayesian optimization. We propose an efficient algorithm based on Metropolis-Hastings sampling and demonstrate its efficacy empirically on synthetic and real-world data sets. Throughout all our experiments we reliably discover hidden additive structure whenever it exists and exploit it to yield significantly faster convergence.},
  groups    = {High-dimension BO},
  pdf       = {http://proceedings.mlr.press/v54/gardner17a/gardner17a.pdf},
  url       = {https://proceedings.mlr.press/v54/gardner17a.html},
}

@Article{Khatamsaz2021md,
  author   = {Khatamsaz, Danial and Molkeri, Abhilash and Couperthwaite, Richard and James, Jaylen and Arróyave, Raymundo and Srivastava, Ankit and Allaire, Douglas},
  journal  = {Materials {\&} Design},
  title    = {Adaptive active subspace-based efficient multifidelity materials design},
  year     = {2021},
  issn     = {0264-1275},
  pages    = {110001},
  volume   = {209},
  abstract = {Materials design calls for an optimal exploration and exploitation of the process-structure-property (PSP) relationships to produce materials with targeted properties. Recently, we developed and deployed a closed-loop multi-information source fusion (multi-fidelity) Bayesian Optimization (BO) framework to optimize the mechanical performance of a dual-phase material by adjusting the material composition and processing parameters. While promising, BO frameworks tend to underperform as the dimensionality of the problem increases. Herein, we employ an adaptive active subspace method to efficiently handle the large dimensionality of the design space of a typical PSP-based material design problem within our multi-fidelity BO framework. Our adaptive active subspace method significantly accelerates the design process by prioritizing searches in the important regions of the high-dimensional design space. A detailed discussion of the various components and demonstration of three approaches to implementing the adaptive active subspace method within the multi-fidelity BO framework is presented.},
  doi      = {10.1016/j.matdes.2021.110001},
  groups   = {BO applications, Acquisition functions},
  keywords = {Adaptive dimensionality reduction, Active subspace, Multifidelity design, Bayesian optimization, PSP relationships, Dual-phase materials},
  url      = {https://www.sciencedirect.com/science/article/pii/S0264127521005566},
}

@InProceedings{Rana2017,
  author    = {Santu Rana and Cheng Li and Sunil Gupta and Vu Nguyen and Svetha Venkatesh},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  title     = {High Dimensional {B}ayesian Optimization with Elastic {G}aussian Process},
  year      = {2017},
  month     = {06--11 Aug},
  pages     = {2883--2891},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {70},
  abstract  = {Bayesian optimization is an efficient way to optimize expensive black-box functions such as designing a new product with highest quality or hyperparameter tuning of a machine learning algorithm. However, it has a serious limitation when the parameter space is high-dimensional as Bayesian optimization crucially depends on solving a global optimization of a surrogate utility function in the same sized dimensions. The surrogate utility function, known commonly as acquisition function is a continuous function but can be extremely sharp at high dimension - having only a few peaks marooned in a large terrain of almost flat surface. Global optimization algorithms such as DIRECT are infeasible at higher dimensions and gradient-dependent methods cannot move if initialized in the flat terrain. We propose an algorithm that enables local gradient-dependent algorithms to move through the flat terrain by using a sequence of gross-to-finer Gaussian process priors on the objective function as we leverage two underlying facts - a) there exists a large enough length-scales for which the acquisition function can be made to have a significant gradient at any location in the parameter space, and b) the extrema of the consecutive acquisition functions are close although they are different only due to a small difference in the length-scales. Theoretical guarantees are provided and experiments clearly demonstrate the utility of the proposed method at high dimension using both benchmark test functions and real-world case studies.},
  groups    = {Constrained BO, Others},
  pdf       = {http://proceedings.mlr.press/v70/rana17a/rana17a.pdf},
  url       = {https://proceedings.mlr.press/v70/rana17a.html},
}

@InProceedings{WangZ2018,
  author    = {Wang, Zi and Gehring, Clement and Kohli, Pushmeet and Jegelka, Stefanie},
  booktitle = {Proceedings of the 21st International Conference on Artificial Intelligence and Statistics},
  title     = {Batched large-scale {B}ayesian optimization in high-dimensional spaces},
  year      = {2018},
  pages     = {745--754},
  publisher = {PMLR},
  volume    = {84},
  groups    = {High-dimension BO},
  issn      = {2640-3498},
  journal   = {International Conference on Artificial Intelligence and Statistics},
  url       = {https://proceedings.mlr.press/v84/wang18c.html},
}

@Article{Constantine2014,
  author    = {Constantine, Paul G. and Dow, Eric and Wang, Qiqi},
  title     = {Active Subspace Methods in Theory and Practice: Applications to {K}riging Surfaces},
  journal   = {SIAM Journal on Scientific Computing},
  year      = {2014},
  volume    = {36},
  number    = {4},
  pages     = {A1500--A1524},
  month     = jan,
  issn      = {1064-8275},
  abstract  = {Many multivariate functions in engineering models vary primarily along a few directions in the space of input parameters. When these directions correspond to coordinate directions, one may apply global sensitivity measures to determine the most influential parameters. However, these methods perform poorly when the directions of variability are not aligned with the natural coordinates of the input space. We present a method to first detect the directions of the strongest variability using evaluations of the gradient and subsequently exploit these directions to construct a response surface on a low-dimensional subspace---i.e., the active subspace---of the inputs. We develop a theoretical framework with error bounds, and we link the theoretical quantities to the parameters of a kriging response surface on the active subspace. We apply the method to an elliptic PDE model with coefficients parameterized by 100 Gaussian random variables and compare it with a local sensitivity analysis method for dimension reduction.},
  comment   = {doi: 10.1137/130916138},
  doi       = {10.1137/130916138},
  groups    = {Others},
  publisher = {Society for Industrial and Applied Mathematics},
  url       = {https://doi.org/10.1137/130916138},
}

@Article{GomezBombarelli2018,
  author    = {Gómez-Bombarelli, Rafael and Wei, Jennifer N. and Duvenaud, David and Hernández-Lobato, José Miguel and Sánchez-Lengeling, Benjamín and Sheberla, Dennis and Aguilera-Iparraguirre, Jorge and Hirzel, Timothy D. and Adams, Ryan P. and Aspuru-Guzik, Alán},
  title     = {Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules},
  journal   = {ACS Central Science},
  year      = {2018},
  volume    = {4},
  number    = {2},
  pages     = {268--276},
  month     = feb,
  issn      = {2374-7943},
  comment   = {doi: 10.1021/acscentsci.7b00572},
  doi       = {10.1021/acscentsci.7b00572},
  groups    = {High-dimension BO, Discrete BO},
  publisher = {American Chemical Society},
  url       = {https://doi.org/10.1021/acscentsci.7b00572},
}

@Article{Roy2011,
  author   = {Roy, Christopher J. and Oberkampf, William L.},
  title    = {A comprehensive framework for verification, validation, and uncertainty quantification in scientific computing},
  journal  = {Computer Methods in Applied Mechanics and Engineering},
  year     = {2011},
  volume   = {200},
  number   = {25},
  pages    = {2131--2144},
  issn     = {0045-7825},
  abstract = {An overview of a comprehensive framework is given for estimating the predictive uncertainty of scientific computing applications. The framework is comprehensive in the sense that it treats both types of uncertainty (aleatory and epistemic), incorporates uncertainty due to the mathematical form of the model, and it provides a procedure for including estimates of numerical error in the predictive uncertainty. Aleatory (random) uncertainties in model inputs are treated as random variables, while epistemic (lack of knowledge) uncertainties are treated as intervals with no assumed probability distributions. Approaches for propagating both types of uncertainties through the model to the system response quantities of interest are briefly discussed. Numerical approximation errors (due to discretization, iteration, and computer round off) are estimated using verification techniques, and the conversion of these errors into epistemic uncertainties is discussed. Model form uncertainty is quantified using (a) model validation procedures, i.e., statistical comparisons of model predictions to available experimental data, and (b) extrapolation of this uncertainty structure to points in the application domain where experimental data do not exist. Finally, methods for conveying the total predictive uncertainty to decision makers are presented. The different steps in the predictive uncertainty framework are illustrated using a simple example in computational fluid dynamics applied to a hypersonic wind tunnel.},
  doi      = {10.1016/j.cma.2011.03.016},
  groups   = {Surveys},
  keywords = {Uncertainty quantification, Verification, Validation, Modeling, Computational simulation},
  url      = {https://www.sciencedirect.com/science/article/pii/S0045782511001290},
}

@Article{Hemez2004,
  author   = {Hemez, François M. and Ben-Haim, Yakov},
  title    = {Info-gap robustness for the correlation of tests and simulations of a non-linear transient},
  journal  = {Mechanical Systems and Signal Processing},
  year     = {2004},
  volume   = {18},
  number   = {6},
  pages    = {1443--1467},
  issn     = {0888-3270},
  abstract = {An alternative to the theory of probability is applied to the problem of assessing the robustness, to uncertainty in model parameters, of the correlation between measurements and computer simulations. The analysis is based on the theory of information-gap uncertainty, which models the clustering of uncertain events in families of nested sets instead of assuming a probability structure. The system investigated is the propagation of a transient impact through a layer of hyper-elastic material. The two sources of non-linearity are (1) the softening of the constitutive law representing the hyper-elastic material and (2) the contact dynamics at the interface between metallic and crushable materials. The robustness of the correlation between test and simulation, to sources of parameter variability, is first studied to identify the parameters of the model that significantly influence the agreement between measurements and predictions. Model updating under non-probabilistic uncertainty is then illustrated, based on two complementary immunity functions: the robustness to uncertainty and the opportunity from uncertainty. Finally an info-gap model is embedded within a probability density function to represent uncertainty in the knowledge of the model's parameters and their correlation structure. Although computationally expensive, it is demonstrated that info-gap reasoning can greatly enhance our understanding of a moderately complex system when the theory of probability cannot be applied due to insufficient information.},
  doi      = {10.1016/j.ymssp.2004.03.001},
  groups   = {Others},
  keywords = {Information-gap, Nonlinear dynamics, Test-analysis correlation, Robustness, Uncertainty},
  url      = {https://www.sciencedirect.com/science/article/pii/S0888327004000317},
}

@Article{Caflisch1998,
  author    = {Caflisch, Russel E.},
  journal   = {Acta Numerica},
  title     = {{Monte Carlo and quasi-Monte Carlo methods}},
  year      = {1998},
  issn      = {0962-4929},
  pages     = {1--49},
  volume    = {7},
  abstract  = {Monte Carlo is one of the most versatile and widely used numerical methods. Its convergence rate, O(N−1/2), is independent of dimension, which shows Monte Carlo to be very robust but also slow. This article presents an introduction to Monte Carlo methods for integration problems, including convergence theory, sampling methods and variance reduction techniques. Accelerated convergence for Monte Carlo quadrature is attained using quasi-random (also called low-discrepancy) sequences, which are a deterministic alternative to random or pseudo-random sequences. The points in a quasi-random sequence are correlated to provide greater uniformity. The resulting quadrature method, called quasi-Monte Carlo, has a convergence rate of approximately O((logN)kN−1). For quasi-Monte Carlo, both theoretical error estimates and practical limitations are presented. Although the emphasis in this article is on integration, Monte Carlo simulation of rarefied gas dynamics is also discussed. In the limit of small mean free path (that is, the fluid dynamic limit), Monte Carlo loses its effectiveness because the collisional distance is much less than the fluid dynamic length scale. Computational examples are presented throughout the text to illustrate the theory. A number of open problems are described.},
  database  = {Cambridge Core},
  doi       = {10.1017/S0962492900002804},
  groups    = {Others},
  publisher = {Cambridge University Press},
  url       = {https://www.cambridge.org/core/article/monte-carlo-and-quasimonte-carlo-methods/FE7C779B350CFEA45DB2A4CCB2DA9B5C},
}

@Article{Crestaux2009,
  author   = {Crestaux, Thierry and Le Maıˆtre, Olivier and Martinez, Jean-Marc},
  title    = {Polynomial chaos expansion for sensitivity analysis},
  journal  = {Reliability Engineering {\&} System Safety},
  year     = {2009},
  volume   = {94},
  number   = {7},
  pages    = {1161--1172},
  issn     = {0951-8320},
  abstract = {In this paper, the computation of Sobol's sensitivity indices from the polynomial chaos expansion of a model output involving uncertain inputs is investigated. It is shown that when the model output is smooth with regards to the inputs, a spectral convergence of the computed sensitivity indices is achieved. However, even for smooth outputs the method is limited to a moderate number of inputs, say 10-20, as it becomes computationally too demanding to reach the convergence domain. Alternative methods (such as sampling strategies) are then more attractive. The method is also challenged when the output is non-smooth even when the number of inputs is limited.},
  doi      = {10.1016/j.ress.2008.10.008},
  groups   = {Others},
  keywords = {Sensitivity analysis, Sobol's decomposition, Polynomial chaos, Uncertainty quantification},
  url      = {https://www.sciencedirect.com/science/article/pii/S0951832008002561},
}

@Article{Anderson2012,
  author   = {Anderson, Travis V. and Mattson, Christopher A.},
  title    = {Propagating Skewness and Kurtosis Through Engineering Models for Low-Cost, Meaningful, Nondeterministic Design},
  journal  = {Journal of Mechanical Design},
  year     = {2012},
  volume   = {134},
  number   = {10},
  month    = sep,
  issn     = {1050-0472},
  abstract = {System models help designers predict actual system output. Generally, variation in system inputs creates variation in system outputs. Designers often propagate variance through a system model by taking a derivative-based weighted sum of each input’s variance. This method is based on a Taylor-series expansion. Having an output mean and variance, designers typically assume the outputs are Gaussian. This paper demonstrates that outputs are rarely Gaussian for nonlinear functions, even with Gaussian inputs. This paper also presents a solution for system designers to more meaningfully describe the system output distribution. This solution consists of using equations derived from a second-order Taylor series that propagate skewness and kurtosis through a system model. If a second-order Taylor series is used to propagate variance, these higher-order statistics can also be propagated with minimal additional computational cost. These higher-order statistics allow the system designer to more accurately describe the distribution of possible outputs. The benefits of including higher-order statistics in error propagation are clearly illustrated in the example of a flat-rolling metalworking process used to manufacture metal plates.},
  doi      = {10.1115/1.4007389},
  groups   = {Others},
  url      = {https://doi.org/10.1115/1.4007389},
}

@Article{OHagan1991,
  author   = {O'Hagan, A.},
  title    = {{Bayes-Hermite quadrature}},
  journal  = {Journal of Statistical Planning and Inference},
  year     = {1991},
  volume   = {29},
  number   = {3},
  pages    = {245--260},
  issn     = {0378-3758},
  abstract = {Bayesian quadrature treats the problem of numerical integration as one of statistical inference. A prior Gaussian process distribution is assumed for the integrand, observations arise from evaluating the integrand at selected points, and a posterior distribution is derived for the integrand and the integral. Methods are developed for quadrature in Rp. A particular application is integrating the posterior density arising from some other Bayesian analysis. Simulation results are presented, to show that the resulting Bayes-Hermite quadrature rules may perform better than the conventional Gauss-Hermite rules for this application. A key result is derived for product designs, which makes Bayesian quadrature practically useful for integrating in several dimensions. Although the method does not at present provide a solution to the more difficult problem of quadrature in high dimensions, it does seem to offer real improvements over existing methods in relatively low dimensions.},
  doi      = {10.1016/0378-3758(91)90002-V},
  groups   = {Others},
  keywords = {Bayesian quadrature, numerical integration, Gaussian process, product rule, Gaussian quadrature},
  url      = {https://www.sciencedirect.com/science/article/pii/037837589190002V},
}

@InProceedings{Bossek2020,
  author    = {Bossek, Jakob and Doerr, Carola and Kerschke, Pascal},
  title     = {{Initial Design Strategies and Their Effects on Sequential Model-Based Optimization: An Exploratory Case Study Based on BBOB}},
  booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference},
  year      = {2020},
  series    = {GECCO '20},
  pages     = {778–786},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  abstract  = {Sequential model-based optimization (SMBO) approaches are algorithms for solving problems that require computationally or otherwise expensive function evaluations. The key design principle of SMBO is a substitution of the true objective function by a surrogate, which is used to propose the point(s) to be evaluated next.SMBO algorithms are intrinsically modular, leaving the user with many important design choices. Significant research efforts go into understanding which settings perform best for which type of problems. Most works, however, focus on the choice of the model, the acquisition function, and the strategy used to optimize the latter. The choice of the initial sampling strategy, however, receives much less attention. Not surprisingly, quite diverging recommendations can be found in the literature.We analyze in this work how the size and the distribution of the initial sample influences the overall quality of the efficient global optimization (EGO) algorithm, a well-known SMBO approach. While, overall, small initial budgets using Halton sampling seem preferable, we also observe that the performance landscape is rather unstructured. We furthermore identify several situations in which EGO performs unfavorably against random sampling. Both observations indicate that an adaptive SMBO design could be beneficial, making SMBO an interesting test-bed for automated algorithm design.},
  doi       = {10.1145/3377930.3390155},
  groups    = {Others},
  isbn      = {9781450371285},
  keywords  = {initial design, design of experiments, continuous black-box optimization, sequential model-based optimization},
  location  = {Canc\'{u}n, Mexico},
  numpages  = {9},
  url       = {https://doi-org.ezproxy.lib.uh.edu/10.1145/3377930.3390155},
}

@InProceedings{ZhangF2023,
  author    = {Zhang, Fengxue and Song, Jialin and Bowden, James C and Ladd, Alexander and Yue, Yisong and Desautels, Thomas and Chen, Yuxin},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  title     = {Learning Regions of Interest for {B}ayesian Optimization with Adaptive Level-Set Estimation},
  year      = {2023},
  month     = {23--29 Jul},
  pages     = {41579--41595},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {202},
  abstract  = {We study Bayesian optimization (BO) in high-dimensional and non-stationary scenarios. Existing algorithms for such scenarios typically require extensive hyperparameter tuning, which limits their practical effectiveness. We propose a framework, called BALLET, which adaptively filters for a high-confidence region of interest (ROI) as a superlevel-set of a nonparametric probabilistic model such as a Gaussian process (GP). Our approach is easy to tune, and is able to focus on local region of the optimization space that can be tackled by existing BO methods. The key idea is to use two probabilistic models: a coarse GP to identify the ROI, and a localized GP for optimization within the ROI. We show theoretically that BALLET can efficiently shrink the search space, and can exhibit a tighter regret bound than standard BO without ROI filtering. We demonstrate empirically the effectiveness of BALLET on both synthetic and real-world optimization tasks.},
  groups    = {Others},
  pdf       = {https://proceedings.mlr.press/v202/zhang23aj/zhang23aj.pdf},
  url       = {https://proceedings.mlr.press/v202/zhang23aj.html},
}

@Article{Sobieski1997,
  author   = {Sobieszczanski-Sobieski, J. and Haftka, R. T.},
  title    = {Multidisciplinary aerospace design optimization: survey of recent developments},
  journal  = {Structural optimization},
  year     = {1997},
  volume   = {14},
  number   = {1},
  pages    = {1--23},
  issn     = {1615-1488},
  abstract = {The increasing complexity of engineering systems has sparked rising interest in multidisciplinary optimization (MDO). This paper surveys recent publications in the field of aerospace, in which the interest in MDO has been particularly intense. The primary c hallenges in MDO are computational expense and organizational complexity. Accordingly, this survey focuses on various methods used by different researchers to address these challenges. The survey is organized by a breakdown of MDO into its conceptual components, reflected in sections on mathematical modelling, approximation concepts, optimization procedures, system sensitivity, and human interface. Because the authors' primary area of expertise is in the structures discipline, the majority of the references focus on the interaction of this discipline with others. In particular, two sections at the end of this review focus on two interactions that have recently been pursued with vigour: the simultaneous optimization of structures and aerodynamics and the simultaneous optimization of structures with active control.},
  doi      = {10.1007/BF01197554},
  groups   = {Surveys},
  refid    = {Sobieszczanski-Sobieski1997},
  url      = {https://doi.org/10.1007/BF01197554},
}

@InCollection{Torczon1998,
  author    = {Torczon, Virginia and Trosset, Michael},
  booktitle = {The 7th AIAA/USAF/NASA/ISSMO Symposium on Multidisciplinary Analysis and Optimization},
  publisher = {American Institute of Aeronautics and Astronautics},
  title     = {Using approximations to accelerate engineering design optimization},
  year      = {1998},
  month     = sep,
  series    = {Multidisciplinary Analysis Optimization Conferences},
  comment   = {doi:10.2514/6.1998-4800},
  doi       = {10.2514/6.1998-4800},
  groups    = {Others},
  url       = {https://doi.org/10.2514/6.1998-4800},
}

@Article{Simpson2001,
  author    = {Simpson, Timothy W. and Mauery, Timothy M. and Korte, John J. and Mistree, Farrokh},
  title     = {Kriging Models for Global Approximation in Simulation-Based Multidisciplinary Design Optimization},
  journal   = {AIAA Journal},
  year      = {2001},
  volume    = {39},
  number    = {12},
  pages     = {2233--2241},
  month     = dec,
  issn      = {0001-1452},
  comment   = {doi: 10.2514/2.1234},
  doi       = {10.2514/2.1234},
  groups    = {Others},
  publisher = {American Institute of Aeronautics and Astronautics},
  url       = {https://doi.org/10.2514/2.1234},
}

@Article{Box1951,
  author    = {Box, G. E. P. and Wilson, K. B.},
  journal   = {Journal of the Royal Statistical Society. Series B (Methodological)},
  title     = {On the Experimental Attainment of Optimum Conditions},
  year      = {1951},
  number    = {1},
  pages     = {1--45},
  volume    = {13},
  database  = {JSTOR},
  groups    = {Others},
  publisher = {[Royal Statistical Society, Wiley]},
  url       = {http://www.jstor.org/stable/2983966},
}

@Article{Toal2008,
  author    = {Toal, David J. J. and Bressloff, Neil W. and Keane, Andy J.},
  title     = {Kriging Hyperparameter Tuning Strategies},
  journal   = {AIAA Journal},
  year      = {2008},
  volume    = {46},
  number    = {5},
  pages     = {1240--1252},
  month     = may,
  issn      = {0001-1452},
  comment   = {doi: 10.2514/1.34822},
  doi       = {10.2514/1.34822},
  groups    = {Others},
  publisher = {American Institute of Aeronautics and Astronautics},
  url       = {https://doi.org/10.2514/1.34822},
}

@Article{Hertog2006,
  author    = {den Hertog, D. and Kleijnen, J. P. C. and Siem, A. Y. D.},
  title     = {The correct {K}riging variance estimated by bootstrapping},
  journal   = {Journal of the Operational Research Society},
  year      = {2006},
  volume    = {57},
  number    = {4},
  pages     = {400--409},
  month     = apr,
  issn      = {0160-5682},
  comment   = {doi: 10.1057/palgrave.jors.2601997},
  doi       = {10.1057/palgrave.jors.2601997},
  groups    = {Others},
  publisher = {Taylor \& Francis},
  url       = {https://doi.org/10.1057/palgrave.jors.2601997},
}

@Article{Knill1999,
  author    = {Knill, Duane L. and Giunta, Anthony A. and Baker, Chuck A. and Grossman, Bernard and Mason, William H. and Haftka, Raphael T. and Watson, Layne T.},
  title     = {Response Surface Models Combining Linear and Euler Aerodynamics for Supersonic Transport Design},
  journal   = {Journal of Aircraft},
  year      = {1999},
  volume    = {36},
  number    = {1},
  pages     = {75--86},
  month     = jan,
  issn      = {0021-8669},
  comment   = {doi: 10.2514/2.2415},
  doi       = {10.2514/2.2415},
  groups    = {Others},
  publisher = {American Institute of Aeronautics and Astronautics},
  url       = {https://doi.org/10.2514/2.2415},
}

@Article{Hussain2002,
  author   = {Hussain, Mohammed F. and Barton, Russel R. and Joshi, Sanjay B.},
  title    = {{Metamodeling: Radial basis functions, versus polynomials}},
  journal  = {European Journal of Operational Research},
  year     = {2002},
  volume   = {138},
  number   = {1},
  pages    = {142--154},
  issn     = {0377-2217},
  abstract = {For many years, metamodels have been used in simulation to provide approximations to the input-output functions provided by a simulation model. In this paper, metamodels based on radial basis functions are applied to approximate test functions known from the literature. These tests were conducted to gain insights into the behavior of these metamodels, their ease of computation and their ability to capture the shape and minima of the test functions. These metamodels are compared against polynomial metamodels by using surface and contour graphs of the error function (difference between metamodel and the given function) and by evaluating the numerical stability of the required computations. Full factorial and Latin hypercube designs were used to fit the metamodels. Graphical and statistical methods were used to analyze the test results. Factorial designs were generally more successful for fitting the test functions as compared to Latin hypercube designs. Radial basis function metamodels using factorial and Latin hypercube designs provided better fit than polynomial metamodels using full factorial designs.},
  doi      = {10.1016/S0377-2217(01)00076-5},
  groups   = {Others},
  keywords = {Metamodeling, Radial basis functions, Response surface, Polynomial metamodels},
  url      = {https://www.sciencedirect.com/science/article/pii/S0377221701000765},
}

@Article{Girosi1998,
  author   = {Girosi, Federico},
  journal  = {Neural Computation},
  title    = {An Equivalence Between Sparse Approximation and Support Vector Machines},
  year     = {1998},
  issn     = {0899-7667},
  month    = aug,
  number   = {6},
  pages    = {1455--1480},
  volume   = {10},
  abstract = {This article shows a relationship between two different approximation techniques: the support vector machines (SVM), proposed by V. Vapnik (1995) and a sparse approximation scheme that resembles the basis pursuit denoising algorithm (Chen, 1995; Chen, Donoho, \&amp; Saunders, 1995). SVM is a technique that can be derived from the structural risk minimization principle (Vapnik, 1982) and can be used to estimate the parameters of several different approximation schemes, including radial basis functions, algebraic and trigonometric polynomials, B-splines, and some forms of multilayer perceptrons. Basis pursuit denoising is a sparse approximation technique in which a function is reconstructed by using a small number of basis functions chosen from a large set (the dictionary). We show that if the data are noiseless, the modified version of basis pursuit denoising proposed in this article is equivalent to SVM in the following sense: if applied to the same data set, the two techniques give the same solution, which is obtained by solving the same quadratic programming problem. In the appendix, we present a derivation of the SVM technique in the framework of regularization theory, rather than statistical learning theory, establishing a connection between SVM, sparse approximation, and regularization theory.},
  doi      = {10.1162/089976698300017269},
  groups   = {Others},
  url      = {https://doi.org/10.1162/089976698300017269},
}

@Article{Cressie1990,
  author   = {Cressie, Noel},
  title    = {The origins of kriging},
  journal  = {Mathematical Geology},
  year     = {1990},
  volume   = {22},
  number   = {3},
  pages    = {239--252},
  issn     = {1573-8868},
  abstract = {In this article, kriging is equated with spatial optimal linear prediction, where the unknown random-process mean is estimated with the best linear unbiased estimator. This allows early appearances of (spatial) prediction techniques to be assessed in terms of how close they came to kriging.},
  doi      = {10.1007/BF00889887},
  groups   = {Others},
  refid    = {Cressie1990},
  url      = {https://doi.org/10.1007/BF00889887},
}

@Article{Kleijnen2009,
  author   = {Kleijnen, Jack P. C.},
  title    = {Kriging metamodeling in simulation: A review},
  journal  = {European Journal of Operational Research},
  year     = {2009},
  volume   = {192},
  number   = {3},
  pages    = {707--716},
  issn     = {0377-2217},
  abstract = {This article reviews Kriging (also called spatial correlation modeling). It presents the basic Kriging assumptions and formulas--contrasting Kriging and classic linear regression metamodels. Furthermore, it extends Kriging to random simulation, and discusses bootstrapping to estimate the variance of the Kriging predictor. Besides classic one-shot statistical designs such as Latin Hypercube Sampling, it reviews sequentialized and customized designs for sensitivity analysis and optimization. It ends with topics for future research.},
  doi      = {10.1016/j.ejor.2007.10.013},
  groups   = {Surveys},
  keywords = {Kriging, Metamodel, Response surface, Interpolation, Optimization, Design},
  url      = {https://www.sciencedirect.com/science/article/pii/S0377221707010090},
}

@Article{Goel2007,
  author       = {Goel, Tushar and Haftka, Raphael T. and Shyy, Wei and Queipo, Nestor V.},
  journal      = {Structural and Multidisciplinary Optimization},
  title        = {Ensemble of surrogates},
  year         = {2007},
  issn         = {1615-1488},
  number       = {3},
  pages        = {199--216},
  volume       = {33},
  abstract     = {The custom in surrogate-based modeling of complex engineering problems is to fit one or more surrogate models and select the one surrogate model that performs best. In this paper, we extend the utility of an ensemble of surrogates to (1) identify regions of possible high errors at locations where predictions of surrogates widely differ, and (2) provide a more robust approximation approach. We explore the possibility of using the best surrogate or a weighted average surrogate model instead of individual surrogate models. The weights associated with each surrogate model are determined based on the errors in surrogates. We demonstrate the advantages of an ensemble of surrogates using analytical problems and one engineering problem. We show that for a single problem the choice of test surrogate can depend on the design of experiments.},
  comment-ruda = {An error-based weighted average of LFMs.},
  doi          = {10.1007/s00158-006-0051-9},
  groups       = {Others},
  refid        = {Goel2007},
  url          = {https://doi.org/10.1007/s00158-006-0051-9},
}

@Article{Sobester2004,
  author   = {András Sóbester and Leary, S. J. and Keane, A. J.},
  title    = {A parallel updating scheme for approximating and optimizing high fidelity computer simulations},
  journal  = {Structural and Multidisciplinary Optimization},
  year     = {2004},
  volume   = {27},
  number   = {5},
  pages    = {371--383},
  issn     = {1615-1488},
  abstract = {Approximation methods are often used to construct surrogate models, which can replace expensive computer simulations for the purposes of optimization. One of the most important aspects of such optimization techniques is the choice of model updating strategy. In this paper we employ parallel updates by searching an expected improvement surface generated from a radial basis function model. We look at optimization based on standard and gradient-enhanced models. Given Np processors, the best Np local maxima of the expected improvement surface are highlighted and further runs are performed on these designs. To test these ideas, simple analytic functions and a finite element model of a simple structure are analysed and various approaches compared.},
  doi      = {10.1007/s00158-004-0397-9},
  groups   = {Sequential model-based},
  refid    = {Sóbester2004},
  url      = {https://doi.org/10.1007/s00158-004-0397-9},
}

@Article{ChenZ2023,
  author    = {Chen, Zhehui and Mak, Simon and Wu, C. F. Jeff},
  journal   = {Journal of the American Statistical Association},
  title     = {{A Hierarchical Expected Improvement Method for Bayesian Optimization}},
  year      = {2023},
  issn      = {0162-1459},
  pages     = {1--14},
  comment   = {doi: 10.1080/01621459.2023.2210803},
  doi       = {10.1080/01621459.2023.2210803},
  groups    = {Acquisition functions},
  publisher = {Taylor \& Francis},
  url       = {https://doi.org/10.1080/01621459.2023.2210803},
}

@InProceedings{Kaufmann2012,
  author    = {Kaufmann, Emilie and Cappe, Olivier and Garivier, Aurelien},
  booktitle = {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},
  title     = {{On Bayesian upper confidence bounds for bandit problems}},
  year      = {2012},
  address   = {La Palma, Canary Islands},
  editor    = {Lawrence, Neil D. and Girolami, Mark},
  month     = {21--23 Apr},
  pages     = {592--600},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {22},
  abstract  = {Stochastic bandit problems have been analyzed from two different perspectives: a frequentist view, where the parameter is a deterministic unknown quantity, and a Bayesian approach, where the parameter is drawn from a prior distribution.  We show in this paper that methods derived from this second perspective prove optimal when evaluated using the frequentist cumulated regret as a measure of performance. We give a general formulation for a class of Bayesian index policies that rely on quantiles of the posterior distribution. For binary bandits, we prove that the corresponding algorithm, termed Bayes-UCB, satisfies finite-time regret bounds that imply its asymptotic optimality.  More generally, Bayes-UCB appears as an unifying framework for several variants of the UCB algorithm addressing different bandit problems (parametric multi-armed bandits, Gaussian bandits with unknown mean and variance, linear bandits). But the generality of the Bayesian approach makes it possible to address more challenging models. In particular, we show how to handle linear bandits with sparsity constraints by resorting to Gibbs sampling.},
  groups    = {Acquisition functions},
  pdf       = {http://proceedings.mlr.press/v22/kaufmann12/kaufmann12.pdf},
  url       = {https://proceedings.mlr.press/v22/kaufmann12.html},
}

@Article{Blanchard2021jcp,
  author   = {Blanchard, Antoine and Sapsis, Themistoklis},
  journal  = {Journal of Computational Physics},
  title    = {Bayesian optimization with output-weighted optimal sampling},
  year     = {2021},
  issn     = {0021-9991},
  pages    = {109901},
  volume   = {425},
  abstract = {In Bayesian optimization, accounting for the importance of the output relative to the input is a crucial yet challenging exercise, as it can considerably improve the final result but often involves inaccurate and cumbersome entropy estimations. We approach the problem from the perspective of importance-sampling theory, and advocate the use of the likelihood ratio to guide the search algorithm towards regions of the input space where the objective function to minimize assumes abnormally small values. The likelihood ratio acts as a sampling weight and can be computed at each iteration without severely deteriorating the overall efficiency of the algorithm. In particular, it can be approximated in a way that makes the approach tractable in high dimensions. The “likelihood-weighted” acquisition functions introduced in this work are found to outperform their unweighted counterparts in a number of applications.},
  doi      = {10.1016/j.jcp.2020.109901},
  groups   = {Acquisition functions},
  keywords = {Bayesian optimization, Optimal sampling, Extreme events},
  url      = {https://www.sciencedirect.com/science/article/pii/S0021999120306756},
}

@Article{Rodriguez2001,
  author   = {Rodríguez, J. F. and Pérez, V. M. and Padmanabhan, D. and Renaud, J. E.},
  title    = {Sequential approximate optimization using variable fidelity response surface approximations},
  journal  = {Structural and Multidisciplinary Optimization},
  year     = {2001},
  volume   = {22},
  number   = {1},
  pages    = {24--34},
  issn     = {1615-1488},
  abstract = {The dimensionality and complexity of typical multidisciplinary systems hinders the use of formal optimization techniques in application to this class of problems. The use of approximations to represent the system design metrics and constraints has become vital for achieving good performance in many multidisciplinary design optimization (MDO) algorithms. This paper reports recent research efforts on the use of variable fidelity response surface approximations (RSA) to drive the convergence of MDO problems using a trust region model management algorithm. The present study focuses on a comparative study of different response sampling strategies based on design of experiment (DOE) approaches within the disciplines to generate the zero order data to build the RSAs. Two MDO test problems that have complex coupling between disciplines are used to benchmark the performance of each sampling strategy. The results show that these types of variable fidelity RSAs can be effectively managed by the trust region model management strategy to drive convergence of MDO problems. It is observed that the efficiency of the optimization algorithm depends on the sampling strategy used. A comparison of the DOE approaches with those obtained using a optimization based sampling strategy (i.e. concurrent subspace optimization - CSSO) shows the DOE methodologies to be competitive with the CSSO based sampling methodology in some cases. However, the CSSO based sampling strategy was found to be, in general, more efficient in driving the optimization.},
  doi      = {10.1007/s001580100122},
  groups   = {Sequential model-based},
  refid    = {Rodríguez2001},
  url      = {https://doi.org/10.1007/s001580100122},
}

@Book{Chiles1999,
  author    = {Chiles, Jean-Paul and Delfiner, Pierre},
  publisher = {John Wiley {\&} Sons},
  title     = {Geostatistics: modeling spatial uncertainty},
  year      = {1999},
  address   = {New York, USA},
  doi       = {10.1002/9781118136188},
  groups    = {Books},
}

@InCollection{Rao2019,
  author    = {Singiresu S. Rao},
  booktitle = {Engineering Optimization Theory and Practice},
  publisher = {John Wiley {\&} Sons},
  title     = {Dynamic Programming},
  year      = {2019},
  isbn      = {9781119454816},
  month     = oct,
  pages     = {497--536},
  abstract  = {Summary Developed by Richard Bellman, dynamic programming is a mathematical technique well suited for the optimization of multistage decision problems. As applied to dynamic programming, a multistage decision process is one in which a number of single-stage processes are connected in series so that the output of one stage is the input of the succeeding stage. The dynamic programming makes use of the concept of suboptimization and the principle of optimality in solving this problem. The concept of suboptimization and the principle of optimality are explained through the example of an initial value problem. A linear programming problem can be formulated as a dynamic programming problem. The chapter explains this by illustrating the conversion of a linear programming problem into a dynamic programming problem. An infinite-stage or continuous decision problem may arise in several practical problems.},
  comment   = {Wiley Online Books},
  doi       = {10.1002/9781119454816.ch9},
  groups    = {Books},
  keywords  = {continuous dynamic programming, initial value problem, linear programming problem, multistage decision processes, principle of optimality, Richard Bellman, suboptimization},
  url       = {https://doi.org/10.1002/9781119454816.ch9},
}

@InProceedings{WuJ2016,
  author    = {Wu, Jian and Frazier, Peter I.},
  booktitle = {Proceedings of the 30th Conference on Neural Information Processing Systems},
  title     = {{The parallel knowledge gradient method for batch Bayesian optimization}},
  year      = {2016},
  pages     = {3126--3134},
  publisher = {NIPS 2016},
  groups    = {Acquisition functions},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2016/hash/18d10dc6e666eab6de9215ae5b3d54df-Abstract.html},
}

@InProceedings{Kandasamy2016,
  author    = {Kandasamy, Kirthevasan and Dasarathy, Gautam and Oliva, Junier B and Schneider, Jeff and Poczos, Barnabas},
  title     = {Gaussian Process Bandit Optimisation with Multi-fidelity Evaluations},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2016},
  volume    = {29},
  pages     = {992--1000},
  publisher = {NIPS 2016},
  groups    = {Acquisition functions},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2016/file/605ff764c617d3cd28dbbdd72be8f9a2-Paper.pdf},
}

@Book{Luenberger2008,
  author    = {Luenberger, David G. and Ye, Yinyu},
  publisher = {Springer},
  title     = {Linear and nonlinear programming},
  year      = {2008},
  address   = {California, USA},
  doi       = {10.1007/978-0-387-74503-9},
  groups    = {Books},
}

@Article{Larson2019,
  author    = {Larson, Jeffrey and Menickelly, Matt and Wild, Stefan M.},
  journal   = {Acta Numerica},
  title     = {Derivative-free optimization methods},
  year      = {2019},
  issn      = {0962-4929},
  pages     = {287--404},
  volume    = {28},
  abstract  = {In many optimization problems arising from scientific, engineering and artificial intelligence applications, objective and constraint functions are available only as the output of a black-box or simulation oracle that does not provide derivative information. Such settings necessitate the use of methods for derivative-free, or zeroth-order, optimization. We provide a review and perspectives on developments in these methods, with an emphasis on highlighting recent developments and on unifying treatment of such problems in the non-linear optimization and machine learning literature. We categorize methods based on assumed properties of the black-box functions, as well as features of the methods. We first overview the primary setting of deterministic methods applied to unconstrained, non-convex optimization problems where the objective function is defined by a deterministic black-box oracle. We then discuss developments in randomized methods, methods that assume some additional structure about the objective (including convexity, separability and general non-smooth compositions), methods for problems where the output of the black-box oracle is stochastic, and methods for handling different types of constraints.},
  database  = {Cambridge Core},
  doi       = {10.1017/S0962492919000060},
  groups    = {Others},
  publisher = {Cambridge University Press},
  url       = {https://www.cambridge.org/core/article/derivativefree-optimization-methods/84479E2B03A9BFFE0F9CD46CF9FCD289},
}

@Article{Zhou2018,
  author   = {Zhou, Qi and Wang, Yan and Choi, Seung-Kyum and Jiang, Ping and Shao, Xinyu and Hu, Jiexiang and Shu, Leshi},
  journal  = {Structural and Multidisciplinary Optimization},
  title    = {A robust optimization approach based on multi-fidelity metamodel},
  year     = {2018},
  issn     = {1615-1488},
  number   = {2},
  pages    = {775--797},
  volume   = {57},
  abstract = {Multi-fidelity (MF) metamodeling approaches have recently attracted a significant amount of attention in simulation-based design optimization due to their ability to conduct trade-offs between high accuracy and low computational expenses by integrating the information from high-fidelity (HF) and low-fidelity (LF) models. While existing MF metamodel assisted design optimization approaches may yield an inferior or even infeasible solution since they generally treat the MF metamodel as the real HF model and ignore the interpolation uncertainties from the MF metamodel. This situation will be more serious in non-deterministic optimization. Hence, in this work, a MF metamodel assisted robust optimization approach is developed, in which the interpolation uncertainty of the MF metamodel and design variable uncertainty are quantified and taken into consideration. To demonstrate the effectiveness and merits of the proposed approach, two numerical examples and a long cylinder pressure vessel design optimization problem are tested. Results show that for the test cases the proposed approach can obtain a solution that is both optimal and within the feasible region even with perturbation of the uncertain variables.},
  doi      = {10.1007/s00158-017-1783-4},
  refid    = {Zhou2018},
  url      = {https://doi.org/10.1007/s00158-017-1783-4},
}

@Article{Erickson2018,
  author   = {Erickson, Collin B. and Ankenman, Bruce E. and Sanchez, Susan M.},
  journal  = {European Journal of Operational Research},
  title    = {Comparison of {G}aussian process modeling software},
  year     = {2018},
  issn     = {0377-2217},
  number   = {1},
  pages    = {179--192},
  volume   = {266},
  abstract = {Gaussian process fitting, or kriging, is often used to create a model from a set of data. Many available software packages do this, but we show that very different results can be obtained from different packages even when using the same data and model. We describe the parameterization, features, and optimization used by eight different fitting packages that run on four different platforms. We then compare these eight packages using various data functions and data sets, revealing that there are stark differences between the packages. In addition to comparing the prediction accuracy, the predictive variance - which is important for evaluating precision of predictions and is often used in stopping criteria - is also evaluated.},
  doi      = {10.1016/j.ejor.2017.10.002},
  keywords = {Simulation, Gaussian processes, Stochastic kriging, Metamodels, Computer experiments},
  url      = {https://www.sciencedirect.com/science/article/pii/S0377221717308962},
}

@Article{Christianson2023,
  author    = {Christianson, Ryan B. and Pollyea, Ryan M. and Gramacy, Robert B.},
  journal   = {Statistical Analysis and Data Mining},
  title     = {Traditional kriging versus modern Gaussian processes for large-scale mining data},
  year      = {2023},
  issn      = {1932-1864},
  month     = oct,
  number    = {5},
  pages     = {488--506},
  volume    = {16},
  abstract  = {Abstract The canonical technique for nonlinear modeling of spatial/point-referenced data is known as kriging in geostatistics, and as Gaussian Process (GP) regression for surrogate modeling and statistical learning. This article reviews many similarities shared between kriging and GPs, but also highlights some important differences. One is that GPs impose a process that can be used to automate kernel/variogram inference, thus removing the human from the loop. The GP framework also suggests a probabilistically valid means of scaling to handle a large corpus of training data, that is, an alternative to ordinary kriging. Finally, recent GP implementations are tailored to make the most of modern computing architectures, such as multi-core workstations and multi-node supercomputers. We argue that such distinctions are important even in classically geostatistical settings. To back that up, we present out-of-sample validation exercises using two, real, large-scale borehole data sets acquired in the mining of gold and other minerals. We compare classic kriging with several variations of modern GPs and conclude that the latter is more economical (fewer human and compute resources), more accurate and offers better uncertainty quantification. We go on to show how the fully generative modeling apparatus provided by GPs can gracefully accommodate left-censoring of small measurements, as commonly occurs in mining data and other borehole assays.},
  doi       = {10.1002/sam.11635},
  keywords  = {Gaussian process regression, multiple imputation, ordinary kriging, surrogate modeling, variogram, Vecchia approximation},
  publisher = {John Wiley & Sons, Ltd},
  url       = {https://doi.org/10.1002/sam.11635},
}

@Book{Antoulas2005,
  author    = {Antoulas, Athanasios C.},
  publisher = {Society for Industrial and Applied Mathematics},
  title     = {Approximation of Large-Scale Dynamical Systems},
  year      = {2005},
  doi       = {10.1137/1.9780898718713},
  url       = {https://epubs.siam.org/doi/abs/10.1137/1.9780898718713},
}

@Article{LiuB2016,
  author   = {Liu, Bo and Koziel, Slawomir and Zhang, Qingfu},
  journal  = {Journal of Computational Science},
  title    = {A multi-fidelity surrogate-model-assisted evolutionary algorithm for computationally expensive optimization problems},
  year     = {2016},
  issn     = {1877-7503},
  pages    = {28--37},
  volume   = {12},
  abstract = {Integrating data-driven surrogate models and simulation models of different accuracies (or fidelities) in a single algorithm to address computationally expensive global optimization problems has recently attracted considerable attention. However, handling discrepancies between simulation models with multiple fidelities in global optimization is a major challenge. To address it, the two major contributions of this paper include: (1) development of a new multi-fidelity surrogate-model-based optimization framework, which substantially improves reliability and efficiency of optimization compared to many existing methods, and (2) development of a data mining method to address the discrepancy between the low- and high-fidelity simulation models. A new efficient global optimization method is then proposed, referred to as multi-fidelity Gaussian process and radial basis function-model-assisted memetic differential evolution. Its advantages are verified by mathematical benchmark problems and a real-world antenna design automation problem.},
  doi      = {10.1016/j.jocs.2015.11.004},
  keywords = {Multi-fidelity, Multilevel, Variable fidelity, Surrogate-model-assisted evolutionary algorithm, Expensive optimization},
  url      = {https://www.sciencedirect.com/science/article/pii/S1877750315300387},
}

@Article{Zhou2017kbs,
  author   = {Zhou, Qi and Wang, Yan and Choi, Seung-Kyum and Jiang, Ping and Shao, Xinyu and Hu, Jiexiang},
  journal  = {Knowledge-Based Systems},
  title    = {A sequential multi-fidelity metamodeling approach for data regression},
  year     = {2017},
  issn     = {0950-7051},
  pages    = {199--212},
  volume   = {134},
  abstract = {Multi-fidelity (MF) metamodeling approaches have attracted significant attention recently for data regression because they can make a trade-off between high accuracy and low computational expense by integrating the information from high-fidelity (HF) and low-fidelity (LF) models. To facilitate the usage of the MF metamodeling approaches, there are still challenging issues on the sample size ratio between HF and LF models and the locations of samples since these two components have profound effects on the prediction accuracy of the MF metamodels. In this study, a sequential multi-fidelity (SMF) metamodeling approach is proposed to address the issues of 1) where to allocate the LF and HF sample points, and 2) how to obtain an optimal combination of the high and low-fidelity sample sizes for a given computational budget and a high-to-low simulation cost ratio. Firstly, sequential objective formulations, with the objective to reduce the estimation of prediction error of MF metamodel, are constructed to update the LF and HF sampling data. Secondly, a decision criterion is proposed to determine whether one HF experiment or several LF experiments with the equivalent computational cost should be selected to update the MF metamodel. The proposed criterion is developed according to which selection will have a greater potential value to improve the prediction accuracy of the MF metamodel. To demonstrate the effectiveness and merits of the proposed SMF metamodeling approach, two numerical examples and a practical aerospace application example are used. Results show that the proposed approach can generate more accurate MF metamodels by providing the optimal high-to-low sample size ratio and sample locations.},
  doi      = {10.1016/j.knosys.2017.07.033},
  keywords = {Multi-fidelity information, Gaussian process model, Sequential design, Prediction accuracy},
  url      = {https://www.sciencedirect.com/science/article/pii/S0950705117303556},
}

@Article{Pang2017,
  author   = {Pang, Guofei and Perdikaris, Paris and Cai, Wei and Karniadakis, George Em},
  journal  = {Journal of Computational Physics},
  title    = {Discovering variable fractional orders of advection-dispersion equations from field data using multi-fidelity {B}ayesian optimization},
  year     = {2017},
  issn     = {0021-9991},
  pages    = {694--714},
  volume   = {348},
  abstract = {The fractional advection-dispersion equation (FADE) can describe accurately the solute transport in groundwater but its fractional order has to be determined a priori. Here, we employ multi-fidelity Bayesian optimization to obtain the fractional order under various conditions, and we obtain more accurate results compared to previously published data. Moreover, the present method is very efficient as we use different levels of resolution to construct a stochastic surrogate model and quantify its uncertainty. We consider two different problem set ups. In the first set up, we obtain variable fractional orders of one-dimensional FADE, considering both synthetic and field data. In the second set up, we identify constant fractional orders of two-dimensional FADE using synthetic data. We employ multi-resolution simulations using two-level and three-level Gaussian process regression models to construct the surrogates.},
  doi      = {10.1016/j.jcp.2017.07.052},
  keywords = {Fractional modeling, Porous media, Machine learning, Inverse problem, Gaussian process regression, Uncertainty quantification, Model uncertainty},
  url      = {https://www.sciencedirect.com/science/article/pii/S0021999117305600},
}

@Article{Palar2023,
  author   = {Palar, Pramudita Satria and Parussini, Lucia and Bregant, Luigi and Shimoyama, Koji and Zuhal, Lavi Rizki},
  journal  = {Structural and Multidisciplinary Optimization},
  title    = {On kernel functions for bi-fidelity {G}aussian process regressions},
  year     = {2023},
  issn     = {1615-1488},
  number   = {2},
  pages    = {37},
  volume   = {66},
  abstract = {This paper investigates the impact of kernel functions on the accuracy of bi-fidelity Gaussian process regressions (GPR) for engineering applications. The potential of composite kernel learning (CKL) and model selection is also studied, aiming to ease the process of manual kernel selection. Using the autoregressive Gaussian process as the base model, this paper studies four kernel functions and their combinations: Gaussian, Matern-3/2, Matern-5/2, and Cubic. Experiments on four engineering test problems show that the best kernel is problem dependent and sometimes might be counter-intuitive, even when a large amount of low-fidelity data already aids the model. In this regard, using CKL or automatic kernel selection via cross validation and maximum likelihood can reduce the tendency to select a poor-performing kernel. In addition, the CKL technique can create a slightly more accurate model than the best-performing individual kernel. The main drawback of CKL is its significantly expensive computational cost. The results also show that, given a sufficient amount of samples, tuning the regression term is important to improve the accuracy and robustness of bi-fidelity GPR, while decreasing the importance of the proper kernel selection.},
  doi      = {10.1007/s00158-023-03487-y},
  refid    = {Palar2023},
  url      = {https://doi.org/10.1007/s00158-023-03487-y},
}

@Article{Verweij2003,
  author   = {Verweij, Bram and Ahmed, Shabbir and Kleywegt, Anton J. and Nemhauser, George and Shapiro, Alexander},
  journal  = {Computational Optimization and Applications},
  title    = {The Sample Average Approximation Method Applied to Stochastic Routing Problems: A Computational Study},
  year     = {2003},
  issn     = {1573-2894},
  number   = {2},
  pages    = {289--333},
  volume   = {24},
  abstract = {The sample average approximation (SAA) method is an approach for solving stochastic optimization problems by using Monte Carlo simulation. In this technique the expected objective function of the stochastic problem is approximated by a sample average estimate derived from a random sample. The resulting sample average approximating problem is then solved by deterministic optimization techniques. The process is repeated with different samples to obtain candidate solutions along with statistical estimates of their optimality gaps.},
  doi      = {10.1023/A:1021814225969},
  refid    = {Verweij2003},
  url      = {https://doi.org/10.1023/A:1021814225969},
}

@Article{Brochu2010,
  author  = {Brochu, Eric and Cora, Vlad M. and De Freitas, Nando},
  journal = {arXiv preprint arXiv:1012.2599},
  title   = {{A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning}},
  year    = {2010},
  doi     = {10.48550/arXiv.1012.2599},
}

@Article{Scott2010,
  author    = {Scott, Steven L.},
  journal   = {Applied Stochastic Models in Business and Industry},
  title     = {A modern {B}ayesian look at the multi-armed bandit},
  year      = {2010},
  issn      = {1524-1904},
  month     = nov,
  number    = {6},
  pages     = {639--658},
  volume    = {26},
  abstract  = {Abstract A multi-armed bandit is an experiment with the goal of accumulating rewards from a payoff distribution with unknown parameters that are to be learned sequentially. This article describes a heuristic for managing multi-armed bandits called randomized probability matching, which randomly allocates observations to arms according the Bayesian posterior probability that each arm is optimal. Advances in Bayesian computation have made randomized probability matching easy to apply to virtually any payoff distribution. This flexibility frees the experimenter to work with payoff distributions that correspond to certain classical experimental designs that have the potential to outperform methods that are ?optimal? in simpler contexts. I summarize the relationships between randomized probability matching and several related heuristics that have been used in the reinforcement learning literature. Copyright ? 2010 John Wiley & Sons, Ltd.},
  doi       = {10.1002/asmb.874},
  keywords  = {probability matching, exploration vs exploitation, sequential design, Bayesian adaptive design},
  publisher = {John Wiley & Sons, Ltd},
  url       = {https://doi.org/10.1002/asmb.874},
}

@Article{Russo2016,
  author    = {Russo, Daniel J. and Van Roy, Benjamin},
  journal   = {The Journal of Machine Learning Research},
  title     = {An information-theoretic analysis of {T}hompson sampling},
  year      = {2016},
  issn      = {1532-4435},
  number    = {1},
  pages     = {2442--2471},
  volume    = {17},
  publisher = {JMLR. org},
  url       = {https://www.jmlr.org/papers/v17/14-087.html},
}

@InProceedings{Paleyes2019,
  author    = {Paleyes, Andrei and Pullin, Mark and Mahsereci, Maren and McCollum, Cliff and Lawrence, Neil D. and González, Javier},
  booktitle = {Second workshop on machine learning and the physical sciences (NeurIPS 2019)},
  title     = {Emulation of physical processes with {E}mukit},
  year      = {2019},
  address   = {Vancouver, Canada},
  month     = dec,
  eprint    = {https://arxiv.org/abs/2110.13293},
  url       = {https://ml4physicalsciences.github.io/2019/files/NeurIPS_ML4PS_2019_113.pdf},
}

@InProceedings{Daulton2023,
  author    = {Daulton, Sam and Balandat, Maximilian and Bakshy, Eytan},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  title     = {{Hypervolume knowledge gradient: A lookahead approach for multi-objective {B}ayesian optimization with partial information}},
  year      = {2023},
  month     = {23--29 Jul},
  pages     = {7167--7204},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {202},
  abstract  = {Bayesian optimization is a popular method for sample efficient multi-objective optimization. However, existing Bayesian optimization techniques fail to effectively exploit common and often-neglected problem structure such as decoupled evaluations, where objectives can be queried independently from one another and each may consume different resources, or multi-fidelity evaluations, where lower fidelity-proxies of the objectives can be evaluated at lower cost. In this work, we propose a general one-step lookahead acquisition function based on the Knowledge Gradient that addresses the complex question of what to evaluate when and at which design points in a principled Bayesian decision-theoretic fashion. Hence, our approach naturally addresses decoupled, multi-fidelity, and standard multi-objective optimization settings in a unified Bayesian decision making framework. By construction, our method is the one-step Bayes-optimal policy for hypervolume maximization. Empirically, we demonstrate that our method improves sample efficiency in a wide variety of synthetic and real-world problems. Furthermore, we show that our method is general-purpose and yields competitive performance in standard (potentially noisy) multi-objective optimization.},
  pdf       = {https://proceedings.mlr.press/v202/daulton23a/daulton23a.pdf},
  url       = {https://proceedings.mlr.press/v202/daulton23a.html},
}

@Article{Deb2002,
  author  = {Deb, Kalyanmoy and Pratap, Amrit and Agarwal, Sameer and Meyarivan, TAMT},
  journal = {IEEE Transactions on Evolutionary Computation},
  title   = {{A fast and elitist multiobjective genetic algorithm: NSGA-II}},
  year    = {2002},
  issn    = {1089-778X},
  number  = {2},
  pages   = {182--197},
  volume  = {6},
  doi     = {10.1109/4235.996017},
  url     = {https://doi.org/10.1109/4235.996017},
}

@Article{ZhangQ2007,
  author      = {Zhang, Q. and Li, H.},
  journal     = {IEEE Transactions on Evolutionary Computation},
  title       = {{MOEA/D: A multiobjective evolutionary algorithm based on decomposition}},
  year        = {2007},
  issn        = {1941-0026},
  number      = {6},
  pages       = {712--731},
  volume      = {11},
  call-number = {11},
  doi         = {10.1109/TEVC.2007.892759},
}

@InProceedings{Suzuki2020,
  author    = {Suzuki, Shinya and Takeno, Shion and Tamura, Tomoyuki and Shitara, Kazuki and Karasuyama, Masayuki},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  title     = {Multi-objective {B}ayesian Optimization using {P}areto-frontier Entropy},
  year      = {2020},
  month     = {13--18 Jul},
  pages     = {9279--9288},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {119},
  abstract  = {This paper studies an entropy-based multi-objective Bayesian optimization (MBO). Existing entropy-based MBO methods need complicated approximations to evaluate entropy or employ over-simplification that ignores trade-off among objectives. We propose a novel entropy-based MBO called Pareto-frontier entropy search (PFES), which is based on the information gain of Pareto-frontier. We show that our entropy evaluation can be reduced to a closed form whose computation is quite simple while capturing the trade-off relation in Pareto-frontier. We further propose an extension for the “decoupled” setting, in which each objective function can be observed separately, and show that the PFES-based approach derives a natural extension of the original acquisition function which can also be evaluated simply. Our numerical experiments show effectiveness of PFES through several benchmark datasets, and real-word datasets from materials science.},
  pdf       = {http://proceedings.mlr.press/v119/suzuki20a/suzuki20a.pdf},
  url       = {https://proceedings.mlr.press/v119/suzuki20a.html},
}

@InProceedings{Ponweiser2008,
  author    = {Ponweiser, Wolfgang and Wagner, Tobias and Biermann, Dirk and Vincze, Markus},
  booktitle = {Parallel Problem Solving from Nature - PPSN X},
  title     = {Multiobjective Optimization on a Limited Budget of Evaluations Using Model-Assisted $\mathcal{S}$-Metric Selection},
  year      = {2008},
  address   = {Berlin, Heidelberg},
  editor    = {Rudolph, Günter and Jansen, Thomas and Beume, Nicola and Lucas, Simon and Poloni, Carlo},
  pages     = {784--794},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {Real-world optimization problems often require the consideration of multiple contradicting objectives. These multiobjective problems are even more challenging when facing a limited budget of evaluations due to expensive experiments or simulations. In these cases, a specific class of multiobjective optimization algorithms (MOOA) has to be applied. This paper provides a review of contemporary multiobjective approaches based on the singleobjective meta-model-assisted ’Efficient Global Optimization’ (EGO) procedure and describes their main concepts. Additionally, a new EGO-based MOOA is introduced, which utilizes the $\mathcal{S}$-metric or hypervolume contribution to decide which solution is evaluated next. A benchmark on recently proposed test functions is performed allowing a budget of 130 evaluations. The results point out that the maximization of the hypervolume contribution within a real multiobjective optimization is superior to straightforward adaptations of EGO making our new approach capable of approximating the Pareto front of common problems within the allowed budget of evaluations.},
  isbn      = {978-3-540-87700-4},
  refid     = {10.1007/978-3-540-87700-4_78},
}

@Article{Emmerich2006,
  author      = {Emmerich, M. T. M. and Giannakoglou, K. C. and Naujoks, B.},
  journal     = {IEEE Transactions on Evolutionary Computation},
  title       = {Single- and multiobjective evolutionary optimization assisted by {G}aussian random field metamodels},
  year        = {2006},
  issn        = {1941-0026},
  number      = {4},
  pages       = {421--439},
  volume      = {10},
  call-number = {10},
  doi         = {10.1109/TEVC.2005.859463},
}

@Article{Keane2006,
  author    = {Keane, A. J.},
  journal   = {AIAA Journal},
  title     = {Statistical Improvement Criteria for Use in Multiobjective Design Optimization},
  year      = {2006},
  issn      = {0001-1452},
  month     = apr,
  number    = {4},
  pages     = {879--891},
  volume    = {44},
  comment   = {doi: 10.2514/1.16875},
  doi       = {10.2514/1.16875},
  publisher = {American Institute of Aeronautics and Astronautics},
  url       = {https://doi.org/10.2514/1.16875},
}

@Article{Das1997,
  author   = {Das, I. and Dennis, J. E.},
  journal  = {Structural optimization},
  title    = {A closer look at drawbacks of minimizing weighted sums of objectives for {P}areto set generation in multicriteria optimization problems},
  year     = {1997},
  issn     = {1615-1488},
  number   = {1},
  pages    = {63--69},
  volume   = {14},
  abstract = {A standard technique for generating the Pareto set in multicriteria optimization problems is to minimize (convex) weighted sums of the different objectives for various different settings of the weights. However, it is well-known that this method succeeds in getting points from all parts of the Pareto set only when the Pareto curve is convex. This article provides a geometrical argument as to why this is the case.},
  doi      = {10.1007/BF01197559},
  refid    = {Das1997},
  url      = {https://doi.org/10.1007/BF01197559},
}

@Book{Elishakoff2010,
  author    = {Elishakoff, Isaac and Ohsaki, Makoto},
  publisher = {Imperial College Press},
  title     = {{Optimization and anti-optimization of structures under uncertainty}},
  year      = {2010},
  address   = {London},
  pages     = {424},
  url       = {https://www.worldscientific.com/worldscibooks/10.1142/p678},
}

@Article{Geng2019,
  author   = {Geng, Xinbo and Xie, Le},
  journal  = {Annual Reviews in Control},
  title    = {Data-driven decision making in power systems with probabilistic guarantees: Theory and applications of chance-constrained optimization},
  year     = {2019},
  issn     = {1367-5788},
  pages    = {341--363},
  volume   = {47},
  abstract = {Uncertainties from deepening penetration of renewable energy resources have posed critical challenges to the secure and reliable operations of future electric grids. Among various approaches for decision making in uncertain environments, this paper focuses on chance-constrained optimization, which provides explicit probabilistic guarantees on the feasibility of optimal solutions. Although quite a few methods have been proposed to solve chance-constrained optimization problems, there is a lack of comprehensive review and comparative analysis of the proposed methods. We first review three categories of existing methods to chance-constrained optimization: (1) scenario approach; (2) sample average approximation; and (3) robust optimization based methods. Data-driven methods, which are not constrained by any particular distributions of the underlying uncertainties, are of particular interest. Key results of the analytical reformulation approach for specific distributions are briefly discussed. We then provide a comprehensive review on the applications of chance-constrained optimization in power systems. Finally, this paper provides a critical comparison of existing methods based on numerical simulations, which are conducted on standard power system test cases.},
  doi      = {10.1016/j.arcontrol.2019.05.005},
  keywords = {Data-driven, Power system, Chance constraint, Probabilistic constraint, Stochastic programming, Robust optimization, Chance-constrained optimization},
  url      = {https://www.sciencedirect.com/science/article/pii/S1367578819300306},
}

@Article{Aoues2010,
  author   = {Aoues, Younes and Chateauneuf, Alaa},
  journal  = {Structural and Multidisciplinary Optimization},
  title    = {Benchmark study of numerical methods for reliability-based design optimization},
  year     = {2010},
  issn     = {1615-1488},
  number   = {2},
  pages    = {277--294},
  volume   = {41},
  abstract = {The reliability-based design optimization (RBDO) seeks for the best compromise between cost and safety, by considering system uncertainties. In order to overcome computational difficulties, many formulations have been recently developed, leading to confusion about what method should be selected for a given application, due to the lack of full-scale comparative studies. In this context, the present paper aims at giving an overview of various RBDO approaches which are tested on a benchmark constituted of four examples using mathematical and finite element models, with different levels of difficulties. The study is focused on the three main approaches, namely the two-level approach, the single loop approach and the decoupled approach; for each category, two RBDO formulations are discussed, implemented and tested for numerical examples. The benchmark study allows us to give comprehensive overview of various approaches, to give clear ideas about their capabilities and limitations, and to draw useful conclusions regarding robustness and numerical performance.},
  doi      = {10.1007/s00158-009-0412-2},
  refid    = {Aoues2010},
  url      = {https://doi.org/10.1007/s00158-009-0412-2},
}

@Article{Bichon2008,
  author    = {Bichon, B. J. and Eldred, M. S. and Swiler, L. P. and Mahadevan, S. and McFarland, J. M.},
  journal   = {AIAA Journal},
  title     = {Efficient Global Reliability Analysis for Nonlinear Implicit Performance Functions},
  year      = {2008},
  issn      = {0001-1452},
  month     = oct,
  number    = {10},
  pages     = {2459--2468},
  volume    = {46},
  comment   = {doi: 10.2514/1.34321},
  doi       = {10.2514/1.34321},
  publisher = {American Institute of Aeronautics and Astronautics},
  url       = {https://doi.org/10.2514/1.34321},
}

@Article{Calafiore2006,
  author      = {Calafiore, G. C. and Campi, M. C.},
  journal     = {IEEE Transactions on Automatic Control},
  title       = {The scenario approach to robust control design},
  year        = {2006},
  issn        = {1558-2523},
  number      = {5},
  pages       = {742--753},
  volume      = {51},
  call-number = {51},
  doi         = {10.1109/TAC.2006.875041},
}

@Article{Nemirovski2012,
  author   = {Nemirovski, Arkadi},
  journal  = {European Journal of Operational Research},
  title    = {On safe tractable approximations of chance constraints},
  year     = {2012},
  issn     = {0377-2217},
  number   = {3},
  pages    = {707--718},
  volume   = {219},
  abstract = {A natural way to handle optimization problem with data affected by stochastic uncertainty is to pass to a chance constrained version of the problem, where candidate solutions should satisfy the randomly perturbed constraints with probability at least 1−∊. While being attractive from modeling viewpoint, chance constrained problems “as they are” are, in general, computationally intractable. In this survey paper, we overview several simulation-based and simulation-free computationally tractable approximations of chance constrained convex programs, primarily, those of chance constrained linear, conic quadratic and semidefinite programming.},
  doi      = {10.1016/j.ejor.2011.11.006},
  keywords = {Uncertainty modeling, Convex programming, Optimization under uncertainty, Chance constraints, Robust Optimization},
  url      = {https://www.sciencedirect.com/science/article/pii/S0377221711009933},
}

@Article{Huynh2023,
  author   = {Huynh, Van Thu and Tangaramvong, Sawekchai and Do, Bach and Gao, Wei and Limkatanyu, Suchart},
  journal  = {Reliability Engineering {\&} System Safety},
  title    = {Sequential most probable point update combining {G}aussian process and comprehensive learning {PSO} for structural reliability-based design optimization},
  year     = {2023},
  issn     = {0951-8320},
  pages    = {109164},
  volume   = {235},
  abstract = {This paper proposes an efficient reliability-based design optimization (RBDO) method that advantageously decouples comprehensive learning particle swarm optimization (CLPSO) algorithm with Gaussian process regression (GPR) model, termed as GPR-CLPSO. The method iteratively performs the CLPSO with deterministic parameters based on the most probable point (MPP) underpinning limit-state functions (LSFs) iteratively updated by the active learning reliability evaluation process. The GPR model approximates, from the design data given by CLPSO, the spectrum of LSFs under random parameters, and hence enables a significant computational reduction of Monte-Carlo simulations (MCSs) for failure probability approximation. The expected feasibility function is maximized using the CLPSO code to systematically refine the GPR model by adaptively adding new (intelligent) learning points in the region with high-reliability sensitivity leading to the more accurate prediction of failure probability. A novel inverse MCS constraint boundary method is developed to redefine the MPP assigned for the CLPSO algorithm in determining the new optimal design. The method efficiently leverages the decoupling approach, whilst significantly alleviating computing efforts, to quickly and accurately capture the optimal RBDO design. The resulting failure probability well satisfies the allowable limit. Four RBDO examples are provided to illustrate applications and robustness of the proposed decoupling GPR-CLPSO approach.},
  doi      = {10.1016/j.ress.2023.109164},
  keywords = {Reliability-based design optimization, Most probable point, Gaussian process regression, Comprehensive learning particle swarm optimization, Expected feasibility function, Inverse MCS constraint boundary},
  url      = {https://www.sciencedirect.com/science/article/pii/S0951832023000790},
}

@Book{Melchers2018,
  author    = {Melchers, Robert E and Beck, Andr{\'{e}} T},
  publisher = {John Wiley {\&} Sons},
  title     = {{Structural reliability analysis and prediction}},
  year      = {2018},
  address   = {New York},
  edition   = {3rd},
  url       = {https://www.wiley.com/en-br/Structural+Reliability+Analysis+and+Prediction,+3rd+Edition-p-9781119265993},
}

@Article{Xie2018,
  author    = {Xie, Weijun and Ahmed, Shabbir},
  journal   = {SIAM Journal on Optimization},
  title     = {On Deterministic Reformulations of Distributionally Robust Joint Chance Constrained Optimization Problems},
  year      = {2018},
  issn      = {1052-6234},
  month     = jan,
  number    = {2},
  pages     = {1151--1182},
  volume    = {28},
  abstract  = {A joint chance constrained optimization problem involves multiple uncertain constraints, i.e., constraints with stochastic parameters, that are jointly required to be satisfied with probability exceeding a prespecified threshold. In a distributionally robust joint chance constrained optimization problem (DRCCP), the joint chance constraint is required to hold for all probability distributions of the stochastic parameters from a given ambiguity set. In this work, we consider DRCCPs involving convex nonlinear uncertain constraints and an ambiguity set specified by convex moment constraints. We investigate deterministic reformulations of such problems and conditions under which such deterministic reformulations are convex. In particular we show that a DRCCP can be reformulated as a convex program if one the following conditions hold: (i) there is a single uncertain constraint, (ii) the ambiguity set is defined by a single moment constraint, (iii) the ambiguity set is defined by linear moment constraints, and (iv) the uncertain and moment constraints are positively homogeneous with respect to uncertain parameters. We further show that if the decision variables are binary and the uncertain constraints are linear then a DRCCP can be reformulated as a deterministic mixed integer convex program. Finally, we present a numerical study to illustrate that the proposed mixed integer convex reformulation can be solved efficiently by existing solvers.},
  comment   = {doi: 10.1137/16M1094725},
  doi       = {10.1137/16M1094725},
  publisher = {Society for Industrial and Applied Mathematics},
  url       = {https://doi.org/10.1137/16M1094725},
}

@Article{Chaudhuri2020,
  author   = {Chaudhuri, Anirban and Kramer, Boris and Willcox, Karen E.},
  journal  = {Reliability Engineering {\&} System Safety},
  title    = {Information Reuse for Importance Sampling in Reliability-Based Design Optimization},
  year     = {2020},
  issn     = {0951-8320},
  pages    = {106853},
  volume   = {201},
  abstract = {This paper introduces a new approach for importance-sampling-based reliability-based design optimization (RBDO) that reuses information from past optimization iterations to reduce computational effort. RBDO is a two-loop process--an uncertainty quantification loop embedded within an optimization loop--that can be computationally prohibitive due to the numerous evaluations of expensive high-fidelity models to estimate the probability of failure in each optimization iteration. In this work, we use the existing information from past optimization iterations to create efficient biasing densities for importance sampling estimates of probability of failure. The method involves two levels of information reuse: (1) reusing the current batch of samples to construct an a posteriori biasing density with optimal parameters, and (2) reusing the a posteriori biasing densities of the designs visited in past optimization iterations to construct the biasing density for the current design. We demonstrate for the RBDO of a benchmark speed reducer problem and a combustion engine problem that the proposed method leads to computational savings in the range of 51% to 76%, compared to building biasing densities with no reuse in each iteration.},
  doi      = {10.1016/j.ress.2020.106853},
  keywords = {Information reuse, Importance sampling, Biasing density, Probability of failure, Reliability analysis, Optimization under uncertainty, Reliability-based optimization, RBDO},
  url      = {https://www.sciencedirect.com/science/article/pii/S0951832019301620},
}

@InProceedings{Titsias2009,
  author    = {Titsias, Michalis},
  booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
  title     = {Variational Learning of Inducing Variables in Sparse Gaussian Processes},
  year      = {2009},
  month     = apr,
  pages     = {567--574},
  publisher = {PMLR},
  volume    = {5},
  abstract  = {Sparse Gaussian process methods that use inducing variables require the selection of the inducing inputs and the kernel hyperparameters. We introduce a variational formulation for sparse approximations that jointly infers the inducing inputs and the kernel hyperparameters by maximizing a lower bound of the true log marginal likelihood. The key property of this formulation is that the inducing inputs  are defined to be variational parameters  which are selected by minimizing  the Kullback-Leibler divergence between  the variational distribution and the exact posterior distribution over the latent function values. We apply this technique to regression and we compare it with other approaches in the literature.},
  refid     = {pmlr-v5-titsias09a},
  url       = {https://proceedings.mlr.press/v5/titsias09a.html},
}

@Article{Conn2009,
  author    = {Conn, Andrew R. and Scheinberg, Katya and Vicente, Luís N.},
  journal   = {SIAM Journal on Optimization},
  title     = {Global Convergence of General Derivative-Free Trust-Region Algorithms to First- and Second-Order Critical Points},
  year      = {2009},
  issn      = {1052-6234},
  month     = jan,
  number    = {1},
  pages     = {387--415},
  volume    = {20},
  abstract  = {In this paper we prove global convergence for first- and second-order stationary points of a class of derivative-free trust-region methods for unconstrained optimization. These methods are based on the sequential minimization of quadratic (or linear) models built from evaluating the objective function at sample sets. The derivative-free models are required to satisfy Taylor-type bounds, but, apart from that, the analysis is independent of the sampling techniques. A number of new issues are addressed, including global convergence when acceptance of iterates is based on simple decrease of the objective function, trust-region radius maintenance at the criticality step, and global convergence for second-order critical points.},
  comment   = {doi: 10.1137/060673424},
  doi       = {10.1137/060673424},
  publisher = {Society for Industrial and Applied Mathematics},
  url       = {https://doi.org/10.1137/060673424},
}

@InProceedings{Wilson2020,
  author    = {Wilson, James T. and Borovitskiy, Viacheslav and Terenin, Alexander and Mostowsky, Peter and Deisenroth, Marc Peter},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  title     = {Efficiently Sampling Functions from {G}aussian Process Posteriors},
  year      = {2020},
  pages     = {10292--10302},
  publisher = {PMLR},
  abstract  = {Gaussian processes are the gold standard for many real-world modeling problems, especially in cases where a model's success hinges upon its ability to faithfully represent predictive uncertainty. These problems typically exist as parts of larger frameworks, wherein quantities of interest are ultimately defined by integrating over posterior distributions. These quantities are frequently intractable, motivating the use of Monte Carlo methods. Despite substantial progress in scaling up Gaussian processes to large training sets, methods for accurately generating draws from their posterior distributions still scale cubically in the number of test locations. We identify a decomposition of Gaussian processes that naturally lends itself to scalable sampling by separating out the prior from the data. Building off of this factorization, we propose an easy-to-use and general-purpose approach for fast posterior sampling, which seamlessly pairs with sparse approximations to afford scalability both during training and at test time. In a series of experiments designed to test competing sampling schemes' statistical properties and practical ramifications, we demonstrate how decoupled sample paths accurately represent Gaussian process posteriors at a fraction of the usual cost.},
  articleno = {953},
  numpages  = {11},
  url       = {https://proceedings.mlr.press/v119/wilson20a.html},
}

@Article{Lindauer2022,
  author     = {Lindauer, Marius and Eggensperger, Katharina and Feurer, Matthias and Biedenkapp, Andr\'{e} and Deng, Difan and Benjamins, Carolin and Ruhkopf, Tim and Sass, Ren\'{e} and Hutter, Frank},
  journal    = {Journal of Machine Learning Research},
  title      = {{SMAC3: A Versatile Bayesian Optimization Package for Hyperparameter Optimization}},
  year       = {2022},
  issn       = {1532-4435},
  month      = {jan},
  number     = {1},
  volume     = {23},
  abstract   = {Algorithm parameters, in particular hyperparameters of machine learning algorithms, can substantially impact their performance. To support users in determining well-performing hyperparameter configurations for their algorithms, datasets and applications at hand, SMAC3 offers a robust and flexible framework for Bayesian Optimization, which can improve performance within a few evaluations. It offers several facades and pre-sets for typical use cases, such as optimizing hyperparameters, solving low dimensional continuous (artificial) global optimization problems and configuring algorithms to perform well across multiple problem instances. The SMAC3 package is available under a permissive BSD-license.},
  articleno  = {54},
  groups     = {Bayesian optimization},
  issue_date = {January 2022},
  keywords   = {Bayesian optimization, hyperparameter optimization, multi-fidelity optimization, automated machine learning, algorithm configuration},
  numpages   = {9},
  publisher  = {JMLR},
  url        = {https://www.jmlr.org/papers/v23/21-0888.html},
}

@InProceedings{Balandat2020,
  author    = {Balandat, Maximilian and Karrer, Brian and Jiang, Daniel and Daulton, Samuel and Letham, Ben and Wilson, Andrew G and Bakshy, Eytan},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {{BoTorch: A Framework for Efficient Monte-Carlo Bayesian Optimization}},
  year      = {2020},
  pages     = {21524--21538},
  groups    = {Bayesian optimization},
  publisher = {NeurIPS 2020},
  volume    = {33},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2020/file/f5b1b89d98b7286673128a5fb112cb9a-Paper.pdf},
}

@Article{Blanchard2021siamjuq,
  author    = {Blanchard, Antoine and Sapsis, Themistoklis},
  journal   = {SIAM/ASA Journal on Uncertainty Quantification},
  title     = {Output-Weighted Optimal Sampling for {B}ayesian Experimental Design and Uncertainty Quantification},
  year      = {2021},
  month     = jan,
  number    = {2},
  pages     = {564--592},
  volume    = {9},
  abstract  = {We introduce a class of acquisition functions for sample selection that lead to faster convergence in applications related to Bayesian experimental design and uncertainty quantification. The approach follows the paradigm of active learning, whereby existing samples of a black-box function are utilized to optimize the next most informative sample. The proposed method aims to take advantage of the fact that some input directions of the black-box function have a larger impact on the output than others, which is important especially for systems exhibiting rare and extreme events. The acquisition functions introduced in this work leverage the properties of the likelihood ratio, a quantity that acts as a probabilistic sampling weight and guides the active-learning algorithm toward regions of the input space that are deemed most relevant. We demonstrate the proposed approach in the uncertainty quantification of a hydrological system as well as the probabilistic quantification of rare events in dynamical systems and the identification of their precursors in up to 30 dimensions.},
  comment   = {doi: 10.1137/20M1347486},
  doi       = {10.1137/20M1347486},
  groups    = {Bayesian optimization},
  publisher = {Society for Industrial and Applied Mathematics},
  url       = {https://doi.org/10.1137/20M1347486},
}

@Book{Martins2021,
  author    = {Martins, Joaquim R. R. A. and Ning, Andrew},
  publisher = {Cambridge University Press},
  title     = {Engineering Design Optimization},
  year      = {2021},
  address   = {Cambridge},
  isbn      = {9781108833417},
  abstract  = {Based on course-tested material, this rigorous yet accessible graduate textbook covers both fundamental and advanced optimization theory and algorithms. It covers a wide range of numerical methods and topics, including both gradient-based and gradient-free algorithms, multidisciplinary design optimization, and uncertainty, with instruction on how to determine which algorithm should be used for a given application. It also provides an overview of models and how to prepare them for use with numerical optimization, including derivative computation. Over 400 high-quality visualizations and numerous examples facilitate understanding of the theory, and practical tips address common issues encountered in practical engineering design optimization and how to address them. Numerous end-of-chapter homework problems, progressing in difficulty, help put knowledge into practice. Accompanied online by a solutions manual for instructors and source code for problems, this is ideal for a one- or two-semester graduate course on optimization in aerospace, civil, mechanical, electrical, and chemical engineering departments.},
  database  = {Cambridge Core},
  doi       = {10.1017/9781108980647},
  url       = {https://www.cambridge.org/core/books/engineering-design-optimization/B1B23D00AF79E45502C4649A0E43135B},
}

@Article{Neumaier2004,
  author    = {Neumaier, Arnold},
  journal   = {Acta Numerica},
  title     = {Complete search in continuous global optimization and constraint satisfaction},
  year      = {2004},
  issn      = {0962-4929},
  pages     = {271--369},
  volume    = {13},
  abstract  = {This survey covers the state of the art of techniques for solving general-purpose constrained global optimization problems and continuous constraint satisfaction problems, with emphasis on complete techniques that provably find all solutions (if there are finitely many). The core of the material is presented in sufficient detail that the survey may serve as a text for teaching constrained global optimization.After giving motivations for and important examples of applications of global optimization, a precise problem definition is given, and a general form of the traditional first-order necessary conditions for a solution. Then more than a dozen software packages for complete global search are described.A quick review of incomplete methods for bound-constrained problems and recipes for their use in the constrained case follows; an explicit example is discussed, introducing the main techniques used within branch and bound techniques. Sections on interval arithmetic, constrained propagation and local optimization are followed by a discussion of how to avoid the cluster problem. Then a discussion of important problem transformations follows, in particular of linear, convex, and semilinear (= mixed integer linear) relaxations that are important for handling larger problems.Next, reliability issues - centring on rounding error handling and testing methodologies - are discussed, and the COCONUT framework for the integration of the different techniques is introduced. A list of challenges facing the field in the near future concludes the survey.},
  database  = {Cambridge Core},
  doi       = {10.1017/S0962492904000194},
  publisher = {Cambridge University Press},
  url       = {https://www.cambridge.org/core/article/complete-search-in-continuous-global-optimization-and-constraint-satisfaction/DFCFABCCB8440B34AD3F4F1B4F3FB2A2},
}

@Article{Wild2011,
  author       = {Wild, Stefan M. and Shoemaker, Christine},
  journal      = {SIAM Journal on Optimization},
  title        = {Global Convergence of Radial Basis Function Trust Region Derivative-Free Algorithms},
  year         = {2011},
  issn         = {1052-6234},
  month        = jul,
  number       = {3},
  pages        = {761--781},
  volume       = {21},
  abstract     = {We analyze globally convergent derivative-free trust region algorithms relying on radial basis function interpolation models. Our results extend the recent work of Conn, Scheinberg, and Vicente [SIAM J. Optim., 20 (2009), pp. 387?415] to fully linear models that have a nonlinear term. We characterize the types of radial basis functions that fit in our analysis and thus show global convergence to first-order critical points for the ORBIT algorithm of Wild, Regis, and Shoemaker [SIAM J. Sci. Comput., 30 (2008), pp. 3197?3219]. Using ORBIT, we present numerical results for different types of radial basis functions on a series of test problems. We also demonstrate the use of ORBIT in finding local minima on a computationally expensive environmental engineering problem.},
  comment-ruda = {Trust region w/ radial basis functions (RBF).},
  doi          = {10.1137/09074927X},
  file         = {:Wild2011.pdf:PDF},
  publisher    = {Society for Industrial and Applied Mathematics},
  url          = {https://doi.org/10.1137/09074927X},
}

@Article{Booker1999,
  author       = {Booker, A. J. and Dennis, J. E. and Frank, P. D. and Serafini, D. B. and Torczon, V. and Trosset, M. W.},
  journal      = {Structural optimization},
  title        = {A rigorous framework for optimization of expensive functions by surrogates},
  year         = {1999},
  issn         = {1615-1488},
  number       = {1},
  pages        = {1--13},
  volume       = {17},
  abstract     = {The goal of the research reported here is to develop rigorous optimization algorithms to apply to some engineering design problems for which direct application of traditional optimization approaches is not practical. This paper presents and analyzes a framework for generating a sequence of approximations to the objective function and managing the use of these approximations as surrogates for optimization. The result is to obtain convergence to a minimizer of an expensive objective function subject to simple constraints. The approach is widely applicable because it does not require, or even explicitly approximate, derivatives of the objective. Numerical results are presented for a 31-variable helicopter rotor blade design example and for a standard optimization test example.},
  comment-ruda = {A multi-fidelity pattern search algorithm, with application in structure design. (MFO, derivative-free but not BO)},
  doi          = {10.1007/BF01197708},
  refid        = {Booker1999},
  url          = {https://doi.org/10.1007/BF01197708},
}

@Article{Kennedy2000,
  author    = {Kennedy, M. C. and O'Hagan, A.},
  journal   = {Biometrika},
  title     = {Predicting the Output from a Complex Computer Code When Fast Approximations Are Available},
  year      = {2000},
  number    = {1},
  pages     = {1--13},
  volume    = {87},
  abstract  = {[We consider prediction and uncertainty analysis for complex computer codes which can be run at different levels of sophistication. In particular, we wish to improve efficiency by combining expensive runs of the most complex versions of the code with relatively cheap runs from one or more simpler approximations. A Bayesian approach is described in which prior beliefs about the codes are represented in terms of Gaussian processes. An example is presented using two versions of an oil reservoir simulator.]},
  doi       = {10.1093/biomet/87.1.1},
  file      = {:Kennedy2000.pdf:PDF},
  groups    = {MF modeling, Additive, Core refs},
  publisher = {[Oxford University Press, Biometrika Trust]},
  url       = {http://www.jstor.org/stable/2673557},
}

@Article{Gratiet2014,
  author  = {Gratiet, Loic Le and Garnier, Josselin},
  journal = {International Journal for Uncertainty Quantification},
  title   = {Recursive cokriging model for design ofcomputer experiments with multiple levels of fidelity},
  year    = {2014},
  issn    = {2152-5080},
  number  = {5},
  pages   = {365--386},
  volume  = {4},
  doi     = {10.1615/Int.J.UncertaintyQuantification.2014006914},
  file    = {:Gratiet2014.pdf:PDF},
  groups  = {MF modeling, Hybrid, Core refs},
}

@Article{Forrester2007,
  author    = {Forrester, Alexander I. J. and Sóbester, András and Keane, Andy J.},
  journal   = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  title     = {Multi-fidelity optimization via surrogate modelling},
  year      = {2007},
  month     = oct,
  number    = {2088},
  pages     = {3251--3269},
  volume    = {463},
  comment   = {doi: 10.1098/rspa.2007.1900},
  doi       = {10.1098/rspa.2007.1900},
  file      = {:Forrester2007.pdf:PDF},
  groups    = {Opti Algorithms, MF modeling, Additive, Core refs},
  publisher = {Royal Society},
  url       = {https://doi.org/10.1098/rspa.2007.1900},
}

@Article{Qian2008,
  author    = {Qian, Peter Z. G. and Wu, C. F. Jeff},
  journal   = {Technometrics},
  title     = {Bayesian Hierarchical Modeling for Integrating Low-Accuracy and High-Accuracy Experiments},
  year      = {2008},
  issn      = {0040-1706},
  month     = may,
  number    = {2},
  pages     = {192--204},
  volume    = {50},
  comment   = {doi: 10.1198/004017008000000082},
  doi       = {10.1198/004017008000000082},
  groups    = {MF modeling, Hybrid, Core refs},
  publisher = {Taylor \& Francis},
  url       = {https://doi.org/10.1198/004017008000000082},
}

@Article{Han2012,
  author    = {Han, Zhong-Hua and Görtz, Stefan},
  journal   = {AIAA Journal},
  title     = {Hierarchical Kriging Model for Variable-Fidelity Surrogate Modeling},
  year      = {2012},
  issn      = {0001-1452},
  month     = sep,
  number    = {9},
  pages     = {1885--1896},
  volume    = {50},
  comment   = {doi: 10.2514/1.J051354},
  doi       = {10.2514/1.J051354},
  groups    = {Additive, Core refs},
  publisher = {American Institute of Aeronautics and Astronautics},
  url       = {https://doi.org/10.2514/1.J051354},
}

@Article{Ji2023,
  author    = {Ji, Yi and Mak, Simon and Soeder, Derek and Paquet, J. F. and Bass, Steffen A.},
  journal   = {Technometrics},
  title     = {A graphical multi-fidelity Gaussian process model, with application to emulation of heavy-ion collisions},
  year      = {2023},
  number    = {ja},
  pages     = {1--25},
  volume    = {0},
  doi       = {10.1080/00401706.2023.2281940},
  eprint    = {https://doi.org/10.1080/00401706.2023.2281940},
  groups    = {MF modeling, Hybrid, Core refs},
  publisher = {Taylor & Francis},
  url       = {https://doi.org/10.1080/00401706.2023.2281940},
}

@Article{Gano2005,
  author    = {Gano, Shawn E. and Renaud, John E. and Sanders, Brian},
  journal   = {AIAA Journal},
  title     = {Hybrid Variable Fidelity Optimization by Using a Kriging-Based Scaling Function},
  year      = {2005},
  issn      = {0001-1452},
  month     = nov,
  number    = {11},
  pages     = {2422--2433},
  volume    = {43},
  comment   = {doi: 10.2514/1.12466},
  doi       = {10.2514/1.12466},
  groups    = {Hybrid, Core refs},
  publisher = {American Institute of Aeronautics and Astronautics},
  url       = {https://doi.org/10.2514/1.12466},
}

@Article{Han2013,
  author   = {Han, Zhong-Hua and Görtz, Stefan and Zimmermann, Ralf},
  journal  = {Aerospace Science and Technology},
  title    = {Improving variable-fidelity surrogate modeling via gradient-enhanced kriging and a generalized hybrid bridge function},
  year     = {2013},
  issn     = {1270-9638},
  number   = {1},
  pages    = {177--189},
  volume   = {25},
  abstract = {Variable-fidelity surrogate modeling offers an efficient way to generate aerodynamic data for aero-loads prediction based on a set of CFD methods with varying degree of fidelity and computational expense. In this paper, direct Gradient-Enhanced Kriging (GEK) and a newly developed Generalized Hybrid Bridge Function (GHBF) have been combined in order to improve the efficiency and accuracy of the existing Variable-Fidelity Modeling (VFM) approach. The new algorithms and features are demonstrated and evaluated for analytical functions and are subsequently used to construct a global surrogate model for the aerodynamic coefficients and drag polar of an RAE 2822 airfoil. It is shown that the gradient-enhanced GHBF proposed in this paper is very promising and can be used to significantly improve the efficiency, accuracy and robustness of VFM in the context of aero-loads prediction.},
  doi      = {10.1016/j.ast.2012.01.006},
  groups   = {Hybrid},
  keywords = {Surrogate model, Variable-fidelity model, Kriging model, Computational fluid dynamics},
  url      = {https://www.sciencedirect.com/science/article/pii/S127096381200017X},
}

@Article{Wang2006,
  author   = {Wang, G. Gary and Shan, S.},
  journal  = {Journal of Mechanical Design},
  title    = {Review of Metamodeling Techniques in Support of Engineering Design Optimization},
  year     = {2006},
  issn     = {1050-0472},
  month    = may,
  number   = {4},
  pages    = {370--380},
  volume   = {129},
  abstract = {Computation-intensive design problems are becoming increasingly common in manufacturing industries. The computation burden is often caused by expensive analysis and simulation processes in order to reach a comparable level of accuracy as physical testing data. To address such a challenge, approximation or metamodeling techniques are often used. Metamodeling techniques have been developed from many different disciplines including statistics, mathematics, computer science, and various engineering disciplines. These metamodels are initially developed as “surrogates” of the expensive simulation process in order to improve the overall computation efficiency. They are then found to be a valuable tool to support a wide scope of activities in modern engineering design, especially design optimization. This work reviews the state-of-the-art metamodel-based techniques from a practitioner’s perspective according to the role of metamodeling in supporting design optimization, including model approximation, design space exploration, problem formulation, and solving various types of optimization problems. Challenges and future development of metamodeling in support of engineering design is also analyzed and discussed.},
  doi      = {10.1115/1.2429697},
  groups   = {Surveys, Examples of computational cost, Core refs},
  url      = {https://doi.org/10.1115/1.2429697},
}

@Book{Forrester2008,
  author    = {Forrester, Alexander I. J. and András Sóbester and Keane, Andy},
  publisher = {John Wiley {\&} Sons},
  title     = {Engineering design via surrogate modelling: a practical guide},
  year      = {2008},
  address   = {West Sussex, UK},
  isbn      = {0470770791},
  file      = {:Forrester2008.pdf:PDF},
  groups    = {Surveys},
}

@Article{Forrester2009,
  author   = {Forrester, Alexander I. J. and Keane, Andy J.},
  journal  = {Progress in Aerospace Sciences},
  title    = {Recent advances in surrogate-based optimization},
  year     = {2009},
  issn     = {0376-0421},
  number   = {1},
  pages    = {50--79},
  volume   = {45},
  abstract = {The evaluation of aerospace designs is synonymous with the use of long running and computationally intensive simulations. This fuels the desire to harness the efficiency of surrogate-based methods in aerospace design optimization. Recent advances in surrogate-based design methodology bring the promise of efficient global optimization closer to reality. We review the present state of the art of constructing surrogate models and their use in optimization strategies. We make extensive use of pictorial examples and, since no method is truly universal, give guidance as to each method's strengths and weaknesses.},
  doi      = {10.1016/j.paerosci.2008.11.001},
  groups   = {Surveys, Core refs},
  url      = {https://www.sciencedirect.com/science/article/pii/S0376042108000766},
}

@Article{Viana2014,
  author    = {Viana, Felipe A. C. and Simpson, Timothy W. and Balabanov, Vladimir and Toropov, Vasilli},
  journal   = {AIAA Journal},
  title     = {{Special Section on Multidisciplinary Design Optimization: Metamodeling in Multidisciplinary Design Optimization: How Far Have We Really Come?}},
  year      = {2014},
  issn      = {0001-1452},
  month     = jan,
  number    = {4},
  pages     = {670--690},
  volume    = {52},
  abstract  = {The use of metamodeling techniques in the design and analysis of computer experiments has progressed remarkably in the past 25 years, but how far has the field really come? This is the question addressed in this paper, namely, the extent to which the use of metamodeling techniques in multidisciplinary design optimization have evolved in the 25 years since the seminal paper on design and analysis of computer experiments by Sacks et al. (?Design and Analysis of Computer Experiments,? Statistical Science, Vol. 4, No. 4, 1989, pp. 409?435). Rather than a technical review of the entire body of metamodeling literature, the focus is on the evolution and motivation for advancements in metamodeling with some discussion on the research itself; not surprisingly, much of the current research motivation is the same as it was in the past. Based on current research thrusts in the field, multifidelity approximations and ensembles (i.e., sets) of metamodels, as well as the availability of metamodels within commercial software, are emphasized. Design space exploration and visualization via metamodels are also presented as they rely heavily on metamodels for rapid design evaluations during exploration. The closing remarks offer insight into future research directions, mostly motivated by the need for new capabilities and the ability to handle more complex simulations.},
  comment   = {doi: 10.2514/1.J052375},
  doi       = {10.2514/1.J052375},
  groups    = {Surveys, Core refs},
  publisher = {American Institute of Aeronautics and Astronautics},
  url       = {https://doi.org/10.2514/1.J052375},
}

@Article{FernandezGodino2016,
  author  = {Fernández-Godino, M. Giselle and Park, Chanyoung and Kim, Nam-Ho and Haftka, Raphael T.},
  journal = {arXiv preprint arXiv.1609.07196},
  title   = {Review of multi-fidelity models},
  year    = {2016},
  groups  = {Surveys, Core refs},
  url     = {https://arxiv.org/abs/1609.07196},
}

@Article{Peherstorfer2018,
  author    = {Peherstorfer, Benjamin and Willcox, Karen and Gunzburger, Max},
  journal   = {SIAM Review},
  title     = {Survey of Multifidelity Methods in Uncertainty Propagation, Inference, and Optimization},
  year      = {2018},
  issn      = {0036-1445},
  month     = jan,
  number    = {3},
  pages     = {550--591},
  volume    = {60},
  abstract  = {In many situations across computational science and engineering, multiple computational models are available that describe a system of interest. These different models have varying evaluation costs and varying fidelities. Typically, a computationally expensive high-fidelity model describes the system with the accuracy required by the current application at hand, while lower-fidelity models are less accurate but computationally cheaper than the high-fidelity model. Outer-loop applications, such as optimization, inference, and uncertainty quantification, require multiple model evaluations at many different inputs, which often leads to computational demands that exceed available resources if only the high-fidelity model is used. This work surveys multifidelity methods that accelerate the solution of outer-loop applications by combining high-fidelity and low-fidelity model evaluations, where the low-fidelity evaluations arise from an explicit low-fidelity model (e.g., a simplified physics approximation, a reduced model, a data-fit surrogate) that approximates the same output quantity as the high-fidelity model. The overall premise of these multifidelity methods is that low-fidelity models are leveraged for speedup while the high-fidelity model is kept in the loop to establish accuracy and/or convergence guarantees. We categorize multifidelity methods according to three classes of strategies: adaptation, fusion, and filtering. The paper reviews multifidelity methods in the outer-loop contexts of uncertainty propagation, inference, and optimization.},
  comment   = {doi: 10.1137/16M1082469},
  doi       = {10.1137/16M1082469},
  file      = {:Peherstorfer2018.pdf:PDF},
  groups    = {Surveys, Core refs},
  publisher = {Society for Industrial and Applied Mathematics},
  url       = {https://doi.org/10.1137/16M1082469},
}

@Article{Brevault2020,
  author   = {Brevault, Loïc and Balesdent, Mathieu and Hebbal, Ali},
  journal  = {Aerospace Science and Technology},
  title    = {{Overview of Gaussian process based multi-fidelity techniques with variable relationship between fidelities, application to aerospace systems}},
  year     = {2020},
  issn     = {1270-9638},
  pages    = {106339},
  volume   = {107},
  abstract = {The design process of complex systems such as new configurations of aircraft or launch vehicles is usually decomposed in different phases which are characterized by the depth of the analyses in terms of number of design variables and fidelity of the physical models. At each phase, the designers have to deal with accurate but computationally intensive models as well as cheap but inaccurate models. Multi-fidelity modeling is a way to merge different fidelity models to provide engineers with accurate results with a limited computational cost. Within the context of multi-fidelity modeling, approaches based on Gaussian Processes emerge as popular techniques to fuse information between the different fidelity models. The relationship between the fidelity models is a key aspect in multi-fidelity modeling. This paper provides an overview of Gaussian process-based multi-fidelity modeling techniques for variable relationship between the fidelity models (e.g., linearity, non-linearity, variable correlation). Each technique is described within a unified framework and the links between the different techniques are highlighted. All approaches are numerically compared on a series of analytical test cases and four aerospace related engineering problems in order to assess their benefits and disadvantages with respect to the problem characteristics.},
  doi      = {10.1016/j.ast.2020.106339},
  groups   = {Surveys, Core refs},
  keywords = {Multi-fidelity, Gaussian process, Aerospace system analysis},
  url      = {https://www.sciencedirect.com/science/article/pii/S127096382031021X},
}

@InProceedings{Cutajar2019,
  author    = {Cutajar, Kurt and Pullin, Mark and Damianou, Andreas and Lawrence, Neil and González, Javier},
  booktitle = {Third workshop on Bayesian Deep Learning (NeurIPS 2018)},
  title     = {Deep {G}aussian processes for multi-fidelity modeling},
  year      = {2019},
  address   = {Montréal, Canada},
  eprint    = {https://arxiv.org/abs/1903.07320},
  groups    = {Output-output space mapping, Core refs},
  url       = {http://bayesiandeeplearning.org/2018/papers/26.pdf},
}

@Book{Nocedal2006,
  author    = {Nocedal, Jorge and Wright, Stephen J.},
  publisher = {Springer},
  title     = {Numerical optimization},
  year      = {2006},
  address   = {New York, USA},
  edition   = {2nd},
  doi       = {10.1007/978-0-387-40065-5},
  file      = {:Nocedal2006.pdf:PDF},
  groups    = {Books, Core refs},
}

@Book{Kochenderfer2019,
  author    = {Kochenderfer, Mykel J. and Wheeler, Tim A.},
  publisher = {MIT Press},
  title     = {Algorithms for optimization},
  year      = {2019},
  address   = {Massachusetts, USA},
  isbn      = {0262039427},
  file      = {:Kochenderfer2019.pdf:PDF},
  groups    = {Books, Core refs},
}

@Article{Alexandrov1998,
  author   = {Alexandrov, N. M. and Dennis, J. E. and Lewis, R. M. and Torczon, V.},
  journal  = {Structural optimization},
  title    = {A trust-region framework for managing the use of approximation models in optimization},
  year     = {1998},
  issn     = {1615-1488},
  number   = {1},
  pages    = {16--23},
  volume   = {15},
  abstract = {This paper presents an analytically robust, globally convergent approach to managing the use of approximation models of varying fidelity in optimization. By robust global behaviour we mean the mathematical assurance that the iterates produced by the optimization algorithm, started at an arbitrary initial iterate, will converge to a stationary point or local optimizer for the original problem. The approach presented is based on the trust region idea from nonlinear programming and is shown to be provably convergent to a solution of the original high-fidelity problem. The proposed method for managing approximations in engineering optimization suggests ways to decide when the fidelity, and thus the cost, of the approximations might be fruitfully increased or decreased in the course of the optimization iterations. The approach is quite general. We make no assumptions on the structure of the original problem, in particular, no assumptions of convexity and separability, and place only mild requirements on the approximations. The approximations used in the framework can be of any nature appropriate to an application; for instance, they can be represented by analyses, simulations, or simple algebraic models. This paper introduces the approach and outlines the convergence analysis.},
  doi      = {10.1007/BF01197433},
  groups   = {Opti Algorithms, Gradient-based, Core refs},
  refid    = {Alexandrov1998},
  url      = {https://doi.org/10.1007/BF01197433},
}

@Article{Alexandrov2001,
  author    = {Alexandrov, Natalia M. and Lewis, Robert Michael and Gumbert, Clyde R. and Green, Lawrence L. and Newman, Perry A.},
  journal   = {Journal of Aircraft},
  title     = {Approximation and Model Management in Aerodynamic Optimization with Variable-Fidelity Models},
  year      = {2001},
  issn      = {0021-8669},
  month     = nov,
  number    = {6},
  pages     = {1093--1101},
  volume    = {38},
  comment   = {doi: 10.2514/2.2877},
  doi       = {10.2514/2.2877},
  groups    = {Gradient-based, Core refs},
  publisher = {American Institute of Aeronautics and Astronautics},
  url       = {https://doi.org/10.2514/2.2877},
}

@Article{March2012smo,
  author       = {March, Andrew and Willcox, Karen},
  journal      = {Structural and Multidisciplinary Optimization},
  title        = {Constrained multifidelity optimization using model calibration},
  year         = {2012},
  issn         = {1615-1488},
  number       = {1},
  pages        = {93--109},
  volume       = {46},
  abstract     = {Multifidelity optimization approaches seek to bring higher-fidelity analyses earlier into the design process by using performance estimates from lower-fidelity models to accelerate convergence towards the optimum of a high-fidelity design problem. Current multifidelity optimization methods generally fall into two broad categories: provably convergent methods that use either the high-fidelity gradient or a high-fidelity pattern-search, and heuristic model calibration approaches, such as interpolating high-fidelity data or adding a Kriging error model to a lower-fidelity function. This paper presents a multifidelity optimization method that bridges these two ideas; our method iteratively calibrates lower-fidelity information to the high-fidelity function in order to find an optimum of the high-fidelity design problem. The algorithm developed minimizes a high-fidelity objective function subject to a high-fidelity constraint and other simple constraints. The algorithm never computes the gradient of a high-fidelity function; however, it achieves first-order optimality using sensitivity information from the calibrated low-fidelity models, which are constructed to have negligible error in a neighborhood around the solution. The method is demonstrated for aerodynamic shape optimization and shows at least an 80% reduction in the number of high-fidelity analyses compared other single-fidelity derivative-free and sequential quadratic programming methods. The method uses approximately the same number of high-fidelity analyses as a multifidelity trust-region algorithm that estimates the high-fidelity gradient using finite differences.},
  comment-ruda = {Use trust region w/ radial basis functions for error interpolation (calibration).},
  doi          = {10.1007/s00158-011-0749-1},
  groups       = {Gradient-based},
  refid        = {March2012},
  url          = {https://doi.org/10.1007/s00158-011-0749-1},
}

@Article{March2012aiaa,
  author       = {March, Andrew and Willcox, Karen},
  journal      = {AIAA Journal},
  title        = {Provably Convergent Multifidelity Optimization Algorithm Not Requiring High-Fidelity Derivatives},
  year         = {2012},
  issn         = {0001-1452},
  month        = may,
  number       = {5},
  pages        = {1079--1089},
  volume       = {50},
  comment      = {doi: 10.2514/1.J051125},
  comment-ruda = {With a quadratic penalty function.},
  doi          = {10.2514/1.J051125},
  groups       = {Gradient-based, Core refs},
  publisher    = {American Institute of Aeronautics and Astronautics},
  url          = {https://doi.org/10.2514/1.J051125},
}

@Article{Huang2006smo,
  author   = {Huang, D. and Allen, T. T. and Notz, W. I. and Miller, R. A.},
  journal  = {Structural and Multidisciplinary Optimization},
  title    = {Sequential kriging optimization using multiple-fidelity evaluations},
  year     = {2006},
  issn     = {1615-1488},
  number   = {5},
  pages    = {369--382},
  volume   = {32},
  abstract = {When cost per evaluation on a system of interest is high, surrogate systems can provide cheaper but lower-fidelity information. In the proposed extension of the sequential kriging optimization method, surrogate systems are exploited to reduce the total evaluation cost. The method utilizes data on all systems to build a kriging metamodel that provides a global prediction of the objective function and a measure of prediction uncertainty. The location and fidelity level of the next evaluation are selected by maximizing an augmented expected improvement function, which is connected with the evaluation costs. The proposed method was applied to test functions from the literature and a metal-forming process design problem via finite element simulations. The method manifests sensible search patterns, robust performance, and appreciable reduction in total evaluation cost as compared to the original method.},
  doi      = {10.1007/s00158-005-0587-0},
  groups   = {Sequential model-based, Core refs},
  refid    = {Huang2006},
  url      = {https://doi.org/10.1007/s00158-005-0587-0},
}

@Article{Chen2016,
  author    = {Chen, Shishi and Jiang, Zhen and Yang, Shuxing and Chen, Wei},
  journal   = {AIAA Journal},
  title     = {Multimodel Fusion Based Sequential Optimization},
  year      = {2016},
  issn      = {0001-1452},
  month     = aug,
  number    = {1},
  pages     = {241--254},
  volume    = {55},
  abstract  = {Simulation models with different levels of fidelity have been widely used in engineering design. Even though the nonhierarchical multimodel fusion approach has been developed for integrating data from multiple competing low-fidelity models and a high-fidelity model, how to allocate samples from multifidelity models for the purpose of design optimization still remains challenging. In this work, a new multimodel fusion-based sequential optimization approach is proposed to address the issues of 1) where in the design space to allocate more samples, and 2) which model to evaluate at the chosen infilling sample sites. First, an objective-oriented sampling criterion that balances global exploration and local exploitation is employed to identify the infilling sample location to address the first question. To address the second question, an improved preposterior analysis is developed to determine which simulation model to evaluate, considering both predictive accuracy and computational cost. The improved preposterior analysis not only eliminates the time-consuming Monte Carlo loop in the conventional method but also adopts an analytical model updating formula to further improve the efficiency. To demonstrate the merits of the current proposed multimodel fusion-based sequential optimization approach, two numerical examples and a vehicle engine piston design example are tested. It is shown that the proposed multimodel fusion-based sequential optimization approach is capable of allocating samples from multifidelity models to sequentially update the predictive model for optimization at less computational cost compared to the conventional kriging-based sequential optimization approach.},
  comment   = {doi: 10.2514/1.J054729},
  doi       = {10.2514/1.J054729},
  groups    = {Sequential model-based, Core refs},
  publisher = {American Institute of Aeronautics and Astronautics},
  url       = {https://doi.org/10.2514/1.J054729},
}

@Article{Sacher2021,
  author  = {Matthieu Sacher and Olivier Le Maître and R. Duvigneau and F. Hauville and M. Durand and C. Lothodé},
  journal = {International Journal for Uncertainty Quantification},
  title   = {A Non-Nested Infilling Strategy for Multi-Fidelity based Efficient Global Optimization},
  year    = {2021},
  issn    = {2152-5080},
  number  = {1},
  pages   = {1--30},
  volume  = {11},
  doi     = {10.1615/Int.J.UncertaintyQuantification.2020032982},
  groups  = {Opti Algorithms, Sequential model-based, Core refs},
}

@InProceedings{Kandasamy2017,
  author    = {Kirthevasan Kandasamy and Gautam Dasarathy and Jeff Schneider and Barnab{\'a}s P{\'o}czos},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  title     = {Multi-fidelity {B}ayesian Optimisation with Continuous Approximations},
  year      = {2017},
  month     = {06--11 Aug},
  pages     = {1799--1808},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {70},
  abstract  = {Bandit methods for black-box optimisation, such as Bayesian optimisation, are used in a variety of applications including hyper-parameter tuning and experiment design. Recently, <em>multi-fidelity</em> methods have garnered considerable attention since function evaluations have become increasingly expensive in such applications. Multi-fidelity methods use cheap approximations to the function of interest to speed up the overall optimisation process. However, most multi-fidelity methods assume only a finite number of approximations. On the other hand, in many practical applications, a continuous spectrum of approximations might be available. For instance, when tuning an expensive neural network, one might choose to approximate the cross validation performance using less data $N$ and/or few training iterations $T$. Here, the approximations are best viewed as arising out of a continuous two dimensional space $(N,T)$. In this work, we develop a Bayesian optimisation method, BOCA, for this setting. We characterise its theoretical properties and show that it achieves better regret than than strategies which ignore the approximations. BOCA outperforms several other baselines in synthetic and real experiments.},
  file      = {:Kandasamy2017.pdf:PDF},
  groups    = {Applications of MF modeling, Opti Algorithms, MF modeling, Sequential model-based, Core refs},
  pdf       = {http://proceedings.mlr.press/v70/kandasamy17a/kandasamy17a.pdf},
  url       = {https://proceedings.mlr.press/v70/kandasamy17a.html},
}

@InCollection{Meliani2019,
  author    = {Meliani, Mostafa and Bartoli, Nathalie and Lefebvre, Thierry and Bouhlel, Mohamed-Amine and Martins, Joaquim R. R. A. and Morlier, Joseph},
  booktitle = {AIAA Aviation 2019 Forum},
  publisher = {American Institute of Aeronautics and Astronautics},
  title     = {Multi-fidelity efficient global optimization: Methodology and application to airfoil shape design},
  year      = {2019},
  month     = jun,
  number    = {3236},
  series    = {AIAA AVIATION Forum},
  comment   = {doi:10.2514/6.2019-3236},
  doi       = {10.2514/6.2019-3236},
  groups    = {Sequential model-based, Core refs},
  journal   = {AIAA Aviation 2019 Forum},
  url       = {https://doi.org/10.2514/6.2019-3236},
}

@Article{Jones1998,
  author   = {Jones, Donald R. and Schonlau, Matthias and Welch, William J.},
  journal  = {Journal of Global Optimization},
  title    = {Efficient global optimization of expensive black-box functions},
  year     = {1998},
  issn     = {1573-2916},
  number   = {4},
  pages    = {455--492},
  volume   = {13},
  abstract = {In many engineering optimization problems, the number of function evaluations is severely limited by time or cost. These problems pose a special challenge to the field of global optimization, since existing methods often require more function evaluations than can be comfortably afforded. One way to address this challenge is to fit response surfaces to data collected by evaluating the objective and constraint functions at a few points. These surfaces can then be used for visualization, tradeoff analysis, and optimization. In this paper, we introduce the reader to a response surface methodology that is especially good at modeling the nonlinear, multimodal functions that often occur in engineering. We then show how these approximating functions can be used to construct an efficient global optimization algorithm with a credible stopping rule. The key to using response surfaces for global optimization lies in balancing the need to exploit the approximating surface (by sampling where it is minimized) with the need to improve the approximation (by sampling where prediction error may be high). Striking this balance requires solving certain auxiliary problems which have previously been considered intractable, but we show how these computational obstacles can be overcome.},
  doi      = {10.1023/A:1008306431147},
  file     = {:Jones1998.pdf:PDF},
  groups   = {Earlyworks, Opti Algorithms, Bayesian optimization, Acquisition functions, Core refs},
  refid    = {Jones1998},
  url      = {https://doi.org/10.1023/A:1008306431147},
}

@InProceedings{Snoek2012,
  author    = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Practical {B}ayesian Optimization of Machine Learning Algorithms},
  year      = {2012},
  pages     = {2951--2959},
  publisher = {NIPS 2012},
  volume    = {25},
  file      = {:Snoek2012.pdf:PDF},
  groups    = {Bayesian optimization, Core refs},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2012/file/05311655a15b75fab86956663e1819cd-Paper.pdf},
}

@Article{Shahriari2016,
  author      = {Shahriari, B. and Swersky, K. and Wang, Z. and Adams, R. P. and de Freitas, N.},
  journal     = {Proceedings of the IEEE},
  title       = {{Taking the human out of the loop: A review of Bayesian optimization}},
  year        = {2016},
  issn        = {1558-2256},
  number      = {1},
  pages       = {148--175},
  volume      = {104},
  call-number = {104},
  doi         = {10.1109/JPROC.2015.2494218},
  file        = {:Shahriari2016.pdf:PDF},
  groups      = {Bayesian optimization, Core refs},
  url         = {https://doi.org/10.1109/JPROC.2015.2494218},
}

@InProceedings{Srinivas2010,
  author    = {Srinivas, Niranjan and Krause, Andreas and Kakade, Sham M. and Seeger, Matthias},
  booktitle = {Proceedings of the 27th International Conference on International Conference on Machine Learning},
  title     = {{Gaussian process optimization in the bandit setting: No regret and experimental design}},
  year      = {2010},
  pages     = {1015--1022},
  comment   = {Use Gaussian process upper confidence bound (GP-UCB) as the acquisition function for Bayesian optimization, justified in regret minimization.},
  groups    = {Bayesian optimization, Acquisition functions, Core refs},
  url       = {https://icml.cc/Conferences/2010/papers/422.pdf},
}

@Article{Jones2001,
  author   = {Jones, Donald R.},
  journal  = {Journal of Global Optimization},
  title    = {A Taxonomy of Global Optimization Methods Based on Response Surfaces},
  year     = {2001},
  issn     = {1573-2916},
  number   = {4},
  pages    = {345--383},
  volume   = {21},
  abstract = {This paper presents a taxonomy of existing approaches for using response surfaces for global optimization. Each method is illustrated with a simple numerical example that brings out its advantages and disadvantages. The central theme is that methods that seem quite reasonable often have non-obvious failure modes. Understanding these failure modes is essential for the development of practical algorithms that fulfill the intuitive promise of the response surface approach.},
  doi      = {10.1023/A:1012771025575},
  file     = {:Jones2001.pdf:PDF},
  groups   = {Bayesian optimization, Core refs},
  refid    = {Jones2001},
  url      = {https://doi.org/10.1023/A:1012771025575},
}

@InProceedings{Eriksson2019,
  author       = {Eriksson, David and Pearce, Michael and Gardner, Jacob and Turner, Ryan D. and Poloczek, Matthias},
  booktitle    = {Advances in Neural Information Processing Systems},
  title        = {Scalable global optimization via local {B}ayesian optimization},
  year         = {2019},
  pages        = {5496--5507},
  publisher    = {NIPS 2019},
  volume       = {32},
  comment-ruda = {High-dim BO using TuRBO (Trust Region BO): fits GPs locally; global acquisition, implicit bandit.},
  file         = {:Eriksson2019.pdf:PDF},
  groups       = {High-dimension BO, RBF, Core refs},
  url          = {https://proceedings.neurips.cc/paper_files/paper/2019/hash/6c990b7aca7bc7058f5e98ea909e924b-Abstract.html},
}

@Article{Huang2006jgo,
  author   = {Huang, D. and Allen, T. T. and Notz, W. I. and Zeng, N.},
  journal  = {Journal of Global Optimization},
  title    = {Global Optimization of Stochastic Black-Box Systems via Sequential Kriging Meta-Models},
  year     = {2006},
  issn     = {1573-2916},
  number   = {3},
  pages    = {441--466},
  volume   = {34},
  abstract = {This paper proposes a new method that extends the efficient global optimization to address stochastic black-box systems. The method is based on a kriging meta-model that provides a global prediction of the objective values and a measure of prediction uncertainty at every point. The criterion for the infill sample selection is an augmented expected improvement function with desirable properties for stochastic responses. The method is empirically compared with the revised simplex search, the simultaneous perturbation stochastic approximation, and the DIRECT methods using six test problems from the literature. An application case study on an inventory system is also documented. The results suggest that the proposed method has excellent consistency and efficiency in finding global optimal solutions, and is particularly useful for expensive systems.},
  doi      = {10.1007/s10898-005-2454-3},
  groups   = {BO uncertainty, Core refs},
  refid    = {Huang2006},
  url      = {https://doi.org/10.1007/s10898-005-2454-3},
}

@Article{Picheny2013,
  author   = {Picheny, Victor and Wagner, Tobias and Ginsbourger, David},
  journal  = {Structural and Multidisciplinary Optimization},
  title    = {A benchmark of kriging-based infill criteria for noisy optimization},
  year     = {2013},
  issn     = {1615-1488},
  number   = {3},
  pages    = {607--626},
  volume   = {48},
  abstract = {Responses of many real-world problems can only be evaluated perturbed by noise. In order to make an efficient optimization of these problems possible, intelligent optimization strategies successfully coping with noisy evaluations are required. In this article, a comprehensive review of existing kriging-based methods for the optimization of noisy functions is provided. In summary, ten methods for choosing the sequential samples are described using a unified formalism. They are compared on analytical benchmark problems, whereby the usual assumption of homoscedastic Gaussian noise made in the underlying models is meet. Different problem configurations (noise level, maximum number of observations, initial number of observations) and setups (covariance functions, budget, initial sample size) are considered. It is found that the choices of the initial sample size and the covariance function are not critical. The choice of the method, however, can result in significant differences in the performance. In particular, the three most intuitive criteria are found as poor alternatives. Although no criterion is found consistently more efficient than the others, two specialized methods appear more robust on average.},
  doi      = {10.1007/s00158-013-0919-4},
  groups   = {BO uncertainty, Core refs},
  refid    = {Picheny2013},
  url      = {https://doi.org/10.1007/s00158-013-0919-4},
}

@Book{BenTal2009,
  author    = {Ben-Tal, Aharon and El Ghaoui, Laurent and Nemirovski, Arkadi},
  publisher = {Princeton University Press},
  title     = {Robust optimization},
  year      = {2009},
  address   = {New Jersey, USA},
  doi       = {10.1515/9781400831050},
  groups    = {Books},
  url       = {https://doi.org/10.1515/9781400831050},
}

@Article{Valdebenito2010,
  author   = {Valdebenito, Marcos A. and Schuëller, Gerhart I.},
  journal  = {Structural and Multidisciplinary Optimization},
  title    = {A survey on approaches for reliability-based optimization},
  year     = {2010},
  issn     = {1615-1488},
  number   = {5},
  pages    = {645--663},
  volume   = {42},
  abstract = {Reliability-based Optimization is a most appropriate and advantageous methodology for structural design. Its main feature is that it allows determining the best design solution (with respect to prescribed criteria) while explicitly considering the unavoidable effects of uncertainty. In general, the application of this methodology is numerically involved, as it implies the simultaneous solution of an optimization problem and also the use of specialized algorithms for quantifying the effects of uncertainties. In view of this fact, several approaches have been developed in the literature for applying this methodology in problems of practical interest. This contribution provides a survey on approaches for performing Reliability-based Optimization, with emphasis on the theoretical foundations and the main assumptions involved. Early approaches as well as the most recently developed methods are covered. In addition, a qualitative comparison is performed in order to provide some general guidelines on the range of applicability on the different approaches discussed in this contribution.},
  doi      = {10.1007/s00158-010-0518-6},
  groups   = {Surveys, Core refs},
  refid    = {Valdebenito2010},
  url      = {https://doi.org/10.1007/s00158-010-0518-6},
}

@Article{Beyer2007,
  author   = {Beyer, Hans-Georg and Sendhoff, Bernhard},
  journal  = {Computer Methods in Applied Mechanics and Engineering},
  title    = {{Robust optimization - A comprehensive survey}},
  year     = {2007},
  issn     = {0045-7825},
  number   = {33},
  pages    = {3190--3218},
  volume   = {196},
  abstract = {This paper reviews the state-of-the-art in robust design optimization - the search for designs and solutions which are immune with respect to production tolerances, parameter drifts during operation time, model sensitivities and others. Starting with a short glimps of Taguchi’s robust design methodology, a detailed survey of approaches to robust optimization is presented. This includes a detailed discussion on how to account for design uncertainties and how to measure robustness (i.e., how to evaluate robustness). The main focus will be on the different approaches to perform robust optimization in practice including the methods of mathematical programming, deterministic nonlinear optimization, and direct search methods such as stochastic approximation and evolutionary computation. It discusses the strengths and weaknesses of the different methods, thus, providing a basis for guiding the engineer to the most appropriate techniques. It also addresses performance aspects and test scenarios for direct robust optimization techniques.},
  doi      = {10.1016/j.cma.2007.03.003},
  groups   = {Surveys, Core refs},
  keywords = {Direct search methods, Evolutionary computation, Handling design uncertainties, Mathematical programming, Noisy optimization, Robust design, Robust optimization},
  url      = {https://www.sciencedirect.com/science/article/pii/S0045782507001259},
}

@Article{Fricker2013,
  author    = {Fricker, Thomas E. and Oakley, Jeremy E. and Urban, Nathan M.},
  journal   = {Technometrics},
  title     = {Multivariate {G}aussian Process Emulators With Nonseparable Covariance Structures},
  year      = {2013},
  issn      = {0040-1706},
  month     = feb,
  number    = {1},
  pages     = {47--56},
  volume    = {55},
  comment   = {doi: 10.1080/00401706.2012.715835},
  doi       = {10.1080/00401706.2012.715835},
  file      = {:Fricker2013.pdf:PDF},
  groups    = {MF modeling, Others, Core refs},
  publisher = {Taylor \& Francis},
  url       = {https://doi.org/10.1080/00401706.2012.715835},
}

@Article{Oune2021,
  author   = {Oune, Nicholas and Bostanabad, Ramin},
  journal  = {Computer Methods in Applied Mechanics and Engineering},
  title    = {Latent map {G}aussian processes for mixed variable metamodeling},
  year     = {2021},
  issn     = {0045-7825},
  pages    = {114128},
  volume   = {387},
  abstract = {Gaussian processes (GPs) are ubiquitously used in sciences and engineering as metamodels. Standard GPs, however, can only handle numerical or quantitative variables. In this paper, we introduce latent map Gaussian processes (LMGPs) that inherit the attractive properties of GPs and are also applicable to mixed data which have both quantitative and qualitative inputs. The core idea behind LMGPs is to learn a continuous, low-dimensional latent space or manifold which encodes all qualitative inputs. To learn this manifold, we first assign a unique prior vector representation to each combination of qualitative inputs. We then use a low-rank linear map to project these priors on a manifold that characterizes the posterior representations. As the posteriors are quantitative, they can be directly used in any standard correlation function such as the Gaussian or Matern. Hence, the optimal map and the corresponding manifold, along with other hyperparameters of the correlation function, can be systematically learned via maximum likelihood estimation. Through a wide range of analytic and real-world examples, we demonstrate the advantages of LMGPs over state-of-the-art methods in terms of accuracy and versatility. In particular, we show that LMGPs can handle variable-length inputs, have an explainable neural network interpretation, and provide insights into how qualitative inputs affect the response or interact with each other. We also employ LMGPs in Bayesian optimization and illustrate that they can discover optimal compound compositions more efficiently than conventional methods that convert compositions to qualitative variables via manual featurization.},
  doi      = {10.1016/j.cma.2021.114128},
  groups   = {Latent map, Core refs},
  keywords = {Gaussian processes, Emulation, Metamodeling, Mixed-variable optimization, Computer experiments, Manifold learning},
  url      = {https://www.sciencedirect.com/science/article/pii/S004578252100459X},
}

@Article{Zhou2011,
  author    = {Zhou, Qiang and Qian, Peter Z. G. and Zhou, Shiyu},
  journal   = {Technometrics},
  title     = {A Simple Approach to Emulation for Computer Models With Qualitative and Quantitative Factors},
  year      = {2011},
  issn      = {0040-1706},
  month     = aug,
  number    = {3},
  pages     = {266--273},
  volume    = {53},
  comment   = {doi: 10.1198/TECH.2011.10025},
  doi       = {10.1198/TECH.2011.10025},
  groups    = {Latent map, Core refs},
  publisher = {Taylor \& Francis},
  url       = {https://doi.org/10.1198/TECH.2011.10025},
}

@InProceedings{WuJ2019,
  author    = {Wu, Jian and Frazier, Peter I.},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Practical Two-Step Lookahead {B}ayesian Optimization},
  year      = {2019},
  pages     = {9813--9823},
  publisher = {NeurIPS 2019},
  volume    = {32},
  groups    = {Acquisition functions},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2019/file/2e6d9c6052e99fcdfa61d9b9da273ca2-Paper.pdf},
}

@InProceedings{Jiang2020,
  author    = {Jiang, Shali and Jiang, Daniel R. and Balandat, Maximilian and Karrer, Brian and Gardner, Jacob R. and Garnett, Roman},
  booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
  title     = {Efficient Nonmyopic {Bayesian} Optimization via One-Shot Multi-Step Trees},
  year      = {2020},
  address   = {Red Hook, NY, USA},
  pages     = {18039--18049},
  publisher = {Curran Associates Inc.},
  series    = {NIPS'20},
  abstract  = {Bayesian optimization is a sequential decision making framework for optimizing expensive-to-evaluate black-box functions. Computing a full lookahead policy amounts to solving a highly intractable stochastic dynamic program. Myopic approaches, such as expected improvement, are often adopted in practice, but they ignore the long-term impact of the immediate decision. Existing nonmyopic approaches are mostly heuristic and/or computationally expensive. In this paper, we provide the first efficient implementation of general multi-step lookahead Bayesian optimization, formulated as a sequence of nested optimization problems within a multi-step scenario tree. Instead of solving these problems in a nested way, we equivalently optimize all decision variables in the full tree jointly, in a "oneshot" fashion. Combining this with an efficient method for implementing multistep Gaussian process "fantasization," we demonstrate that multi-step expected improvement is computationally tractable and exhibits performance superior to existing methods on a wide range of benchmarks.},
  articleno = {1514},
  groups    = {Acquisition functions, Core refs},
  isbn      = {9781713829546},
  location  = {Vancouver, BC, Canada},
  numpages  = {11},
}

@InCollection{Schonlau1998,
  author    = {Schonlau, Matthias and Welch, William J. and Jones, Donald R.},
  booktitle = {New developments and applications in experimental design},
  publisher = {Institute of Mathematical Statistics},
  title     = {Global versus Local Search in Constrained Optimization of Computer Models},
  year      = {1998},
  editor    = {Nancy Flournoy and William F. Rosenberger and Weng Kee Wong},
  pages     = {11--25},
  series    = {IMS Lecture Notes-Monograph Series},
  volume    = {34},
  abstract  = {[Engineering systems are now frequently optimized via computer models. The input-output relationships in these models are often highly nonlinear deterministic functions that are expensive to compute. Thus, when searching for the global optimum, it is desirable to minimize the number of function evaluations. Bayesian global optimization methods are well-suited to this task because they make use of all previous evaluations in selecting the next search point. A statistical model is fit to the sampled points which allows predictions to be made elsewhere, along with a measure of possible prediction error (uncertainty). The next point is chosen to maximize a criterion that balances searching where the predicted value of the function is good (local search) with searching where the uncertainty of prediction is large (global search). We extend this methodology in several ways. First, we introduce a parameter that controls the local-global balance. Secondly, we propose a method for dealing with nonlinear inequality constraints from additional response variables. Lastly, we adapt the sequential algorithm to proceed in stages rather than one point at a time. The extensions are illustrated using a shape optimization problem from the automotive industry.]},
  doi       = {10.1214/lnms/1215456182},
  groups    = {Constrained BO, Core refs},
  url       = {http://www.jstor.org/stable/4356058},
}

@Article{Gramacy2016,
  author       = {Gramacy, Robert B. and Gray, Genetha A. and Le Digabel, Sébastien and Lee, Herbert K. H. and Ranjan, Pritam and Wells, Garth and Wild, Stefan M.},
  journal      = {Technometrics},
  title        = {Modeling an Augmented {L}agrangian for Blackbox Constrained Optimization},
  year         = {2016},
  issn         = {0040-1706},
  month        = jan,
  number       = {1},
  pages        = {1--11},
  volume       = {58},
  comment-ruda = {Constrained optimization using GPs for the objective and each constraint; EI for acquisition.},
  doi          = {10.1080/00401706.2015.1014065},
  file         = {:Gramacy2016.pdf:PDF},
  groups       = {Constrained BO, Core refs},
  publisher    = {Taylor \& Francis},
  url          = {https://doi.org/10.1080/00401706.2015.1014065},
}

@Article{WangZ2016,
  author     = {Wang, Ziyu and Hutter, Frank and Zoghi, Masrour and Matheson, David and De Freitas, Nando},
  journal    = {Journal of Artificial Intelligence Research},
  title      = {Bayesian Optimization in a Billion Dimensions via Random Embeddings},
  year       = {2016},
  issn       = {1076-9757},
  month      = {jan},
  number     = {1},
  pages      = {361--387},
  volume     = {55},
  abstract   = {Bayesian optimization techniques have been successfully applied to robotics, planning, sensor placement, recommendation, advertising, intelligent user interfaces and automatic algorithm configuration. Despite these successes, the approach is restricted to problems of moderate dimension, and several workshops on Bayesian optimization have identified its scaling to high-dimensions as one of the holy grails of the field. In this paper, we introduce a novel random embedding idea to attack this problem. The resulting Random EMbedding Bayesian Optimization (REMBO) algorithm is very simple, has important invariance properties, and applies to domains with both categorical and continuous variables. We present a thorough theoretical analysis of REMBO. Empirical results confirm that REMBO can effectively solve problems with billions of dimensions, provided the intrinsic dimensionality is low. They also show that REMBO achieves state-of-the-art performance in optimizing the 47 discrete parameters of a popular mixed integer linear programming solver.},
  address    = {El Segundo, CA, USA},
  groups     = {High-dimension BO},
  issue_date = {January 2016},
  numpages   = {27},
  publisher  = {AI Access Foundation},
  url        = {https://www.jair.org/index.php/jair/article/view/10983},
}

@InProceedings{Nayebi2019,
  author    = {Nayebi, Amin and Munteanu, Alexander and Poloczek, Matthias},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  title     = {A Framework for {B}ayesian Optimization in Embedded Subspaces},
  year      = {2019},
  month     = {09--15 Jun},
  pages     = {4752--4761},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {97},
  abstract  = {We present a theoretically founded approach for high-dimensional Bayesian optimization based on low-dimensional subspace embeddings. We prove that the error in the Gaussian process model is bounded tightly when going from the original high-dimensional search domain to the low-dimensional embedding. This implies that the optimization process in the low-dimensional embedding proceeds essentially as if it were run directly on an unknown active subspace of low dimensionality. The argument applies to a large class of algorithms and GP models, including non-stationary kernels. Moreover, we provide an efficient implementation based on hashing and demonstrate empirically that this subspace embedding achieves considerably better results than the previously proposed methods for high-dimensional BO based on Gaussian matrix projections and structure-learning.},
  groups    = {High-dimension BO},
  pdf       = {http://proceedings.mlr.press/v97/nayebi19a/nayebi19a.pdf},
  url       = {https://proceedings.mlr.press/v97/nayebi19a.html},
}

@Article{LiuH2020,
  author      = {Liu, H. and Ong, Y. S. and Shen, X. and Cai, J.},
  journal     = {IEEE Transactions on Neural Networks and Learning Systems},
  title       = {{When Gaussian process meets big data: A review of scalable GPs}},
  year        = {2020},
  issn        = {2162-2388},
  number      = {11},
  pages       = {4405--4423},
  volume      = {31},
  call-number = {31},
  doi         = {10.1109/TNNLS.2019.2957109},
  groups      = {Others},
}

@Article{Moriconi2020,
  author   = {Moriconi, Riccardo and Deisenroth, Marc Peter and Sesh Kumar, K. S.},
  journal  = {Machine Learning},
  title    = {High-dimensional {B}ayesian optimization using low-dimensional feature spaces},
  year     = {2020},
  issn     = {1573-0565},
  number   = {9},
  pages    = {1925--1943},
  volume   = {109},
  abstract = {Bayesian optimization (BO) is a powerful approach for seeking the global optimum of expensive black-box functions and has proven successful for fine tuning hyper-parameters of machine learning models. However, BO is practically limited to optimizing 10-20 parameters. To scale BO to high dimensions, we usually make structural assumptions on the decomposition of the objective and/or exploit the intrinsic lower dimensionality of the problem, e.g. by using linear projections. We could achieve a higher compression rate with nonlinear projections, but learning these nonlinear embeddings typically requires much data. This contradicts the BO objective of a relatively small evaluation budget. To address this challenge, we propose to learn a low-dimensional feature space jointly with (a) the response surface and (b) a reconstruction mapping. Our approach allows for optimization of BO’s acquisition function in the lower-dimensional subspace, which significantly simplifies the optimization problem. We reconstruct the original parameter space from the lower-dimensional subspace for evaluating the black-box function. For meaningful exploration, we solve a constrained optimization problem.},
  doi      = {10.1007/s10994-020-05899-z},
  groups   = {High-dimension BO, Core refs},
  refid    = {Moriconi2020},
  url      = {https://doi.org/10.1007/s10994-020-05899-z},
}

@Article{Kanno2020,
  author   = {Kanno, Yoshihiro},
  journal  = {Structural and Multidisciplinary Optimization},
  title    = {On three concepts in robust design optimization: absolute robustness, relative robustness, and less variance},
  year     = {2020},
  issn     = {1615-1488},
  number   = {2},
  pages    = {979--1000},
  volume   = {62},
  abstract = {This paper provides a clear perspective on existing several different approaches to robust design optimization of structures. We primarily consider three approaches: the worst-case optimization, the discrepancy (i.e., the maximum gap between the objective values in a nominal case and a possibly occurring case) minimization, and the variance minimization. Some other formulations can also be linked with one of these three approaches. To investigate how the solutions derived by these three approaches differ from each other, we present two numerical examples. This direct comparison clarifies different features of these approaches.},
  doi      = {10.1007/s00158-020-02503-9},
  groups   = {Surveys, Core refs},
  refid    = {Kanno2020},
  url      = {https://doi.org/10.1007/s00158-020-02503-9},
}

@Article{Campi2011,
  author   = {Campi, M. C. and Garatti, S.},
  journal  = {Journal of Optimization Theory and Applications},
  title    = {A Sampling-and-Discarding Approach to Chance-Constrained Optimization: Feasibility and Optimality},
  year     = {2011},
  issn     = {1573-2878},
  number   = {2},
  pages    = {257--280},
  volume   = {148},
  abstract = {In this paper, we study the link between a Chance-Constrained optimization Problem (CCP) and its sample counterpart (SP). SP has a finite number, say N, of sampled constraints. Further, some of these sampled constraints, say k, are discarded, and the final solution is indicated by $x^{\ast}_{N,k}$. Extending previous results on the feasibility of sample convex optimization programs, we establish the feasibility of $x^{\ast}_{N,k}$for the initial CCP problem.},
  doi      = {10.1007/s10957-010-9754-6},
  groups   = {Others, Core refs},
  refid    = {Campi2011},
  url      = {https://doi.org/10.1007/s10957-010-9754-6},
}

@Article{Moustapha2019,
  author   = {Moustapha, Maliki and Sudret, Bruno},
  journal  = {Structural and Multidisciplinary Optimization},
  title    = {Surrogate-assisted reliability-based design optimization: a survey and a unified modular framework},
  year     = {2019},
  issn     = {1615-1488},
  number   = {5},
  pages    = {2157--2176},
  volume   = {60},
  abstract = {Reliability-based design optimization (RBDO) is an active field of research with an ever increasing number of contributions. Numerous methods have been proposed for the solution of RBDO, a complex problem that combines optimization and reliability analysis. Classical approaches are based on approximation methods and have been classified in review papers. In this paper, we first review classical approaches based on approximation methods such as FORM, and also more recent methods that rely upon surrogate modelling and Monte Carlo simulation. We then propose a generalization of the existing surrogate-assisted and simulation-based RBDO techniques using a unified framework that includes three independent blocks, namely adaptive surrogate modelling, reliability analysis, and optimization. These blocks are non-intrusive with respect to each other and can be plugged independently in the framework. After a discussion on numerical considerations that require attention for the framework to yield robust solutions to various types of problems, the latter is applied to three examples (using two analytical functions and a finite element model). Kriging and support vector machines regression together with their own active learning schemes are considered in the surrogate model block. In terms of reliability analysis, the proposed framework is illustrated using both crude Monte Carlo and subset simulation. Finally, the covariance matrix adaptation-evolution scheme (CMA-ES), a global search algorithm, or sequential quadratic programming (SQP), a local gradient-based method, is used in the optimization block. The comparison of the results to benchmark studies shows the effectiveness and efficiency of the proposed framework.},
  doi      = {10.1007/s00158-019-02290-y},
  groups   = {Surveys, Core refs},
  refid    = {Moustapha2019},
  url      = {https://doi.org/10.1007/s00158-019-02290-y},
}

@Article{Barthelemy1993,
  author       = {Barthelemy, J. F. M. and Haftka, R. T.},
  journal      = {Structural optimization},
  title        = {Approximation concepts for optimum structural design -- a review},
  year         = {1993},
  issn         = {1615-1488},
  number       = {3},
  pages        = {129--144},
  volume       = {5},
  abstract     = {This paper reviews the basic approximation concepts used in structural optimization. It also discusses some of the most recent developments in that area since the introduction of approximation concepts in the mid-seventies. The paper distinguishes between local, medium-range and global approximations; it covers function approximations and problem approximations. It shows that, although the lack of comparative data established on reference test cases prevents an accurate assessment, there have been significant improvements. The largest number of developments have been in the areas of local function approximations and use of intermediate variable and response quantities. It appears also that some new methodologies emerge which could greatly benefit from the introduction of new computer architectures.},
  comment-ruda = {A review paper on surrogate-based optimization for structural design: (1) Local multi-fidelity surrogate + trust region; calibrated to first-order consistency conditions. (2) Global approximations.},
  doi          = {10.1007/BF01743349},
  groups       = {Surveys, Core refs},
  refid        = {Barthelemy1993},
  url          = {https://doi.org/10.1007/BF01743349},
}

@InCollection{Simpson2008,
  author    = {Simpson, Timothy and Toropov, Vasilli and Balabanov, Vladimir and Viana, Felipe},
  booktitle = {The 12th AIAA/ISSMO Multidisciplinary Analysis and Optimization Conference},
  publisher = {American Institute of Aeronautics and Astronautics},
  title     = {Design and Analysis of Computer Experiments in Multidisciplinary Design Optimization: A Review of How Far We Have Come - Or Not},
  year      = {2008},
  month     = sep,
  number    = {5802},
  series    = {Multidisciplinary Analysis Optimization Conferences},
  comment   = {doi:10.2514/6.2008-5802},
  doi       = {10.2514/6.2008-5802},
  groups    = {Surveys, Core refs},
  url       = {https://doi.org/10.2514/6.2008-5802},
}

@Article{Queipo2005,
  author   = {Queipo, Nestor V. and Haftka, Raphael T. and Shyy, Wei and Goel, Tushar and Vaidyanathan, Rajkumar and Kevin Tucker, P.},
  journal  = {Progress in Aerospace Sciences},
  title    = {Surrogate-based analysis and optimization},
  year     = {2005},
  issn     = {0376-0421},
  number   = {1},
  pages    = {1--28},
  volume   = {41},
  abstract = {A major challenge to the successful full-scale development of modern aerospace systems is to address competing objectives such as improved performance, reduced costs, and enhanced safety. Accurate, high-fidelity models are typically time consuming and computationally expensive. Furthermore, informed decisions should be made with an understanding of the impact (global sensitivity) of the design variables on the different objectives. In this context, the so-called surrogate-based approach for analysis and optimization can play a very valuable role. The surrogates are constructed using data drawn from high-fidelity models, and provide fast approximations of the objectives and constraints at new design points, thereby making sensitivity and optimization studies feasible. This paper provides a comprehensive discussion of the fundamental issues that arise in surrogate-based analysis and optimization (SBAO), highlighting concepts, methods, techniques, as well as practical implications. The issues addressed include the selection of the loss function and regularization criteria for constructing the surrogates, design of experiments, surrogate selection and construction, sensitivity analysis, convergence, and optimization. The multi-objective optimal design of a liquid rocket injector is presented to highlight the state of the art and to help guide future efforts.},
  doi      = {10.1016/j.paerosci.2005.02.001},
  groups   = {Surveys, Core refs},
  url      = {https://www.sciencedirect.com/science/article/pii/S0376042105000102},
}

@InProceedings{Chapelle2011,
  author    = {Chapelle, Olivier and Li, Lihong},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {An empirical evaluation of {T}hompson sampling},
  year      = {2011},
  pages     = {2249--2257},
  publisher = {NIPS 2011},
  volume    = {24},
  url       = {https://papers.nips.cc/paper_files/paper/2011/hash/e53a0a2978c28872a4505bdb51db06dc-Abstract.html},
}

@Article{Allaire2014,
  author  = {Allaire, Douglas and Willcox, Karen},
  journal = {International Journal for Uncertainty Quantification},
  title   = {A Mathematical and Computational Framework for Multifidelity Design and Analysis with Computer Models},
  year    = {2014},
  issn    = {2152-5080},
  number  = {1},
  pages   = {1--20},
  volume  = {4},
  comment = {A chain of LFMs, build a GP surrogate for each, merge as dependent models.},
  doi     = {10.1615/Int.J.UncertaintyQuantification.2013004121},
}

@InProceedings{WuJ2017,
  author       = {Wu, Jian and Poloczek, Matthias and Wilson, Andrew G and Frazier, Peter},
  booktitle    = {Advances in Neural Information Processing Systems},
  title        = {Bayesian Optimization with Gradients},
  year         = {2017},
  pages        = {5267--5278},
  publisher    = {NIPS 2017},
  volume       = {30},
  comment-ruda = {BO with gradients.},
  url          = {https://proceedings.neurips.cc/paper/2017/hash/64a08e5f1e6c39faeb90108c430eb120-Abstract.html},
}

@InCollection{Frazier2018,
  author    = {Frazier, Peter I.},
  booktitle = {Recent Advances in Optimization and Modeling of Contemporary Problems},
  publisher = {{INFORMS}},
  title     = {Bayesian Optimization},
  year      = {2018},
  chapter   = {11},
  isbn      = {978-0-9906153-2-3},
  month     = oct,
  pages     = {255--278},
  series    = {{INFORMS TutORials in Operations Research}},
  comment   = {A popular tutorial on Bayesian optimization.},
  doi       = {10.1287/educ.2018.0188},
  file      = {:Frazier2018-arxiv.pdf:PDF;:Frazier2018.pdf:PDF},
  groups    = {Bayesian optimization, Core refs},
}

@Book{Garnett2023,
  author    = {Garnett, Roman},
  publisher = {Cambridge University Press},
  title     = {Bayesian Optimization},
  year      = {2023},
  address   = {Cambridge},
  abstract  = {Bayesian optimization is a methodology for optimizing expensive objective functions that has proven success in the sciences, engineering, and beyond. This timely text provides a self-contained and comprehensive introduction to the subject, starting from scratch and carefully developing all the key ideas along the way. This bottom-up approach illuminates unifying themes in the design of Bayesian optimization algorithms and builds a solid theoretical foundation for approaching novel situations. The core of the book is divided into three main parts, covering theoretical and practical aspects of Gaussian process modeling, the Bayesian approach to sequential decision making, and the realization and computation of practical and effective optimization policies. Following this foundational material, the book provides an overview of theoretical convergence results, a survey of notable extensions, a comprehensive history of Bayesian optimization, and an extensive annotated bibliography of applications.},
  database  = {Cambridge Core},
  doi       = {10.1017/9781108348973},
  groups    = {Books},
  issn      = {9781108425780},
}

@Article{Bellman1952,
  author    = {Bellman, Richard},
  journal   = {Proceedings of the National Academy of Sciences},
  title     = {On the Theory of Dynamic Programming},
  year      = {1952},
  month     = aug,
  number    = {8},
  pages     = {716--719},
  volume    = {38},
  comment   = {doi: 10.1073/pnas.38.8.716},
  doi       = {10.1073/pnas.38.8.716},
  publisher = {Proceedings of the National Academy of Sciences},
  url       = {https://doi.org/10.1073/pnas.38.8.716},
}

@Book{Powell2011,
  author    = {Powell, Warren B.},
  publisher = {John Wiley {\&} Sons},
  title     = {Approximate Dynamic Programming: Solving the curses of dimensionality},
  year      = {2011},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:Additive\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Multiplicative\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Hybrid\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:FEM\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Examples of computational cost\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Input-input space mapping\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Output-output space mapping\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Gradient-based\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Model-then-optimize\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Sequential model-based\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Bayesian optimization\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Multi-objective BO\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:High-dimension BO\;0\;0\;0x8a8a8aff\;\;\;;
1 StaticGroup:BO uncertainty\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:BO applications\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:MF optimization under uncertainty\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:MF reliability analysis\;0\;0\;0x8a8a8aff\;\;\;;
1 StaticGroup:MF_UQ\;0\;0\;0x8a8a8aff\;\;\;;
1 StaticGroup:MF BO applications\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Surveys\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Books\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Acquisition functions\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Latent map\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Others\;0\;0\;0x8a8a8aff\;\;\;;
1 StaticGroup:Constrained BO\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Discrete BO\;0\;1\;0x8a8a8aff\;\;\;;
1 TexGroup:Cited entries\;1\;/home/ruda/repo/text/papers/dde-BO/MFBO-review/overleaf/MF_BO_ver1.aux\;1\;0x8a8a8aff\;\;Bib entries cited in the MF BO review paper.\;;
}
